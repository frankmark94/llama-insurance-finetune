{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üöÄ Google Colab Setup for LLaMA Insurance Fine-tuning\n",
    "\n",
    "This notebook sets up the complete environment for fine-tuning LLaMA models on insurance data.\n",
    "\n",
    "## What this notebook does:\n",
    "1. Configures GPU and runtime settings\n",
    "2. Installs all required dependencies\n",
    "3. Sets up authentication (Hugging Face, W&B)\n",
    "4. Clones the project repository\n",
    "5. Mounts Google Drive for persistence\n",
    "6. Verifies the setup\n",
    "\n",
    "**‚ö†Ô∏è Important: Make sure to enable GPU in Runtime > Change runtime type**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpu-check"
   },
   "source": [
    "## 1. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu-check-code"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected! Please enable GPU in Runtime > Change runtime type\")\n",
    "\n",
    "# Check available RAM\n",
    "result = subprocess.run(['free', '-h'], capture_output=True, text=True)\n",
    "print(\"\\nSystem RAM:\")\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install-deps"
   },
   "source": [
    "## 2. Install Dependencies\n",
    "\n",
    "Installing all required packages for LLaMA fine-tuning with LoRA/PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-core"
   },
   "outputs": [],
   "source": [
    "# Install core ML libraries\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers>=4.36.0\n",
    "!pip install -q accelerate>=0.24.0\n",
    "!pip install -q datasets>=2.14.0\n",
    "!pip install -q peft>=0.7.0\n",
    "!pip install -q bitsandbytes>=0.41.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-training"
   },
   "outputs": [],
   "source": [
    "# Install training and evaluation tools\n",
    "!pip install -q wandb>=0.16.0\n",
    "!pip install -q tensorboard>=2.15.0\n",
    "!pip install -q scikit-learn>=1.3.0\n",
    "!pip install -q rouge-score>=0.1.2\n",
    "!pip install -q nltk>=3.8.1\n",
    "!pip install -q evaluate>=0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-utils"
   },
   "outputs": [],
   "source": [
    "# Install data processing utilities\n",
    "!pip install -q pandas>=2.0.0\n",
    "!pip install -q numpy>=1.24.0\n",
    "!pip install -q regex>=2023.10.3\n",
    "!pip install -q tqdm>=4.66.0\n",
    "!pip install -q huggingface-hub>=0.19.0\n",
    "!pip install -q safetensors>=0.4.0\n",
    "!pip install -q ipywidgets>=8.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-optional"
   },
   "outputs": [],
   "source": [
    "# Optional: Install flash attention (may not work on all Colab instances)\n",
    "try:\n",
    "    !pip install -q flash-attn>=2.3.0 --no-build-isolation\n",
    "    print(\"‚úÖ Flash Attention installed successfully\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Flash Attention installation failed (this is optional)\")\n",
    "\n",
    "# Install additional tokenization support\n",
    "!pip install -q sentencepiece>=0.1.99\n",
    "!pip install -q protobuf>=4.25.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "authentication"
   },
   "source": [
    "## 3. Authentication Setup\n",
    "\n",
    "Set up authentication for Hugging Face and Weights & Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hf-auth"
   },
   "outputs": [],
   "source": [
    "# Hugging Face Authentication\n",
    "from huggingface_hub import login\n",
    "\n",
    "print(\"ü§ó Hugging Face Authentication\")\n",
    "print(\"Please enter your Hugging Face token (get one from https://huggingface.co/settings/tokens)\")\n",
    "print(\"This is required to download LLaMA models.\")\n",
    "\n",
    "try:\n",
    "    login()\n",
    "    print(\"‚úÖ Successfully authenticated with Hugging Face\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Authentication failed: {e}\")\n",
    "    print(\"You can also set the HF_TOKEN environment variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wandb-auth"
   },
   "outputs": [],
   "source": [
    "# Weights & Biases Authentication (optional but recommended)\n",
    "import wandb\n",
    "\n",
    "print(\"üìä Weights & Biases Setup (optional for experiment tracking)\")\n",
    "print(\"Get your API key from https://wandb.ai/settings\")\n",
    "\n",
    "try:\n",
    "    wandb.login()\n",
    "    print(\"‚úÖ Successfully authenticated with W&B\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è W&B authentication failed (optional): {e}\")\n",
    "    print(\"You can skip this or authenticate later\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mount-drive"
   },
   "source": [
    "## 4. Mount Google Drive\n",
    "\n",
    "Mount Google Drive to persist models and data across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive-code"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directory in Drive if it doesn't exist\n",
    "project_drive_path = '/content/drive/MyDrive/llama-insurance-finetune'\n",
    "os.makedirs(project_drive_path, exist_ok=True)\n",
    "os.makedirs(f'{project_drive_path}/models', exist_ok=True)\n",
    "os.makedirs(f'{project_drive_path}/data_backup', exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Google Drive mounted and project directory created at: {project_drive_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone-repo"
   },
   "source": [
    "## 5. Clone Project Repository\n",
    "\n",
    "Clone the project repository from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo-code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Change to content directory\n",
    "os.chdir('/content')\n",
    "\n",
    "# Clone repository (replace with your actual repository URL)\n",
    "repo_url = \"https://github.com/franklinmarkley/llama-insurance-finetune.git\"\n",
    "\n",
    "if os.path.exists('llama-insurance-finetune'):\n",
    "    print(\"Repository already exists, pulling latest changes...\")\n",
    "    !cd llama-insurance-finetune && git pull\n",
    "else:\n",
    "    print(f\"Cloning repository from {repo_url}...\")\n",
    "    !git clone {repo_url}\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir('/content/llama-insurance-finetune')\n",
    "print(f\"‚úÖ Working directory: {os.getcwd()}\")\n",
    "\n",
    "# List project contents\n",
    "print(\"\\nProject structure:\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "git-config"
   },
   "source": [
    "## 6. Configure Git (for pushing changes back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "git-config-code"
   },
   "outputs": [],
   "source": [
    "# Configure Git (replace with your information)\n",
    "git_username = \"Your Name\"  # Replace with your name\n",
    "git_email = \"your.email@example.com\"  # Replace with your email\n",
    "\n",
    "!git config --global user.name \"{git_username}\"\n",
    "!git config --global user.email \"{git_email}\"\n",
    "\n",
    "print(\"Git configuration:\")\n",
    "!git config --list | grep user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "verify-setup"
   },
   "source": [
    "## 7. Verify Installation\n",
    "\n",
    "Test that all key components are working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify-imports"
   },
   "outputs": [],
   "source": [
    "# Test core imports\n",
    "try:\n",
    "    import torch\n",
    "    import transformers\n",
    "    import accelerate\n",
    "    import datasets\n",
    "    import peft\n",
    "    import bitsandbytes\n",
    "    import wandb\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"‚úÖ All core libraries imported successfully\")\n",
    "    \n",
    "    # Print versions\n",
    "    print(f\"\\nLibrary versions:\")\n",
    "    print(f\"PyTorch: {torch.__version__}\")\n",
    "    print(f\"Transformers: {transformers.__version__}\")\n",
    "    print(f\"Accelerate: {accelerate.__version__}\")\n",
    "    print(f\"Datasets: {datasets.__version__}\")\n",
    "    print(f\"PEFT: {peft.__version__}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-model-access"
   },
   "outputs": [],
   "source": [
    "# Test model access (this will download tokenizer only)\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "try:\n",
    "    print(f\"Testing access to {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(f\"‚úÖ Successfully loaded tokenizer for {model_name}\")\n",
    "    print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "    \n",
    "    # Test tokenization\n",
    "    test_text = \"This is a test of insurance policy text.\"\n",
    "    tokens = tokenizer.encode(test_text)\n",
    "    print(f\"Test tokenization: '{test_text}' -> {len(tokens)} tokens\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model access failed: {e}\")\n",
    "    print(\"Make sure you have access to LLaMA models and are authenticated with Hugging Face\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "environment-summary"
   },
   "source": [
    "## 8. Environment Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "environment-summary-code"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üéØ LLaMA Insurance Fine-tuning Environment Summary\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Google Drive mounted: {os.path.exists('/content/drive')}\")\n",
    "\n",
    "# Check if config files exist\n",
    "config_files = [\n",
    "    'config/lora_config.json',\n",
    "    'config/training_args.json',\n",
    "    'config/model_card.md'\n",
    "]\n",
    "\n",
    "print(\"\\nProject files:\")\n",
    "for file in config_files:\n",
    "    exists = \"‚úÖ\" if os.path.exists(file) else \"‚ùå\"\n",
    "    print(f\"{exists} {file}\")\n",
    "\n",
    "print(\"\\nüöÄ Setup complete! You can now proceed to the next notebook.\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. 01_data_preprocessing.ipynb - Process your insurance data\")\n",
    "print(\"2. 02_tokenization.ipynb - Prepare data for training\")\n",
    "print(\"3. 03_finetuning_lora.ipynb - Fine-tune the model\")\n",
    "print(\"4. 04_evaluation.ipynb - Evaluate model performance\")\n",
    "print(\"5. 05_inference_demo.ipynb - Test the trained model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "helper-functions"
   },
   "source": [
    "## 9. Helper Functions\n",
    "\n",
    "Useful functions for managing the environment throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "helper-functions-code"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory cache\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(\"‚úÖ GPU memory cleared\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No GPU available\")\n",
    "\n",
    "def check_gpu_memory():\n",
    "    \"\"\"Check current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        \n",
    "        print(f\"GPU Memory:\")\n",
    "        print(f\"  Allocated: {allocated:.1f} GB\")\n",
    "        print(f\"  Reserved:  {reserved:.1f} GB\")\n",
    "        print(f\"  Total:     {total:.1f} GB\")\n",
    "        print(f\"  Free:      {total - reserved:.1f} GB\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No GPU available\")\n",
    "\n",
    "def backup_to_drive(source_path, drive_backup_path=None):\n",
    "    \"\"\"Backup important files to Google Drive\"\"\"\n",
    "    if drive_backup_path is None:\n",
    "        drive_backup_path = f\"/content/drive/MyDrive/llama-insurance-finetune/backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    os.makedirs(drive_backup_path, exist_ok=True)\n",
    "    !cp -r {source_path} {drive_backup_path}\n",
    "    print(f\"‚úÖ Backup created at: {drive_backup_path}\")\n",
    "\n",
    "def load_config(config_path):\n",
    "    \"\"\"Load JSON configuration file\"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_config(config, config_path):\n",
    "    \"\"\"Save configuration to JSON file\"\"\"\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    print(f\"‚úÖ Configuration saved to: {config_path}\")\n",
    "\n",
    "# Test helper functions\n",
    "print(\"üõ†Ô∏è Helper functions loaded:\")\n",
    "print(\"- clear_gpu_memory()\")\n",
    "print(\"- check_gpu_memory()\")\n",
    "print(\"- backup_to_drive(source_path)\")\n",
    "print(\"- load_config(config_path)\")\n",
    "print(\"- save_config(config, config_path)\")\n",
    "\n",
    "# Check current GPU memory\n",
    "check_gpu_memory()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
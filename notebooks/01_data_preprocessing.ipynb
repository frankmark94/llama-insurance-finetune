{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# 📊 Data Preprocessing for Insurance Fine-tuning\n",
    "\n",
    "This notebook handles the complete data preprocessing pipeline for insurance documents:\n",
    "\n",
    "## What this notebook does:\n",
    "1. Load and inspect raw insurance documents\n",
    "2. Remove PII (Personal Identifiable Information)\n",
    "3. Clean and standardize text format\n",
    "4. Create task-specific datasets (classification, QA, summarization)\n",
    "5. Split data into train/validation/test sets\n",
    "6. Validate data quality and save processed datasets\n",
    "\n",
    "**⚠️ Important: Always verify PII removal before proceeding to training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports-code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "\n",
    "# Data processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Text processing\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## 2. Configuration and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config-code"
   },
   "outputs": [],
   "source": [
    "# Directory paths\n",
    "RAW_DATA_DIR = Path(\"data/raw\")\n",
    "PROCESSED_DATA_DIR = Path(\"data/processed\")\n",
    "ANNOTATIONS_DIR = Path(\"data/annotations\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "PROCESSED_DATA_DIR.mkdir(exist_ok=True)\n",
    "ANNOTATIONS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Data split ratios\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "# Insurance task types\n",
    "TASK_TYPES = {\n",
    "    'CLAIM_CLASSIFICATION': 'Categorize insurance claims',\n",
    "    'POLICY_SUMMARIZATION': 'Summarize policy documents',\n",
    "    'FAQ_GENERATION': 'Generate FAQs from policies',\n",
    "    'COMPLIANCE_CHECK': 'Identify compliance requirements',\n",
    "    'CONTRACT_QA': 'Answer questions about contracts'\n",
    "}\n",
    "\n",
    "# Text processing parameters\n",
    "MIN_TEXT_LENGTH = 50  # Minimum character length for valid documents\n",
    "MAX_TEXT_LENGTH = 8192  # Maximum character length for model context\n",
    "MAX_SUMMARY_LENGTH = 512  # Maximum summary length\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"- Raw data directory: {RAW_DATA_DIR}\")\n",
    "print(f\"- Processed data directory: {PROCESSED_DATA_DIR}\")\n",
    "print(f\"- Task types: {list(TASK_TYPES.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pii-patterns"
   },
   "source": [
    "## 3. PII Detection and Removal Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pii-patterns-code"
   },
   "outputs": [],
   "source": [
    "class PIIRemover:\n",
    "    \"\"\"Class to handle PII detection and removal from insurance documents\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Regex patterns for common PII\n",
    "        self.patterns = {\n",
    "            'ssn': r'\\b\\d{3}-\\d{2}-\\d{4}\\b|\\b\\d{9}\\b',\n",
    "            'phone': r'\\b(?:\\+?1[-.]?)?\\(?([0-9]{3})\\)?[-.]?([0-9]{3})[-.]?([0-9]{4})\\b',\n",
    "            'email': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
    "            'zip_code': r'\\b\\d{5}(?:-\\d{4})?\\b',\n",
    "            'credit_card': r'\\b(?:\\d{4}[-\\s]?){3}\\d{4}\\b',\n",
    "            'account_number': r'\\b(?:account|acct|policy)\\s*#?\\s*\\d{6,}\\b',\n",
    "            'date_of_birth': r'\\b(?:0?[1-9]|1[0-2])[/-](?:0?[1-9]|[12]\\d|3[01])[/-](?:19|20)\\d{2}\\b',\n",
    "            'address_number': r'\\b\\d{1,5}\\s+[A-Za-z\\s]+(?:Street|St|Avenue|Ave|Road|Rd|Drive|Dr|Lane|Ln|Boulevard|Blvd)\\b',\n",
    "        }\n",
    "        \n",
    "        # Common name patterns (be careful with these)\n",
    "        self.name_indicators = [\n",
    "            r'\\bMr\\.?\\s+[A-Z][a-z]+\\s+[A-Z][a-z]+\\b',\n",
    "            r'\\bMrs\\.?\\s+[A-Z][a-z]+\\s+[A-Z][a-z]+\\b',\n",
    "            r'\\bMs\\.?\\s+[A-Z][a-z]+\\s+[A-Z][a-z]+\\b',\n",
    "            r'\\bDr\\.?\\s+[A-Z][a-z]+\\s+[A-Z][a-z]+\\b',\n",
    "        ]\n",
    "    \n",
    "    def detect_pii(self, text: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"Detect PII in text and return findings\"\"\"\n",
    "        findings = {}\n",
    "        \n",
    "        for pii_type, pattern in self.patterns.items():\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            if matches:\n",
    "                findings[pii_type] = matches\n",
    "        \n",
    "        # Check for name patterns\n",
    "        name_matches = []\n",
    "        for pattern in self.name_indicators:\n",
    "            matches = re.findall(pattern, text)\n",
    "            name_matches.extend(matches)\n",
    "        \n",
    "        if name_matches:\n",
    "            findings['names'] = name_matches\n",
    "        \n",
    "        return findings\n",
    "    \n",
    "    def remove_pii(self, text: str) -> Tuple[str, Dict[str, int]]:\n",
    "        \"\"\"Remove PII from text and return cleaned text with removal stats\"\"\"\n",
    "        cleaned_text = text\n",
    "        removal_stats = {}\n",
    "        \n",
    "        # Replace PII with generic placeholders\n",
    "        replacements = {\n",
    "            'ssn': '[SSN]',\n",
    "            'phone': '[PHONE]',\n",
    "            'email': '[EMAIL]',\n",
    "            'zip_code': '[ZIP]',\n",
    "            'credit_card': '[CARD_NUMBER]',\n",
    "            'account_number': '[ACCOUNT_NUMBER]',\n",
    "            'date_of_birth': '[DATE_OF_BIRTH]',\n",
    "            'address_number': '[ADDRESS]',\n",
    "        }\n",
    "        \n",
    "        for pii_type, replacement in replacements.items():\n",
    "            pattern = self.patterns[pii_type]\n",
    "            matches = re.findall(pattern, cleaned_text, re.IGNORECASE)\n",
    "            removal_stats[pii_type] = len(matches)\n",
    "            cleaned_text = re.sub(pattern, replacement, cleaned_text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Handle names more conservatively\n",
    "        name_count = 0\n",
    "        for pattern in self.name_indicators:\n",
    "            matches = re.findall(pattern, cleaned_text)\n",
    "            name_count += len(matches)\n",
    "            cleaned_text = re.sub(pattern, '[NAME]', cleaned_text)\n",
    "        \n",
    "        removal_stats['names'] = name_count\n",
    "        \n",
    "        return cleaned_text, removal_stats\n",
    "    \n",
    "    def validate_pii_removal(self, text: str) -> bool:\n",
    "        \"\"\"Validate that no obvious PII remains in text\"\"\"\n",
    "        findings = self.detect_pii(text)\n",
    "        return len(findings) == 0\n",
    "\n",
    "# Initialize PII remover\n",
    "pii_remover = PIIRemover()\n",
    "\n",
    "# Test PII detection on sample text\n",
    "sample_text = \"\"\"John Smith's policy number is 123456789. \n",
    "His SSN is 123-45-6789 and phone is (555) 123-4567. \n",
    "Email: john.smith@email.com. Address: 123 Main Street, Anytown 12345.\"\"\"\n",
    "\n",
    "print(\"Testing PII detection:\")\n",
    "findings = pii_remover.detect_pii(sample_text)\n",
    "print(f\"PII found: {findings}\")\n",
    "\n",
    "cleaned, stats = pii_remover.remove_pii(sample_text)\n",
    "print(f\"\\nOriginal: {sample_text}\")\n",
    "print(f\"Cleaned: {cleaned}\")\n",
    "print(f\"Removal stats: {stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "text-cleaning"
   },
   "source": [
    "## 4. Text Cleaning and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "text-cleaning-code"
   },
   "outputs": [],
   "source": [
    "class TextCleaner:\n",
    "    \"\"\"Class to handle text cleaning and standardization\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and standardize text\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove common document artifacts\n",
    "        text = self._remove_headers_footers(text)\n",
    "        \n",
    "        # Fix encoding issues\n",
    "        text = self._fix_encoding(text)\n",
    "        \n",
    "        # Normalize whitespace\n",
    "        text = self._normalize_whitespace(text)\n",
    "        \n",
    "        # Remove excessive formatting\n",
    "        text = self._remove_excessive_formatting(text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def _remove_headers_footers(self, text: str) -> str:\n",
    "        \"\"\"Remove common headers and footers\"\"\"\n",
    "        # Remove page numbers and headers\n",
    "        text = re.sub(r'^Page \\d+.*$', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'^.*Page \\d+ of \\d+.*$', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove common footer patterns\n",
    "        text = re.sub(r'^.*Confidential.*$', '', text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "        text = re.sub(r'^.*Copyright.*$', '', text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _fix_encoding(self, text: str) -> str:\n",
    "        \"\"\"Fix common encoding issues\"\"\"\n",
    "        # Fix common encoding artifacts\n",
    "        replacements = {\n",
    "            'â€™': \"'\",\n",
    "            'â€œ': '\"',\n",
    "            'â€\\x9d': '\"',\n",
    "            'â€"': '-',\n",
    "            'â€"': '—',\n",
    "            'Â': '',\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            text = text.replace(old, new)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _normalize_whitespace(self, text: str) -> str:\n",
    "        \"\"\"Normalize whitespace and line breaks\"\"\"\n",
    "        # Replace multiple spaces with single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Replace multiple line breaks with double line break\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _remove_excessive_formatting(self, text: str) -> str:\n",
    "        \"\"\"Remove excessive formatting characters\"\"\"\n",
    "        # Remove excessive punctuation\n",
    "        text = re.sub(r'[.]{3,}', '...', text)\n",
    "        text = re.sub(r'[-]{3,}', '---', text)\n",
    "        text = re.sub(r'[=]{3,}', '===', text)\n",
    "        \n",
    "        # Remove form field artifacts\n",
    "        text = re.sub(r'_{3,}', '[FIELD]', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def extract_sections(self, text: str) -> Dict[str, str]:\n",
    "        \"\"\"Extract common insurance document sections\"\"\"\n",
    "        sections = {}\n",
    "        \n",
    "        # Common section headers\n",
    "        section_patterns = {\n",
    "            'coverage': r'(coverage|benefits?)\\s*:?(.{0,2000}?)(?=\\n[A-Z][A-Za-z\\s]{10,}:|$)',\n",
    "            'exclusions': r'(exclusions?)\\s*:?(.{0,2000}?)(?=\\n[A-Z][A-Za-z\\s]{10,}:|$)',\n",
    "            'deductible': r'(deductible|copay)\\s*:?(.{0,500}?)(?=\\n[A-Z][A-Za-z\\s]{10,}:|$)',\n",
    "            'premium': r'(premium|cost|price)\\s*:?(.{0,500}?)(?=\\n[A-Z][A-Za-z\\s]{10,}:|$)',\n",
    "            'terms': r'(terms?\\s+and\\s+conditions?)\\s*:?(.{0,2000}?)(?=\\n[A-Z][A-Za-z\\s]{10,}:|$)',\n",
    "        }\n",
    "        \n",
    "        for section_name, pattern in section_patterns.items():\n",
    "            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "            if match:\n",
    "                sections[section_name] = match.group(2).strip()\n",
    "        \n",
    "        return sections\n",
    "\n",
    "# Initialize text cleaner\n",
    "text_cleaner = TextCleaner()\n",
    "\n",
    "# Test text cleaning\n",
    "sample_dirty_text = \"\"\"Page 1 of 10\n",
    "INSURANCE POLICY\n",
    "Confidential Document\n",
    "\n",
    "This    is   a   sample    policy    document.\n",
    "It   contains   multiple     spaces   and\n",
    "formatting___issues.\n",
    "\n",
    "Coverage: This policy covers medical expenses...\n",
    "\n",
    "â€™s special characters need fixing.\n",
    "\n",
    "Copyright 2024 Insurance Company\"\"\"\n",
    "\n",
    "cleaned_text = text_cleaner.clean_text(sample_dirty_text)\n",
    "print(\"Text cleaning test:\")\n",
    "print(f\"Original length: {len(sample_dirty_text)}\")\n",
    "print(f\"Cleaned length: {len(cleaned_text)}\")\n",
    "print(f\"\\nCleaned text:\\n{cleaned_text}\")\n",
    "\n",
    "# Test section extraction\n",
    "sections = text_cleaner.extract_sections(cleaned_text)\n",
    "print(f\"\\nExtracted sections: {list(sections.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load-data"
   },
   "source": [
    "## 5. Load and Inspect Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-data-code"
   },
   "outputs": [],
   "source": [
    "def load_raw_documents() -> List[Dict[str, str]]:\n",
    "    \"\"\"Load raw insurance documents from various formats\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    # Check if raw data directory exists and has files\n",
    "    if not RAW_DATA_DIR.exists():\n",
    "        print(f\"⚠️ Raw data directory {RAW_DATA_DIR} does not exist\")\n",
    "        return create_sample_data()\n",
    "    \n",
    "    # Load different file types\n",
    "    file_types = {\n",
    "        '*.txt': 'text',\n",
    "        '*.json': 'json',\n",
    "        '*.csv': 'csv'\n",
    "    }\n",
    "    \n",
    "    for pattern, file_type in file_types.items():\n",
    "        files = list(RAW_DATA_DIR.glob(pattern))\n",
    "        print(f\"Found {len(files)} {file_type} files\")\n",
    "        \n",
    "        for file_path in files:\n",
    "            try:\n",
    "                if file_type == 'text':\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                        documents.append({\n",
    "                            'id': file_path.stem,\n",
    "                            'source': str(file_path),\n",
    "                            'content': content,\n",
    "                            'type': 'document'\n",
    "                        })\n",
    "                \n",
    "                elif file_type == 'json':\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        data = json.load(f)\n",
    "                        if isinstance(data, list):\n",
    "                            documents.extend(data)\n",
    "                        else:\n",
    "                            documents.append(data)\n",
    "                \n",
    "                elif file_type == 'csv':\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    for _, row in df.iterrows():\n",
    "                        documents.append(row.to_dict())\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error loading {file_path}: {e}\")\n",
    "    \n",
    "    if not documents:\n",
    "        print(\"📝 No raw data found, creating sample insurance documents...\")\n",
    "        documents = create_sample_data()\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def create_sample_data() -> List[Dict[str, str]]:\n",
    "    \"\"\"Create sample insurance documents for testing\"\"\"\n",
    "    sample_documents = [\n",
    "        {\n",
    "            'id': 'health_policy_001',\n",
    "            'source': 'sample_data',\n",
    "            'content': '''Health Insurance Policy - Premium Coverage\n",
    "            \n",
    "Coverage: This comprehensive health insurance policy provides coverage for medical expenses including hospital stays, doctor visits, prescription medications, and emergency care. The annual coverage limit is $1,000,000 per insured individual.\n",
    "            \n",
    "Deductible: Annual deductible of $1,500 per individual, $3,000 per family. After meeting the deductible, the plan covers 80% of eligible medical expenses.\n",
    "            \n",
    "Exclusions: Pre-existing conditions diagnosed within 12 months prior to policy effective date, cosmetic procedures, experimental treatments, and services not deemed medically necessary are excluded from coverage.\n",
    "            \n",
    "Premium: Monthly premium of $450 for individual coverage, $1,200 for family coverage. Premiums are due on the first of each month.''',\n",
    "            'type': 'health_policy',\n",
    "            'task_type': 'POLICY_SUMMARIZATION'\n",
    "        },\n",
    "        {\n",
    "            'id': 'auto_claim_001',\n",
    "            'source': 'sample_data',\n",
    "            'content': '''Auto Insurance Claim - Vehicle Collision\n",
    "            \n",
    "Claim Details: Vehicle collision occurred on Highway 101 involving two vehicles. Insured vehicle sustained front-end damage requiring repair. No injuries reported. Police report filed, case number 2024-001234.\n",
    "            \n",
    "Coverage Applied: Collision coverage with $500 deductible. Estimated repair cost $3,200. Coverage approved for $2,700 after deductible.\n",
    "            \n",
    "Settlement: Claim approved and processed. Payment issued to approved repair facility. Rental car coverage provided for 5 days during repair period.''',\n",
    "            'type': 'auto_claim',\n",
    "            'task_type': 'CLAIM_CLASSIFICATION'\n",
    "        },\n",
    "        {\n",
    "            'id': 'life_policy_001',\n",
    "            'source': 'sample_data',\n",
    "            'content': '''Term Life Insurance Policy\n",
    "            \n",
    "Coverage Amount: $500,000 death benefit for 20-year term period. Coverage begins on policy effective date and remains level throughout the term.\n",
    "            \n",
    "Premium: Annual premium of $1,800, payable annually, semi-annually, or monthly. Policy includes 30-day grace period for premium payments.\n",
    "            \n",
    "Beneficiaries: Primary beneficiary designation required. Contingent beneficiaries may be named. Beneficiary changes require written notice to the insurance company.\n",
    "            \n",
    "Terms: Policy renewable at end of term subject to company approval and premium adjustment based on attained age.''',\n",
    "            'type': 'life_policy',\n",
    "            'task_type': 'CONTRACT_QA'\n",
    "        },\n",
    "        {\n",
    "            'id': 'compliance_doc_001',\n",
    "            'source': 'sample_data',\n",
    "            'content': '''Insurance Regulatory Compliance Requirements\n",
    "            \n",
    "HIPAA Compliance: All health insurance operations must comply with Health Insurance Portability and Accountability Act requirements for protecting patient health information privacy and security.\n",
    "            \n",
    "State Regulations: Insurance products must be filed with and approved by state insurance commissioners before sale. Rate changes require regulatory approval.\n",
    "            \n",
    "Consumer Protection: All marketing materials must be clear, truthful, and not misleading. Claims processing must be fair and timely according to state prompt payment laws.\n",
    "            \n",
    "Financial Requirements: Insurance companies must maintain minimum capital and surplus requirements and file annual financial statements with regulators.''',\n",
    "            'type': 'compliance',\n",
    "            'task_type': 'COMPLIANCE_CHECK'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Save sample data to files for future use\n",
    "    sample_file = RAW_DATA_DIR / 'sample_insurance_docs.json'\n",
    "    RAW_DATA_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    with open(sample_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(sample_documents, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"✅ Created sample data file: {sample_file}\")\n",
    "    return sample_documents\n",
    "\n",
    "# Load raw documents\n",
    "print(\"Loading raw insurance documents...\")\n",
    "raw_documents = load_raw_documents()\n",
    "\n",
    "print(f\"\\nLoaded {len(raw_documents)} documents\")\n",
    "print(f\"Document types: {set(doc.get('type', 'unknown') for doc in raw_documents)}\")\n",
    "\n",
    "# Display sample document\n",
    "if raw_documents:\n",
    "    sample_doc = raw_documents[0]\n",
    "    print(f\"\\nSample document:\")\n",
    "    print(f\"ID: {sample_doc.get('id', 'N/A')}\")\n",
    "    print(f\"Type: {sample_doc.get('type', 'N/A')}\")\n",
    "    print(f\"Content preview: {sample_doc.get('content', '')[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "process-documents"
   },
   "source": [
    "## 6. Process Documents Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "process-documents-code"
   },
   "outputs": [],
   "source": [
    "def process_documents(documents: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Process all documents through the complete pipeline\"\"\"\n",
    "    processed_docs = []\n",
    "    pii_stats_total = {}\n",
    "    \n",
    "    print(f\"Processing {len(documents)} documents...\")\n",
    "    \n",
    "    for doc in tqdm(documents, desc=\"Processing documents\"):\n",
    "        try:\n",
    "            # Extract content\n",
    "            content = doc.get('content', '')\n",
    "            if len(content) < MIN_TEXT_LENGTH:\n",
    "                print(f\"⚠️ Skipping document {doc.get('id', 'unknown')}: too short\")\n",
    "                continue\n",
    "            \n",
    "            # Clean text first\n",
    "            cleaned_content = text_cleaner.clean_text(content)\n",
    "            \n",
    "            # Remove PII\n",
    "            pii_free_content, pii_stats = pii_remover.remove_pii(cleaned_content)\n",
    "            \n",
    "            # Accumulate PII statistics\n",
    "            for pii_type, count in pii_stats.items():\n",
    "                pii_stats_total[pii_type] = pii_stats_total.get(pii_type, 0) + count\n",
    "            \n",
    "            # Validate PII removal\n",
    "            if not pii_remover.validate_pii_removal(pii_free_content):\n",
    "                print(f\"⚠️ Warning: PII may remain in document {doc.get('id', 'unknown')}\")\n",
    "            \n",
    "            # Truncate if too long\n",
    "            if len(pii_free_content) > MAX_TEXT_LENGTH:\n",
    "                pii_free_content = pii_free_content[:MAX_TEXT_LENGTH] + \"...\"\n",
    "            \n",
    "            # Extract sections\n",
    "            sections = text_cleaner.extract_sections(pii_free_content)\n",
    "            \n",
    "            # Create processed document\n",
    "            processed_doc = {\n",
    "                'id': doc.get('id', f\"doc_{len(processed_docs)}\"),\n",
    "                'source': doc.get('source', 'unknown'),\n",
    "                'original_length': len(content),\n",
    "                'processed_length': len(pii_free_content),\n",
    "                'content': pii_free_content,\n",
    "                'sections': sections,\n",
    "                'type': doc.get('type', 'unknown'),\n",
    "                'task_type': doc.get('task_type', 'POLICY_SUMMARIZATION'),\n",
    "                'pii_removed': sum(pii_stats.values()),\n",
    "                'processing_timestamp': datetime.now().isoformat(),\n",
    "                'content_hash': hashlib.md5(pii_free_content.encode()).hexdigest()\n",
    "            }\n",
    "            \n",
    "            processed_docs.append(processed_doc)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing document {doc.get('id', 'unknown')}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Print processing summary\n",
    "    print(f\"\\n✅ Successfully processed {len(processed_docs)} documents\")\n",
    "    print(f\"PII removal summary: {pii_stats_total}\")\n",
    "    \n",
    "    return processed_docs\n",
    "\n",
    "# Process all documents\n",
    "processed_documents = process_documents(raw_documents)\n",
    "\n",
    "# Display processing results\n",
    "if processed_documents:\n",
    "    print(f\"\\nProcessing Results:\")\n",
    "    print(f\"Total documents: {len(processed_documents)}\")\n",
    "    \n",
    "    # Length statistics\n",
    "    lengths = [doc['processed_length'] for doc in processed_documents]\n",
    "    print(f\"Content length - Min: {min(lengths)}, Max: {max(lengths)}, Avg: {np.mean(lengths):.0f}\")\n",
    "    \n",
    "    # Task type distribution\n",
    "    task_counts = {}\n",
    "    for doc in processed_documents:\n",
    "        task_type = doc['task_type']\n",
    "        task_counts[task_type] = task_counts.get(task_type, 0) + 1\n",
    "    \n",
    "    print(f\"Task type distribution: {task_counts}\")\n",
    "    \n",
    "    # Show sample processed document\n",
    "    sample = processed_documents[0]\n",
    "    print(f\"\\nSample processed document:\")\n",
    "    print(f\"ID: {sample['id']}\")\n",
    "    print(f\"Type: {sample['type']} -> {sample['task_type']}\")\n",
    "    print(f\"Length: {sample['original_length']} -> {sample['processed_length']}\")\n",
    "    print(f\"PII removed: {sample['pii_removed']} items\")\n",
    "    print(f\"Sections found: {list(sample['sections'].keys())}\")\n",
    "    print(f\"Content preview: {sample['content'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create-datasets"
   },
   "source": [
    "## 7. Create Task-Specific Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-datasets-code"
   },
   "outputs": [],
   "source": [
    "def create_task_datasets(processed_docs: List[Dict]) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"Create task-specific training datasets\"\"\"\n",
    "    task_datasets = {task: [] for task in TASK_TYPES.keys()}\n",
    "    \n",
    "    for doc in processed_docs:\n",
    "        task_type = doc['task_type']\n",
    "        content = doc['content']\n",
    "        sections = doc['sections']\n",
    "        \n",
    "        if task_type == 'CLAIM_CLASSIFICATION':\n",
    "            # Create classification examples\n",
    "            example = {\n",
    "                'text': content,\n",
    "                'label': doc['type'],\n",
    "                'instruction': 'Classify this insurance claim into the appropriate category.',\n",
    "                'input': content,\n",
    "                'output': f\"This is a {doc['type']} claim.\",\n",
    "                'task_type': task_type,\n",
    "                'doc_id': doc['id']\n",
    "            }\n",
    "            task_datasets[task_type].append(example)\n",
    "        \n",
    "        elif task_type == 'POLICY_SUMMARIZATION':\n",
    "            # Create summarization examples\n",
    "            summary = generate_summary(content, sections)\n",
    "            example = {\n",
    "                'text': content,\n",
    "                'summary': summary,\n",
    "                'instruction': 'Summarize this insurance policy document.',\n",
    "                'input': content,\n",
    "                'output': summary,\n",
    "                'task_type': task_type,\n",
    "                'doc_id': doc['id']\n",
    "            }\n",
    "            task_datasets[task_type].append(example)\n",
    "        \n",
    "        elif task_type == 'CONTRACT_QA':\n",
    "            # Create Q&A examples\n",
    "            qa_pairs = generate_qa_pairs(content, sections, doc['type'])\n",
    "            for qa in qa_pairs:\n",
    "                example = {\n",
    "                    'context': content,\n",
    "                    'question': qa['question'],\n",
    "                    'answer': qa['answer'],\n",
    "                    'instruction': 'Answer the question based on the insurance document.',\n",
    "                    'input': f\"Context: {content}\\n\\nQuestion: {qa['question']}\",\n",
    "                    'output': qa['answer'],\n",
    "                    'task_type': task_type,\n",
    "                    'doc_id': doc['id']\n",
    "                }\n",
    "                task_datasets[task_type].append(example)\n",
    "        \n",
    "        elif task_type == 'FAQ_GENERATION':\n",
    "            # Create FAQ examples\n",
    "            faqs = generate_faqs(content, sections)\n",
    "            example = {\n",
    "                'text': content,\n",
    "                'faqs': faqs,\n",
    "                'instruction': 'Generate frequently asked questions for this insurance document.',\n",
    "                'input': content,\n",
    "                'output': format_faqs(faqs),\n",
    "                'task_type': task_type,\n",
    "                'doc_id': doc['id']\n",
    "            }\n",
    "            task_datasets[task_type].append(example)\n",
    "        \n",
    "        elif task_type == 'COMPLIANCE_CHECK':\n",
    "            # Create compliance checking examples\n",
    "            compliance_items = extract_compliance_items(content)\n",
    "            example = {\n",
    "                'text': content,\n",
    "                'compliance_items': compliance_items,\n",
    "                'instruction': 'Identify compliance requirements in this insurance document.',\n",
    "                'input': content,\n",
    "                'output': format_compliance_items(compliance_items),\n",
    "                'task_type': task_type,\n",
    "                'doc_id': doc['id']\n",
    "            }\n",
    "            task_datasets[task_type].append(example)\n",
    "    \n",
    "    return task_datasets\n",
    "\n",
    "def generate_summary(content: str, sections: Dict[str, str]) -> str:\n",
    "    \"\"\"Generate a summary of the insurance document\"\"\"\n",
    "    if sections:\n",
    "        # Use sections to create structured summary\n",
    "        summary_parts = []\n",
    "        \n",
    "        if 'coverage' in sections:\n",
    "            summary_parts.append(f\"Coverage: {sections['coverage'][:200]}...\")\n",
    "        \n",
    "        if 'deductible' in sections:\n",
    "            summary_parts.append(f\"Deductible: {sections['deductible'][:100]}...\")\n",
    "        \n",
    "        if 'premium' in sections:\n",
    "            summary_parts.append(f\"Premium: {sections['premium'][:100]}...\")\n",
    "        \n",
    "        return \" \".join(summary_parts)\n",
    "    else:\n",
    "        # Simple truncation summary\n",
    "        sentences = sent_tokenize(content)\n",
    "        return \" \".join(sentences[:3]) if len(sentences) >= 3 else content[:MAX_SUMMARY_LENGTH]\n",
    "\n",
    "def generate_qa_pairs(content: str, sections: Dict[str, str], doc_type: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Generate question-answer pairs from document\"\"\"\n",
    "    qa_pairs = []\n",
    "    \n",
    "    # Common insurance questions based on document type and sections\n",
    "    if doc_type == 'health_policy':\n",
    "        qa_pairs.extend([\n",
    "            {\"question\": \"What is the annual deductible?\", \"answer\": sections.get('deductible', 'Deductible information not specified.')},\n",
    "            {\"question\": \"What does this policy cover?\", \"answer\": sections.get('coverage', 'Coverage details not specified.')},\n",
    "            {\"question\": \"What is excluded from coverage?\", \"answer\": sections.get('exclusions', 'Exclusions not specified.')}\n",
    "        ])\n",
    "    elif doc_type == 'auto_claim':\n",
    "        qa_pairs.extend([\n",
    "            {\"question\": \"What type of claim is this?\", \"answer\": f\"This is a {doc_type.replace('_', ' ')} claim.\"},\n",
    "            {\"question\": \"What was the outcome of this claim?\", \"answer\": \"Claim details are provided in the document.\"}\n",
    "        ])\n",
    "    elif doc_type == 'life_policy':\n",
    "        qa_pairs.extend([\n",
    "            {\"question\": \"What is the coverage amount?\", \"answer\": sections.get('coverage', 'Coverage amount not specified.')},\n",
    "            {\"question\": \"What are the premium terms?\", \"answer\": sections.get('premium', 'Premium terms not specified.')}\n",
    "        ])\n",
    "    \n",
    "    # Add a general question\n",
    "    qa_pairs.append({\n",
    "        \"question\": \"What is this document about?\",\n",
    "        \"answer\": f\"This document is about {doc_type.replace('_', ' ')} and contains insurance-related information.\"\n",
    "    })\n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "def generate_faqs(content: str, sections: Dict[str, str]) -> List[Dict[str, str]]:\n",
    "    \"\"\"Generate FAQs from document content\"\"\"\n",
    "    faqs = [\n",
    "        {\n",
    "            \"question\": \"What information does this document contain?\",\n",
    "            \"answer\": \"This document contains insurance policy or claim information.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How can I understand my coverage?\",\n",
    "            \"answer\": \"Review the coverage section for details about what is included in your policy.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    if 'deductible' in sections:\n",
    "        faqs.append({\n",
    "            \"question\": \"What is my deductible?\",\n",
    "            \"answer\": \"Your deductible information is detailed in the policy terms.\"\n",
    "        })\n",
    "    \n",
    "    return faqs\n",
    "\n",
    "def extract_compliance_items(content: str) -> List[str]:\n",
    "    \"\"\"Extract compliance-related items from content\"\"\"\n",
    "    compliance_keywords = [\n",
    "        'HIPAA', 'privacy', 'regulation', 'compliance', 'state law', \n",
    "        'federal law', 'requirement', 'mandatory', 'must', 'shall'\n",
    "    ]\n",
    "    \n",
    "    compliance_items = []\n",
    "    sentences = sent_tokenize(content)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if any(keyword.lower() in sentence.lower() for keyword in compliance_keywords):\n",
    "            compliance_items.append(sentence.strip())\n",
    "    \n",
    "    return compliance_items[:5]  # Limit to top 5 compliance items\n",
    "\n",
    "def format_faqs(faqs: List[Dict[str, str]]) -> str:\n",
    "    \"\"\"Format FAQs as text\"\"\"\n",
    "    formatted = []\n",
    "    for i, faq in enumerate(faqs, 1):\n",
    "        formatted.append(f\"Q{i}: {faq['question']}\\nA{i}: {faq['answer']}\")\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "def format_compliance_items(items: List[str]) -> str:\n",
    "    \"\"\"Format compliance items as text\"\"\"\n",
    "    return \"\\n\".join(f\"- {item}\" for item in items)\n",
    "\n",
    "# Create task-specific datasets\n",
    "print(\"Creating task-specific datasets...\")\n",
    "task_datasets = create_task_datasets(processed_documents)\n",
    "\n",
    "# Display dataset statistics\n",
    "print(f\"\\nTask dataset statistics:\")\n",
    "for task_type, examples in task_datasets.items():\n",
    "    print(f\"{task_type}: {len(examples)} examples\")\n",
    "    if examples:\n",
    "        sample = examples[0]\n",
    "        print(f\"  Sample input length: {len(sample['input'])}\")\n",
    "        print(f\"  Sample output length: {len(sample['output'])}\")\n",
    "        print(f\"  Sample instruction: {sample['instruction']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "split-data"
   },
   "source": [
    "## 8. Create Train/Validation/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "split-data-code"
   },
   "outputs": [],
   "source": [
    "def create_data_splits(task_datasets: Dict[str, List[Dict]]) -> Dict[str, DatasetDict]:\n",
    "    \"\"\"Create train/validation/test splits for each task\"\"\"\n",
    "    split_datasets = {}\n",
    "    \n",
    "    for task_type, examples in task_datasets.items():\n",
    "        if len(examples) == 0:\n",
    "            print(f\"⚠️ No examples for task {task_type}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Creating splits for {task_type} ({len(examples)} examples)...\")\n",
    "        \n",
    "        if len(examples) < 3:\n",
    "            # Too few examples, put all in train\n",
    "            train_examples = examples\n",
    "            val_examples = examples[:1] if examples else []\n",
    "            test_examples = examples[:1] if examples else []\n",
    "            print(f\"  ⚠️ Very few examples, using all for training\")\n",
    "        else:\n",
    "            # Split the data\n",
    "            train_examples, temp_examples = train_test_split(\n",
    "                examples, \n",
    "                test_size=(VAL_RATIO + TEST_RATIO),\n",
    "                random_state=42,\n",
    "                shuffle=True\n",
    "            )\n",
    "            \n",
    "            if len(temp_examples) >= 2:\n",
    "                val_examples, test_examples = train_test_split(\n",
    "                    temp_examples,\n",
    "                    test_size=TEST_RATIO/(VAL_RATIO + TEST_RATIO),\n",
    "                    random_state=42,\n",
    "                    shuffle=True\n",
    "                )\n",
    "            else:\n",
    "                val_examples = temp_examples[:1] if temp_examples else []\n",
    "                test_examples = temp_examples[1:] if len(temp_examples) > 1 else temp_examples[:1]\n",
    "        \n",
    "        # Create HuggingFace datasets\n",
    "        train_dataset = Dataset.from_list(train_examples)\n",
    "        val_dataset = Dataset.from_list(val_examples)\n",
    "        test_dataset = Dataset.from_list(test_examples)\n",
    "        \n",
    "        split_datasets[task_type] = DatasetDict({\n",
    "            'train': train_dataset,\n",
    "            'validation': val_dataset,\n",
    "            'test': test_dataset\n",
    "        })\n",
    "        \n",
    "        print(f\"  Train: {len(train_examples)}, Val: {len(val_examples)}, Test: {len(test_examples)}\")\n",
    "    \n",
    "    return split_datasets\n",
    "\n",
    "def create_combined_dataset(task_datasets: Dict[str, List[Dict]]) -> DatasetDict:\n",
    "    \"\"\"Create a combined dataset with all tasks for multi-task training\"\"\"\n",
    "    all_examples = []\n",
    "    \n",
    "    for task_type, examples in task_datasets.items():\n",
    "        all_examples.extend(examples)\n",
    "    \n",
    "    print(f\"Creating combined dataset with {len(all_examples)} examples...\")\n",
    "    \n",
    "    if len(all_examples) < 3:\n",
    "        # Too few examples\n",
    "        train_examples = all_examples\n",
    "        val_examples = all_examples[:1] if all_examples else []\n",
    "        test_examples = all_examples[:1] if all_examples else []\n",
    "    else:\n",
    "        # Split the combined data\n",
    "        train_examples, temp_examples = train_test_split(\n",
    "            all_examples,\n",
    "            test_size=(VAL_RATIO + TEST_RATIO),\n",
    "            random_state=42,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        if len(temp_examples) >= 2:\n",
    "            val_examples, test_examples = train_test_split(\n",
    "                temp_examples,\n",
    "                test_size=TEST_RATIO/(VAL_RATIO + TEST_RATIO),\n",
    "                random_state=42,\n",
    "                shuffle=True\n",
    "            )\n",
    "        else:\n",
    "            val_examples = temp_examples[:1] if temp_examples else []\n",
    "            test_examples = temp_examples[1:] if len(temp_examples) > 1 else temp_examples[:1]\n",
    "    \n",
    "    combined_dataset = DatasetDict({\n",
    "        'train': Dataset.from_list(train_examples),\n",
    "        'validation': Dataset.from_list(val_examples),\n",
    "        'test': Dataset.from_list(test_examples)\n",
    "    })\n",
    "    \n",
    "    print(f\"Combined dataset - Train: {len(train_examples)}, Val: {len(val_examples)}, Test: {len(test_examples)}\")\n",
    "    \n",
    "    return combined_dataset\n",
    "\n",
    "# Create data splits\n",
    "print(\"Creating data splits...\")\n",
    "split_datasets = create_data_splits(task_datasets)\n",
    "combined_dataset = create_combined_dataset(task_datasets)\n",
    "\n",
    "# Display split information\n",
    "print(f\"\\nCreated datasets for {len(split_datasets)} tasks:\")\n",
    "for task_type, dataset_dict in split_datasets.items():\n",
    "    print(f\"{task_type}:\")\n",
    "    for split_name, dataset in dataset_dict.items():\n",
    "        print(f\"  {split_name}: {len(dataset)} examples\")\n",
    "        if len(dataset) > 0:\n",
    "            sample = dataset[0]\n",
    "            print(f\"    Sample keys: {list(sample.keys())}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save-datasets"
   },
   "source": [
    "## 9. Save Processed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-datasets-code"
   },
   "outputs": [],
   "source": [
    "def save_datasets(split_datasets: Dict[str, DatasetDict], combined_dataset: DatasetDict):\n",
    "    \"\"\"Save all processed datasets to disk\"\"\"\n",
    "    \n",
    "    # Save individual task datasets\n",
    "    for task_type, dataset_dict in split_datasets.items():\n",
    "        task_dir = PROCESSED_DATA_DIR / task_type.lower()\n",
    "        task_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        print(f\"Saving {task_type} datasets to {task_dir}...\")\n",
    "        \n",
    "        for split_name, dataset in dataset_dict.items():\n",
    "            # Save as JSON\n",
    "            json_file = task_dir / f\"{split_name}.json\"\n",
    "            dataset.to_json(json_file)\n",
    "            \n",
    "            # Save as HuggingFace dataset format\n",
    "            hf_dir = task_dir / f\"{split_name}_hf\"\n",
    "            dataset.save_to_disk(hf_dir)\n",
    "            \n",
    "        print(f\"  ✅ Saved {task_type} datasets\")\n",
    "    \n",
    "    # Save combined dataset\n",
    "    combined_dir = PROCESSED_DATA_DIR / \"combined\"\n",
    "    combined_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"Saving combined dataset to {combined_dir}...\")\n",
    "    \n",
    "    for split_name, dataset in combined_dataset.items():\n",
    "        # Save as JSON\n",
    "        json_file = combined_dir / f\"{split_name}.json\"\n",
    "        dataset.to_json(json_file)\n",
    "        \n",
    "        # Save as HuggingFace dataset format\n",
    "        hf_dir = combined_dir / f\"{split_name}_hf\"\n",
    "        dataset.save_to_disk(hf_dir)\n",
    "    \n",
    "    print(f\"  ✅ Saved combined dataset\")\n",
    "    \n",
    "    # Save processing metadata\n",
    "    metadata = {\n",
    "        'processing_date': datetime.now().isoformat(),\n",
    "        'total_documents': len(processed_documents),\n",
    "        'task_counts': {task: len(examples) for task, examples in task_datasets.items()},\n",
    "        'split_ratios': {\n",
    "            'train': TRAIN_RATIO,\n",
    "            'validation': VAL_RATIO,\n",
    "            'test': TEST_RATIO\n",
    "        },\n",
    "        'processing_config': {\n",
    "            'min_text_length': MIN_TEXT_LENGTH,\n",
    "            'max_text_length': MAX_TEXT_LENGTH,\n",
    "            'max_summary_length': MAX_SUMMARY_LENGTH\n",
    "        },\n",
    "        'task_types': TASK_TYPES\n",
    "    }\n",
    "    \n",
    "    metadata_file = PROCESSED_DATA_DIR / \"processing_metadata.json\"\n",
    "    with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"  ✅ Saved processing metadata to {metadata_file}\")\n",
    "\n",
    "def create_data_summary():\n",
    "    \"\"\"Create a summary of the processed data\"\"\"\n",
    "    summary = {\n",
    "        'overview': {\n",
    "            'total_raw_documents': len(raw_documents),\n",
    "            'total_processed_documents': len(processed_documents),\n",
    "            'processing_date': datetime.now().isoformat()\n",
    "        },\n",
    "        'task_datasets': {},\n",
    "        'combined_dataset': {\n",
    "            'train': len(combined_dataset['train']),\n",
    "            'validation': len(combined_dataset['validation']),\n",
    "            'test': len(combined_dataset['test'])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for task_type, dataset_dict in split_datasets.items():\n",
    "        summary['task_datasets'][task_type] = {\n",
    "            'train': len(dataset_dict['train']),\n",
    "            'validation': len(dataset_dict['validation']),\n",
    "            'test': len(dataset_dict['test'])\n",
    "        }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Save all datasets\n",
    "save_datasets(split_datasets, combined_dataset)\n",
    "\n",
    "# Create and display summary\n",
    "data_summary = create_data_summary()\n",
    "print(f\"\\n📊 Data Processing Summary:\")\n",
    "print(json.dumps(data_summary, indent=2))\n",
    "\n",
    "# Save summary\n",
    "summary_file = PROCESSED_DATA_DIR / \"data_summary.json\"\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n✅ Data preprocessing complete!\")\n",
    "print(f\"\\nProcessed data saved to: {PROCESSED_DATA_DIR}\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"1. Review the processed datasets in {PROCESSED_DATA_DIR}\")\n",
    "print(f\"2. Run 02_tokenization.ipynb to prepare data for training\")\n",
    "print(f\"3. Proceed to 03_finetuning_lora.ipynb for model training\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ðŸ“Š Data Preprocessing for Insurance Fine-tuning\n",
    "\n",
    "This notebook handles the complete data preprocessing pipeline for insurance documents:\n",
    "\n",
    "## What this notebook does:\n",
    "1. Load and inspect raw insurance documents\n",
    "2. Remove PII (Personal Identifiable Information)\n",
    "3. Clean and standardize text format\n",
    "4. Create task-specific datasets (classification, QA, summarization)\n",
    "5. Split data into train/validation/test sets\n",
    "6. Validate data quality and save processed datasets\n",
    "\n",
    "**âš ï¸ Important: Always verify PII removal before proceeding to training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports-code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "\n",
    "# Data processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Text processing\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## 2. Configuration and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config-code"
   },
   "outputs": [],
   "source": [
    "# Directory paths\n",
    "RAW_DATA_DIR = Path(\"data/raw\")\n",
    "PROCESSED_DATA_DIR = Path(\"data/processed\")\n",
    "ANNOTATIONS_DIR = Path(\"data/annotations\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "PROCESSED_DATA_DIR.mkdir(exist_ok=True)\n",
    "ANNOTATIONS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Data split ratios\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "# Insurance task types\n",
    "TASK_TYPES = {\n",
    "    'CLAIM_CLASSIFICATION': 'Categorize insurance claims',\n",
    "    'POLICY_SUMMARIZATION': 'Summarize policy documents',\n",
    "    'FAQ_GENERATION': 'Generate FAQs from policies',\n",
    "    'COMPLIANCE_CHECK': 'Identify compliance requirements',\n",
    "    'CONTRACT_QA': 'Answer questions about contracts'\n",
    "}\n",
    "\n",
    "# Text processing parameters\n",
    "MIN_TEXT_LENGTH = 50  # Minimum character length for valid documents\n",
    "MAX_TEXT_LENGTH = 8192  # Maximum character length for model context\n",
    "MAX_SUMMARY_LENGTH = 512  # Maximum summary length\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"- Raw data directory: {RAW_DATA_DIR}\")\n",
    "print(f\"- Processed data directory: {PROCESSED_DATA_DIR}\")\n",
    "print(f\"- Task types: {list(TASK_TYPES.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pii-patterns"
   },
   "source": [
    "## 3. PII Detection and Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pii-patterns-code"
   },
   "outputs": [],
   "source": [
    "class PIIRemover:\n",
    "    \"\"\"Class to handle PII detection and removal from insurance documents\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Regex patterns for common PII\n",
    "        self.patterns = {\n",
    "            'ssn': r'\\\\b\\\\d{3}-\\\\d{2}-\\\\d{4}\\\\b|\\\\b\\\\d{9}\\\\b',\n",
    "            'phone': r'\\\\b(?:\\\\+?1[-.]?)?\\\\(?([0-9]{3})\\\\)?[-.]?([0-9]{3})[-.]?([0-9]{4})\\\\b',\n",
    "            'email': r'\\\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Z|a-z]{2,}\\\\b',\n",
    "            'zip_code': r'\\\\b\\\\d{5}(?:-\\\\d{4})?\\\\b',\n",
    "            'credit_card': r'\\\\b(?:\\\\d{4}[-\\\\s]?){3}\\\\d{4}\\\\b',\n",
    "            'account_number': r'\\\\b(?:account|acct|policy)\\\\s*#?\\\\s*\\\\d{6,}\\\\b',\n",
    "            'date_of_birth': r'\\\\b(?:0?[1-9]|1[0-2])[/-](?:0?[1-9]|[12]\\\\d|3[01])[/-](?:19|20)\\\\d{2}\\\\b',\n",
    "            'address_number': r'\\\\b\\\\d{1,5}\\\\s+[A-Za-z\\\\s]+(?:Street|St|Avenue|Ave|Road|Rd|Drive|Dr|Lane|Ln|Boulevard|Blvd)\\\\b',\n",
    "        }\n",
    "    \n",
    "    def detect_pii(self, text: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"Detect PII in text and return findings\"\"\"\n",
    "        findings = {}\n",
    "        \n",
    "        for pii_type, pattern in self.patterns.items():\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            if matches:\n",
    "                findings[pii_type] = matches\n",
    "        \n",
    "        return findings\n",
    "    \n",
    "    def remove_pii(self, text: str) -> Tuple[str, Dict[str, int]]:\n",
    "        \"\"\"Remove PII from text and return cleaned text with removal stats\"\"\"\n",
    "        cleaned_text = text\n",
    "        removal_stats = {}\n",
    "        \n",
    "        # Replace PII with generic placeholders\n",
    "        replacements = {\n",
    "            'ssn': '[SSN]',\n",
    "            'phone': '[PHONE]',\n",
    "            'email': '[EMAIL]',\n",
    "            'zip_code': '[ZIP]',\n",
    "            'credit_card': '[CARD_NUMBER]',\n",
    "            'account_number': '[ACCOUNT_NUMBER]',\n",
    "            'date_of_birth': '[DATE_OF_BIRTH]',\n",
    "            'address_number': '[ADDRESS]',\n",
    "        }\n",
    "        \n",
    "        for pii_type, replacement in replacements.items():\n",
    "            pattern = self.patterns[pii_type]\n",
    "            matches = re.findall(pattern, cleaned_text, re.IGNORECASE)\n",
    "            removal_stats[pii_type] = len(matches)\n",
    "            cleaned_text = re.sub(pattern, replacement, cleaned_text, flags=re.IGNORECASE)\n",
    "        \n",
    "        return cleaned_text, removal_stats\n",
    "\n",
    "# Test PII removal\n",
    "pii_remover = PIIRemover()\n",
    "sample_text = \"John Smith's SSN is 123-45-6789 and phone is (555) 123-4567.\"\n",
    "cleaned, stats = pii_remover.remove_pii(sample_text)\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Cleaned: {cleaned}\")\n",
    "print(f\"Stats: {stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create-sample-data"
   },
   "source": [
    "## 4. Create Sample Insurance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-sample-data-code"
   },
   "outputs": [],
   "source": [
    "def create_sample_insurance_data():\n",
    "    \"\"\"Create sample insurance documents for testing\"\"\"\n",
    "    \n",
    "    sample_documents = [\n",
    "        {\n",
    "            'id': 'health_policy_001',\n",
    "            'source': 'sample_data',\n",
    "            'content': '''Health Insurance Policy - Premium Coverage\n",
    "            \n",
    "Coverage: This comprehensive health insurance policy provides coverage for medical expenses including hospital stays, doctor visits, prescription medications, and emergency care. The annual coverage limit is $1,000,000 per insured individual.\n",
    "            \n",
    "Deductible: Annual deductible of $1,500 per individual, $3,000 per family. After meeting the deductible, the plan covers 80% of eligible medical expenses.\n",
    "            \n",
    "Exclusions: Pre-existing conditions diagnosed within 12 months prior to policy effective date, cosmetic procedures, experimental treatments, and services not deemed medically necessary are excluded from coverage.\n",
    "            \n",
    "Premium: Monthly premium of $450 for individual coverage, $1,200 for family coverage. Premiums are due on the first of each month.''',\n",
    "            'type': 'health_policy',\n",
    "            'task_type': 'POLICY_SUMMARIZATION'\n",
    "        },\n",
    "        {\n",
    "            'id': 'auto_claim_001',\n",
    "            'source': 'sample_data',\n",
    "            'content': '''Auto Insurance Claim - Vehicle Collision\n",
    "            \n",
    "Claim Details: Vehicle collision occurred on Highway 101 involving two vehicles. Insured vehicle sustained front-end damage requiring repair. No injuries reported. Police report filed, case number 2024-001234.\n",
    "            \n",
    "Coverage Applied: Collision coverage with $500 deductible. Estimated repair cost $3,200. Coverage approved for $2,700 after deductible.\n",
    "            \n",
    "Settlement: Claim approved and processed. Payment issued to approved repair facility. Rental car coverage provided for 5 days during repair period.''',\n",
    "            'type': 'auto_claim',\n",
    "            'task_type': 'CLAIM_CLASSIFICATION'\n",
    "        },\n",
    "        {\n",
    "            'id': 'compliance_doc_001',\n",
    "            'source': 'sample_data',\n",
    "            'content': '''Insurance Regulatory Compliance Requirements\n",
    "            \n",
    "HIPAA Compliance: All health insurance operations must comply with Health Insurance Portability and Accountability Act requirements for protecting patient health information privacy and security.\n",
    "            \n",
    "State Regulations: Insurance products must be filed with and approved by state insurance commissioners before sale. Rate changes require regulatory approval.\n",
    "            \n",
    "Consumer Protection: All marketing materials must be clear, truthful, and not misleading. Claims processing must be fair and timely according to state prompt payment laws.''',\n",
    "            'type': 'compliance',\n",
    "            'task_type': 'COMPLIANCE_CHECK'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Save sample data to files\n",
    "    RAW_DATA_DIR.mkdir(exist_ok=True)\n",
    "    sample_file = RAW_DATA_DIR / 'sample_insurance_docs.json'\n",
    "    \n",
    "    with open(sample_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(sample_documents, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"âœ… Created sample data file: {sample_file}\")\n",
    "    return sample_documents\n",
    "\n",
    "# Create sample data\n",
    "sample_docs = create_sample_insurance_data()\n",
    "print(f\"Created {len(sample_docs)} sample documents\")\n",
    "for doc in sample_docs:\n",
    "    print(f\"- {doc['id']}: {doc['type']} ({doc['task_type']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "process-and-save"
   },
   "source": [
    "## 5. Process Data and Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "process-and-save-code"
   },
   "outputs": [],
   "source": [
    "def process_and_create_datasets(documents):\n",
    "    \"\"\"Process documents and create task-specific datasets\"\"\"\n",
    "    \n",
    "    processed_examples = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Clean text and remove PII\n",
    "        content = doc['content']\n",
    "        cleaned_content, pii_stats = pii_remover.remove_pii(content)\n",
    "        \n",
    "        task_type = doc['task_type']\n",
    "        \n",
    "        if task_type == 'POLICY_SUMMARIZATION':\n",
    "            # Create summarization example\n",
    "            summary = f\"This {doc['type']} document covers key insurance terms including coverage details, deductibles, and important policy information.\"\n",
    "            \n",
    "            example = {\n",
    "                'instruction': 'Summarize the following insurance policy document.',\n",
    "                'input': cleaned_content,\n",
    "                'output': summary,\n",
    "                'task_type': task_type,\n",
    "                'doc_id': doc['id']\n",
    "            }\n",
    "            processed_examples.append(example)\n",
    "        \n",
    "        elif task_type == 'CLAIM_CLASSIFICATION':\n",
    "            # Create classification example\n",
    "            example = {\n",
    "                'instruction': 'Classify this insurance claim into the appropriate category.',\n",
    "                'input': cleaned_content,\n",
    "                'output': f\"This is a {doc['type']} claim.\",\n",
    "                'task_type': task_type,\n",
    "                'doc_id': doc['id']\n",
    "            }\n",
    "            processed_examples.append(example)\n",
    "        \n",
    "        elif task_type == 'COMPLIANCE_CHECK':\n",
    "            # Create compliance checking example\n",
    "            example = {\n",
    "                'instruction': 'Identify compliance requirements in this insurance document.',\n",
    "                'input': cleaned_content,\n",
    "                'output': 'Key compliance requirements include HIPAA privacy protections, state regulatory approvals, and consumer protection standards.',\n",
    "                'task_type': task_type,\n",
    "                'doc_id': doc['id']\n",
    "            }\n",
    "            processed_examples.append(example)\n",
    "    \n",
    "    return processed_examples\n",
    "\n",
    "def create_train_test_splits(examples):\n",
    "    \"\"\"Create train/validation/test splits\"\"\"\n",
    "    \n",
    "    if len(examples) < 3:\n",
    "        # Too few examples, use all for training\n",
    "        return {\n",
    "            'train': examples,\n",
    "            'validation': examples[:1] if examples else [],\n",
    "            'test': examples[:1] if examples else []\n",
    "        }\n",
    "    \n",
    "    # Split the data\n",
    "    train_examples, temp_examples = train_test_split(\n",
    "        examples, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    val_examples, test_examples = train_test_split(\n",
    "        temp_examples, test_size=0.5, random_state=42\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'train': train_examples,\n",
    "        'validation': val_examples,\n",
    "        'test': test_examples\n",
    "    }\n",
    "\n",
    "def save_datasets(data_splits):\n",
    "    \"\"\"Save processed datasets\"\"\"\n",
    "    \n",
    "    # Create combined dataset\n",
    "    combined_dir = PROCESSED_DATA_DIR / 'combined'\n",
    "    combined_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    for split_name, examples in data_splits.items():\n",
    "        # Save as JSON\n",
    "        json_file = combined_dir / f\"{split_name}.json\"\n",
    "        with open(json_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(examples, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Save as HuggingFace dataset\n",
    "        if examples:  # Only create dataset if we have examples\n",
    "            dataset = Dataset.from_list(examples)\n",
    "            hf_dir = combined_dir / f\"{split_name}_hf\"\n",
    "            dataset.save_to_disk(hf_dir)\n",
    "        \n",
    "        print(f\"âœ… Saved {split_name}: {len(examples)} examples\")\n",
    "    \n",
    "    # Save processing metadata\n",
    "    metadata = {\n",
    "        'processing_date': datetime.now().isoformat(),\n",
    "        'total_examples': sum(len(examples) for examples in data_splits.values()),\n",
    "        'splits': {split: len(examples) for split, examples in data_splits.items()},\n",
    "        'task_types': list(TASK_TYPES.keys())\n",
    "    }\n",
    "    \n",
    "    metadata_file = combined_dir / 'metadata.json'\n",
    "    with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"âœ… Saved metadata: {metadata_file}\")\n",
    "\n",
    "# Process documents and create datasets\n",
    "print(\"Processing documents and creating datasets...\")\n",
    "processed_examples = process_and_create_datasets(sample_docs)\n",
    "\n",
    "print(f\"\\nCreated {len(processed_examples)} training examples:\")\n",
    "for example in processed_examples:\n",
    "    print(f\"- {example['doc_id']}: {example['task_type']}\")\n",
    "\n",
    "# Create train/test splits\n",
    "data_splits = create_train_test_splits(processed_examples)\n",
    "\n",
    "print(f\"\\nData splits created:\")\n",
    "for split_name, examples in data_splits.items():\n",
    "    print(f\"- {split_name}: {len(examples)} examples\")\n",
    "\n",
    "# Save datasets\n",
    "save_datasets(data_splits)\n",
    "\n",
    "print(f\"\\nâœ… Data preprocessing complete!\")\n",
    "print(f\"Processed data saved to: {PROCESSED_DATA_DIR}\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"1. Review the processed datasets\")\n",
    "print(f\"2. Run 02_tokenization.ipynb to prepare data for training\")\n",
    "print(f\"3. Proceed to 03_finetuning_lora.ipynb for model training\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üî§ Tokenization for LLaMA Insurance Fine-tuning\n",
    "\n",
    "This notebook handles tokenization and data formatting for LLaMA model training:\n",
    "\n",
    "## What this notebook does:\n",
    "1. Load and configure the LLaMA tokenizer\n",
    "2. Format datasets for instruction tuning\n",
    "3. Tokenize training data with proper attention masks\n",
    "4. Handle context length and padding\n",
    "5. Create data loaders for training\n",
    "6. Validate tokenization quality\n",
    "\n",
    "**‚ö†Ô∏è Important: Make sure you have access to LLaMA models through Hugging Face**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports-code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Transformers and datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## 2. Configuration and Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config-code"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"  # or \"meta-llama/Llama-2-7b-hf\"\n",
    "PROCESSED_DATA_DIR = Path(\"data/processed\")\n",
    "TOKENIZED_DATA_DIR = Path(\"data/tokenized\")\n",
    "TOKENIZED_DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Tokenization parameters\n",
    "MAX_LENGTH = 2048  # Maximum sequence length for training\n",
    "PADDING_SIDE = \"right\"  # For causal LM, use right padding\n",
    "TRUNCATION = True\n",
    "ADD_EOS_TOKEN = True\n",
    "\n",
    "# Instruction formatting\n",
    "INSTRUCTION_TEMPLATE = {\n",
    "    'system': \"You are a helpful AI assistant specialized in insurance and financial services. Provide accurate, helpful, and compliant information.\",\n",
    "    'user_prefix': \"[INST]\",\n",
    "    'user_suffix': \"[/INST]\",\n",
    "    'assistant_prefix': \"\",\n",
    "    'assistant_suffix': \"</s>\"\n",
    "}\n",
    "\n",
    "# Task-specific prompts\n",
    "TASK_PROMPTS = {\n",
    "    'CLAIM_CLASSIFICATION': \"Classify the following insurance claim into the appropriate category:\",\n",
    "    'POLICY_SUMMARIZATION': \"Summarize the following insurance policy document:\",\n",
    "    'FAQ_GENERATION': \"Generate frequently asked questions for the following insurance document:\",\n",
    "    'COMPLIANCE_CHECK': \"Identify compliance requirements in the following insurance document:\",\n",
    "    'CONTRACT_QA': \"Answer the following question based on the insurance document provided:\"\n",
    "}\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"- Model: {MODEL_NAME}\")\n",
    "print(f\"- Max length: {MAX_LENGTH}\")\n",
    "print(f\"- Processed data: {PROCESSED_DATA_DIR}\")\n",
    "print(f\"- Tokenized output: {TOKENIZED_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load-tokenizer"
   },
   "source": [
    "## 3. Load and Configure Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-tokenizer-code"
   },
   "outputs": [],
   "source": [
    "def setup_tokenizer(model_name: str) -> AutoTokenizer:\n",
    "    \"\"\"Load and configure the LLaMA tokenizer\"\"\"\n",
    "    print(f\"Loading tokenizer for {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            use_fast=True,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=PADDING_SIDE\n",
    "        )\n",
    "        \n",
    "        # Set special tokens\n",
    "        if tokenizer.pad_token is None:\n",
    "            if tokenizer.eos_token is not None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            else:\n",
    "                tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        \n",
    "        # Ensure we have all required special tokens\n",
    "        special_tokens = {\n",
    "            'bos_token': '<s>',\n",
    "            'eos_token': '</s>',\n",
    "            'unk_token': '<unk>',\n",
    "        }\n",
    "        \n",
    "        tokens_to_add = {}\n",
    "        for token_name, token_value in special_tokens.items():\n",
    "            if getattr(tokenizer, token_name) is None:\n",
    "                tokens_to_add[token_name] = token_value\n",
    "        \n",
    "        if tokens_to_add:\n",
    "            tokenizer.add_special_tokens(tokens_to_add)\n",
    "        \n",
    "        print(f\"‚úÖ Tokenizer loaded successfully\")\n",
    "        print(f\"  Vocab size: {len(tokenizer)}\")\n",
    "        print(f\"  Pad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "        print(f\"  EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "        print(f\"  BOS token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})\")\n",
    "        \n",
    "        return tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading tokenizer: {e}\")\n",
    "        print(\"Make sure you're authenticated with Hugging Face and have access to LLaMA models\")\n",
    "        raise\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = setup_tokenizer(MODEL_NAME)\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"This is a test of the LLaMA tokenizer for insurance documents.\"\n",
    "test_tokens = tokenizer.encode(test_text)\n",
    "decoded_text = tokenizer.decode(test_tokens)\n",
    "\n",
    "print(f\"\\nTokenization test:\")\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Tokens: {test_tokens} ({len(test_tokens)} tokens)\")\n",
    "print(f\"Decoded: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load-data"
   },
   "source": [
    "## 4. Load Processed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-data-code"
   },
   "outputs": [],
   "source": [
    "def load_processed_datasets() -> Dict[str, DatasetDict]:\n",
    "    \"\"\"Load processed datasets from disk\"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    print(f\"Loading processed datasets from {PROCESSED_DATA_DIR}...\")\n",
    "    \n",
    "    # Check for combined dataset first\n",
    "    combined_dir = PROCESSED_DATA_DIR / \"combined\"\n",
    "    if combined_dir.exists():\n",
    "        print(f\"Loading combined dataset...\")\n",
    "        try:\n",
    "            combined_dataset = DatasetDict()\n",
    "            for split in ['train', 'validation', 'test']:\n",
    "                json_file = combined_dir / f\"{split}.json\"\n",
    "                if json_file.exists():\n",
    "                    dataset = Dataset.from_json(str(json_file))\n",
    "                    combined_dataset[split] = dataset\n",
    "                    print(f\"  {split}: {len(dataset)} examples\")\n",
    "            \n",
    "            if combined_dataset:\n",
    "                datasets['combined'] = combined_dataset\n",
    "                print(f\"‚úÖ Combined dataset loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading combined dataset: {e}\")\n",
    "    \n",
    "    # Load individual task datasets\n",
    "    task_dirs = [d for d in PROCESSED_DATA_DIR.iterdir() if d.is_dir() and d.name != 'combined']\n",
    "    \n",
    "    for task_dir in task_dirs:\n",
    "        task_name = task_dir.name.upper()\n",
    "        print(f\"Loading {task_name} dataset...\")\n",
    "        \n",
    "        try:\n",
    "            task_dataset = DatasetDict()\n",
    "            for split in ['train', 'validation', 'test']:\n",
    "                json_file = task_dir / f\"{split}.json\"\n",
    "                if json_file.exists():\n",
    "                    dataset = Dataset.from_json(str(json_file))\n",
    "                    task_dataset[split] = dataset\n",
    "                    print(f\"  {split}: {len(dataset)} examples\")\n",
    "            \n",
    "            if task_dataset:\n",
    "                datasets[task_name] = task_dataset\n",
    "                print(f\"‚úÖ {task_name} dataset loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {task_name} dataset: {e}\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Load datasets\n",
    "datasets = load_processed_datasets()\n",
    "\n",
    "if not datasets:\n",
    "    print(\"‚ùå No datasets found. Please run 01_data_preprocessing.ipynb first.\")\n",
    "else:\n",
    "    print(f\"\\nLoaded {len(datasets)} datasets:\")\n",
    "    for name, dataset_dict in datasets.items():\n",
    "        total_examples = sum(len(dataset_dict[split]) for split in dataset_dict.keys())\n",
    "        print(f\"  {name}: {total_examples} total examples\")\n",
    "        \n",
    "        # Show sample from first dataset\n",
    "        if 'train' in dataset_dict and len(dataset_dict['train']) > 0:\n",
    "            sample = dataset_dict['train'][0]\n",
    "            print(f\"    Sample keys: {list(sample.keys())}\")\n",
    "            if 'instruction' in sample:\n",
    "                print(f\"    Sample instruction: {sample['instruction'][:100]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "format-instructions"
   },
   "source": [
    "## 5. Format Data for Instruction Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "format-instructions-code"
   },
   "outputs": [],
   "source": [
    "def format_instruction(example: Dict[str, Any]) -> str:\n",
    "    \"\"\"Format example into instruction-following format for LLaMA\"\"\"\n",
    "    \n",
    "    # Get the task-specific prompt\n",
    "    task_type = example.get('task_type', 'POLICY_SUMMARIZATION')\n",
    "    task_prompt = TASK_PROMPTS.get(task_type, \"Complete the following task:\")\n",
    "    \n",
    "    # Build the instruction\n",
    "    if 'instruction' in example and example['instruction']:\n",
    "        instruction = example['instruction']\n",
    "    else:\n",
    "        instruction = task_prompt\n",
    "    \n",
    "    # Get input and output\n",
    "    user_input = example.get('input', '')\n",
    "    assistant_output = example.get('output', '')\n",
    "    \n",
    "    # Handle different task types\n",
    "    if task_type == 'CONTRACT_QA' and 'question' in example:\n",
    "        # For Q&A, format with context and question\n",
    "        context = example.get('context', '')\n",
    "        question = example.get('question', '')\n",
    "        user_input = f\"Context: {context}\\n\\nQuestion: {question}\"\n",
    "        assistant_output = example.get('answer', assistant_output)\n",
    "    \n",
    "    # Format in LLaMA chat format\n",
    "    formatted_text = f\"{INSTRUCTION_TEMPLATE['user_prefix']} {instruction}\\n\\n{user_input} {INSTRUCTION_TEMPLATE['user_suffix']} {assistant_output}{INSTRUCTION_TEMPLATE['assistant_suffix']}\"\n",
    "    \n",
    "    return formatted_text\n",
    "\n",
    "def format_dataset_for_training(dataset: Dataset) -> Dataset:\n",
    "    \"\"\"Format entire dataset for instruction tuning\"\"\"\n",
    "    \n",
    "    def format_example(example):\n",
    "        formatted_text = format_instruction(example)\n",
    "        return {\n",
    "            'text': formatted_text,\n",
    "            'task_type': example.get('task_type', 'POLICY_SUMMARIZATION'),\n",
    "            'original_id': example.get('doc_id', 'unknown')\n",
    "        }\n",
    "    \n",
    "    formatted_dataset = dataset.map(\n",
    "        format_example,\n",
    "        remove_columns=[col for col in dataset.column_names if col not in ['task_type', 'doc_id']]\n",
    "    )\n",
    "    \n",
    "    return formatted_dataset\n",
    "\n",
    "# Test instruction formatting\n",
    "if datasets and 'combined' in datasets:\n",
    "    sample_dataset = datasets['combined']['train']\n",
    "    if len(sample_dataset) > 0:\n",
    "        sample_example = sample_dataset[0]\n",
    "        formatted_sample = format_instruction(sample_example)\n",
    "        \n",
    "        print(\"Instruction formatting test:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(formatted_sample)\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Formatted length: {len(formatted_sample)} characters\")\n",
    "        \n",
    "        # Check tokenization length\n",
    "        tokens = tokenizer.encode(formatted_sample)\n",
    "        print(f\"Token count: {len(tokens)} tokens\")\n",
    "        \n",
    "        if len(tokens) > MAX_LENGTH:\n",
    "            print(f\"‚ö†Ô∏è Warning: Example exceeds max length ({MAX_LENGTH} tokens)\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Example fits within max length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tokenize-datasets"
   },
   "source": [
    "## 6. Tokenize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tokenize-datasets-code"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples: Dict[str, List[str]]) -> Dict[str, List[List[int]]]:\n",
    "    \"\"\"Tokenize a batch of examples\"\"\"\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=TRUNCATION,\n",
    "        padding=False,  # We'll pad dynamically during training\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=None,  # Return lists, not tensors\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # For causal LM, labels are the same as input_ids\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "    \n",
    "    # Calculate lengths for filtering\n",
    "    tokenized['length'] = [len(ids) for ids in tokenized['input_ids']]\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "def tokenize_dataset(dataset: Dataset, dataset_name: str) -> Dataset:\n",
    "    \"\"\"Tokenize an entire dataset\"\"\"\n",
    "    print(f\"Tokenizing {dataset_name} dataset ({len(dataset)} examples)...\")\n",
    "    \n",
    "    # First format for instruction tuning\n",
    "    formatted_dataset = format_dataset_for_training(dataset)\n",
    "    \n",
    "    # Then tokenize\n",
    "    tokenized_dataset = formatted_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=formatted_dataset.column_names,\n",
    "        desc=f\"Tokenizing {dataset_name}\"\n",
    "    )\n",
    "    \n",
    "    # Filter out examples that are too long or too short\n",
    "    def filter_length(example):\n",
    "        length = example['length']\n",
    "        return 10 <= length <= MAX_LENGTH  # Filter very short and very long examples\n",
    "    \n",
    "    filtered_dataset = tokenized_dataset.filter(filter_length)\n",
    "    \n",
    "    print(f\"  Original: {len(tokenized_dataset)} examples\")\n",
    "    print(f\"  After filtering: {len(filtered_dataset)} examples\")\n",
    "    \n",
    "    if len(filtered_dataset) > 0:\n",
    "        lengths = [ex['length'] for ex in filtered_dataset]\n",
    "        print(f\"  Length stats - Min: {min(lengths)}, Max: {max(lengths)}, Avg: {np.mean(lengths):.1f}\")\n",
    "    \n",
    "    return filtered_dataset\n",
    "\n",
    "def tokenize_all_datasets(datasets: Dict[str, DatasetDict]) -> Dict[str, DatasetDict]:\n",
    "    \"\"\"Tokenize all datasets\"\"\"\n",
    "    tokenized_datasets = {}\n",
    "    \n",
    "    for dataset_name, dataset_dict in datasets.items():\n",
    "        print(f\"\\nProcessing {dataset_name} dataset...\")\n",
    "        tokenized_dict = DatasetDict()\n",
    "        \n",
    "        for split_name, dataset in dataset_dict.items():\n",
    "            if len(dataset) > 0:\n",
    "                tokenized_split = tokenize_dataset(dataset, f\"{dataset_name}_{split_name}\")\n",
    "                tokenized_dict[split_name] = tokenized_split\n",
    "            else:\n",
    "                print(f\"  Skipping empty {split_name} split\")\n",
    "        \n",
    "        if tokenized_dict:\n",
    "            tokenized_datasets[dataset_name] = tokenized_dict\n",
    "    \n",
    "    return tokenized_datasets\n",
    "\n",
    "# Tokenize all datasets\n",
    "if datasets:\n",
    "    print(\"Starting tokenization process...\")\n",
    "    tokenized_datasets = tokenize_all_datasets(datasets)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Tokenization complete!\")\n",
    "    print(f\"Tokenized {len(tokenized_datasets)} datasets:\")\n",
    "    \n",
    "    for name, dataset_dict in tokenized_datasets.items():\n",
    "        total_examples = sum(len(dataset_dict[split]) for split in dataset_dict.keys())\n",
    "        print(f\"  {name}: {total_examples} total examples\")\n",
    "        \n",
    "        # Show sample tokenized example\n",
    "        if 'train' in dataset_dict and len(dataset_dict['train']) > 0:\n",
    "            sample = dataset_dict['train'][0]\n",
    "            print(f\"    Sample keys: {list(sample.keys())}\")\n",
    "            print(f\"    Input length: {len(sample['input_ids'])} tokens\")\n",
    "            print(f\"    First 10 tokens: {sample['input_ids'][:10]}\")\n",
    "else:\n",
    "    print(\"‚ùå No datasets available for tokenization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data-collator"
   },
   "source": [
    "## 7. Create Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data-collator-code"
   },
   "outputs": [],
   "source": [
    "class InstructionDataCollator:\n",
    "    \"\"\"Custom data collator for instruction tuning\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, max_length=MAX_LENGTH):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        # Extract input_ids and labels\n",
    "        input_ids = [f['input_ids'] for f in features]\n",
    "        labels = [f['labels'] for f in features]\n",
    "        \n",
    "        # Pad sequences\n",
    "        batch = self.tokenizer.pad(\n",
    "            {'input_ids': input_ids},\n",
    "            padding=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Pad labels as well\n",
    "        labels_batch = self.tokenizer.pad(\n",
    "            {'input_ids': labels},\n",
    "            padding=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Replace padding token id's in labels with -100 so they're ignored in loss\n",
    "        labels_batch['input_ids'][labels_batch['input_ids'] == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        batch['labels'] = labels_batch['input_ids']\n",
    "        \n",
    "        return batch\n",
    "\n",
    "# Create data collator\n",
    "data_collator = InstructionDataCollator(tokenizer, max_length=MAX_LENGTH)\n",
    "\n",
    "print(f\"‚úÖ Data collator created\")\n",
    "print(f\"  Max length: {MAX_LENGTH}\")\n",
    "print(f\"  Pad token ID: {tokenizer.pad_token_id}\")\n",
    "\n",
    "# Test data collator with a small batch\n",
    "if tokenized_datasets and 'combined' in tokenized_datasets:\n",
    "    test_dataset = tokenized_datasets['combined']['train']\n",
    "    if len(test_dataset) >= 2:\n",
    "        test_batch = [test_dataset[i] for i in range(2)]\n",
    "        collated_batch = data_collator(test_batch)\n",
    "        \n",
    "        print(f\"\\nData collator test:\")\n",
    "        print(f\"  Batch keys: {list(collated_batch.keys())}\")\n",
    "        print(f\"  Input IDs shape: {collated_batch['input_ids'].shape}\")\n",
    "        print(f\"  Labels shape: {collated_batch['labels'].shape}\")\n",
    "        print(f\"  Attention mask shape: {collated_batch['attention_mask'].shape}\")\n",
    "        \n",
    "        # Check that padding worked correctly\n",
    "        print(f\"  ‚úÖ Batch created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create-dataloaders"
   },
   "source": [
    "## 8. Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-dataloaders-code"
   },
   "outputs": [],
   "source": [
    "def create_dataloaders(tokenized_datasets: Dict[str, DatasetDict], batch_size: int = 4) -> Dict[str, Dict[str, DataLoader]]:\n",
    "    \"\"\"Create PyTorch DataLoaders for training\"\"\"\n",
    "    dataloaders = {}\n",
    "    \n",
    "    for dataset_name, dataset_dict in tokenized_datasets.items():\n",
    "        print(f\"Creating DataLoaders for {dataset_name}...\")\n",
    "        dataset_loaders = {}\n",
    "        \n",
    "        for split_name, dataset in dataset_dict.items():\n",
    "            if len(dataset) > 0:\n",
    "                shuffle = (split_name == 'train')  # Only shuffle training data\n",
    "                \n",
    "                dataloader = DataLoader(\n",
    "                    dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=shuffle,\n",
    "                    collate_fn=data_collator,\n",
    "                    pin_memory=torch.cuda.is_available(),\n",
    "                    num_workers=0  # Use 0 for Colab compatibility\n",
    "                )\n",
    "                \n",
    "                dataset_loaders[split_name] = dataloader\n",
    "                print(f\"  {split_name}: {len(dataset)} examples, {len(dataloader)} batches\")\n",
    "        \n",
    "        dataloaders[dataset_name] = dataset_loaders\n",
    "    \n",
    "    return dataloaders\n",
    "\n",
    "# Create dataloaders\n",
    "if tokenized_datasets:\n",
    "    # Use small batch size for Colab\n",
    "    BATCH_SIZE = 2  # Adjust based on GPU memory\n",
    "    \n",
    "    print(f\"Creating DataLoaders with batch size {BATCH_SIZE}...\")\n",
    "    dataloaders = create_dataloaders(tokenized_datasets, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    print(f\"\\n‚úÖ DataLoaders created for {len(dataloaders)} datasets\")\n",
    "    \n",
    "    # Test a dataloader\n",
    "    if 'combined' in dataloaders and 'train' in dataloaders['combined']:\n",
    "        test_loader = dataloaders['combined']['train']\n",
    "        \n",
    "        print(f\"\\nTesting DataLoader...\")\n",
    "        try:\n",
    "            batch = next(iter(test_loader))\n",
    "            print(f\"  Batch loaded successfully\")\n",
    "            print(f\"  Input IDs shape: {batch['input_ids'].shape}\")\n",
    "            print(f\"  Labels shape: {batch['labels'].shape}\")\n",
    "            print(f\"  Attention mask shape: {batch['attention_mask'].shape}\")\n",
    "            \n",
    "            # Check for proper label masking\n",
    "            masked_labels = (batch['labels'] == -100).sum().item()\n",
    "            total_labels = batch['labels'].numel()\n",
    "            print(f\"  Masked labels: {masked_labels}/{total_labels} ({masked_labels/total_labels:.1%})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error testing DataLoader: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå No tokenized datasets available for DataLoader creation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save-tokenized"
   },
   "source": [
    "## 9. Save Tokenized Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-tokenized-code"
   },
   "outputs": [],
   "source": [
    "def save_tokenized_datasets(tokenized_datasets: Dict[str, DatasetDict], tokenizer: AutoTokenizer):\n",
    "    \"\"\"Save tokenized datasets and tokenizer to disk\"\"\"\n",
    "    \n",
    "    print(f\"Saving tokenized datasets to {TOKENIZED_DATA_DIR}...\")\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer_dir = TOKENIZED_DATA_DIR / \"tokenizer\"\n",
    "    tokenizer.save_pretrained(tokenizer_dir)\n",
    "    print(f\"  ‚úÖ Tokenizer saved to {tokenizer_dir}\")\n",
    "    \n",
    "    # Save each tokenized dataset\n",
    "    for dataset_name, dataset_dict in tokenized_datasets.items():\n",
    "        dataset_dir = TOKENIZED_DATA_DIR / dataset_name\n",
    "        dataset_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        for split_name, dataset in dataset_dict.items():\n",
    "            split_dir = dataset_dir / split_name\n",
    "            dataset.save_to_disk(split_dir)\n",
    "            \n",
    "            # Also save as JSON for inspection\n",
    "            json_file = dataset_dir / f\"{split_name}.json\"\n",
    "            dataset.to_json(json_file)\n",
    "        \n",
    "        print(f\"  ‚úÖ {dataset_name} dataset saved to {dataset_dir}\")\n",
    "    \n",
    "    # Save tokenization metadata\n",
    "    metadata = {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'max_length': MAX_LENGTH,\n",
    "        'padding_side': PADDING_SIDE,\n",
    "        'vocab_size': len(tokenizer),\n",
    "        'pad_token_id': tokenizer.pad_token_id,\n",
    "        'eos_token_id': tokenizer.eos_token_id,\n",
    "        'bos_token_id': tokenizer.bos_token_id,\n",
    "        'datasets': {},\n",
    "        'task_prompts': TASK_PROMPTS,\n",
    "        'instruction_template': INSTRUCTION_TEMPLATE\n",
    "    }\n",
    "    \n",
    "    for dataset_name, dataset_dict in tokenized_datasets.items():\n",
    "        metadata['datasets'][dataset_name] = {\n",
    "            split_name: len(dataset) for split_name, dataset in dataset_dict.items()\n",
    "        }\n",
    "    \n",
    "    metadata_file = TOKENIZED_DATA_DIR / \"tokenization_metadata.json\"\n",
    "    with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"  ‚úÖ Metadata saved to {metadata_file}\")\n",
    "\n",
    "def create_tokenization_summary():\n",
    "    \"\"\"Create a summary of tokenization results\"\"\"\n",
    "    summary = {\n",
    "        'tokenization_config': {\n",
    "            'model_name': MODEL_NAME,\n",
    "            'max_length': MAX_LENGTH,\n",
    "            'vocab_size': len(tokenizer),\n",
    "            'padding_side': PADDING_SIDE\n",
    "        },\n",
    "        'datasets': {},\n",
    "        'total_examples': 0\n",
    "    }\n",
    "    \n",
    "    total_examples = 0\n",
    "    for dataset_name, dataset_dict in tokenized_datasets.items():\n",
    "        dataset_stats = {}\n",
    "        dataset_total = 0\n",
    "        \n",
    "        for split_name, dataset in dataset_dict.items():\n",
    "            count = len(dataset)\n",
    "            dataset_stats[split_name] = count\n",
    "            dataset_total += count\n",
    "        \n",
    "        dataset_stats['total'] = dataset_total\n",
    "        summary['datasets'][dataset_name] = dataset_stats\n",
    "        total_examples += dataset_total\n",
    "    \n",
    "    summary['total_examples'] = total_examples\n",
    "    return summary\n",
    "\n",
    "# Save tokenized datasets\n",
    "if tokenized_datasets:\n",
    "    save_tokenized_datasets(tokenized_datasets, tokenizer)\n",
    "    \n",
    "    # Create and display summary\n",
    "    summary = create_tokenization_summary()\n",
    "    print(f\"\\nüìä Tokenization Summary:\")\n",
    "    print(json.dumps(summary, indent=2))\n",
    "    \n",
    "    # Save summary\n",
    "    summary_file = TOKENIZED_DATA_DIR / \"tokenization_summary.json\"\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Tokenization complete!\")\n",
    "    print(f\"\\nTokenized data saved to: {TOKENIZED_DATA_DIR}\")\n",
    "    print(f\"\\nNext steps:\")\n",
    "    print(f\"1. Review tokenized datasets in {TOKENIZED_DATA_DIR}\")\n",
    "    print(f\"2. Run 03_finetuning_lora.ipynb to start training\")\n",
    "    print(f\"3. Monitor training progress and adjust hyperparameters as needed\")\nelse:\n",
    "    print(\"‚ùå No tokenized datasets to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validation"
   },
   "source": [
    "## 10. Validation and Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validation-code"
   },
   "outputs": [],
   "source": [
    "def validate_tokenization_quality(tokenized_datasets: Dict[str, DatasetDict], tokenizer: AutoTokenizer):\n",
    "    \"\"\"Perform quality checks on tokenized data\"\"\"\n",
    "    \n",
    "    print(\"üîç Performing tokenization quality checks...\")\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    for dataset_name, dataset_dict in tokenized_datasets.items():\n",
    "        print(f\"\\nChecking {dataset_name} dataset:\")\n",
    "        \n",
    "        for split_name, dataset in dataset_dict.items():\n",
    "            if len(dataset) == 0:\n",
    "                issues.append(f\"{dataset_name}/{split_name} is empty\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  {split_name} split ({len(dataset)} examples):\")\n",
    "            \n",
    "            # Check sequence lengths\n",
    "            lengths = [len(ex['input_ids']) for ex in dataset]\n",
    "            min_len, max_len, avg_len = min(lengths), max(lengths), np.mean(lengths)\n",
    "            \n",
    "            print(f\"    Length - Min: {min_len}, Max: {max_len}, Avg: {avg_len:.1f}\")\n",
    "            \n",
    "            if max_len > MAX_LENGTH:\n",
    "                issues.append(f\"{dataset_name}/{split_name} has sequences longer than {MAX_LENGTH}\")\n",
    "            \n",
    "            if min_len < 10:\n",
    "                issues.append(f\"{dataset_name}/{split_name} has very short sequences (< 10 tokens)\")\n",
    "            \n",
    "            # Check for special tokens\n",
    "            sample_ids = dataset[0]['input_ids']\n",
    "            has_bos = tokenizer.bos_token_id in sample_ids if tokenizer.bos_token_id else True\n",
    "            has_eos = tokenizer.eos_token_id in sample_ids if tokenizer.eos_token_id else True\n",
    "            \n",
    "            if not has_eos:\n",
    "                issues.append(f\"{dataset_name}/{split_name} missing EOS tokens\")\n",
    "            \n",
    "            # Check label alignment\n",
    "            sample = dataset[0]\n",
    "            if len(sample['input_ids']) != len(sample['labels']):\n",
    "                issues.append(f\"{dataset_name}/{split_name} input_ids and labels length mismatch\")\n",
    "            \n",
    "            # Sample a few examples and decode them\n",
    "            if len(dataset) >= 3:\n",
    "                sample_indices = [0, len(dataset)//2, -1]\n",
    "                for i in sample_indices:\n",
    "                    example = dataset[i]\n",
    "                    decoded = tokenizer.decode(example['input_ids'], skip_special_tokens=False)\n",
    "                    \n",
    "                    # Check for obvious formatting issues\n",
    "                    if '[INST]' not in decoded or '[/INST]' not in decoded:\n",
    "                        issues.append(f\"{dataset_name}/{split_name} example {i} missing instruction formatting\")\n",
    "                        break\n",
    "            \n",
    "            print(f\"    ‚úÖ Basic checks passed\")\n",
    "    \n",
    "    # Summary\n",
    "    if issues:\n",
    "        print(f\"\\n‚ö†Ô∏è Found {len(issues)} issues:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  - {issue}\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ All quality checks passed!\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "def show_tokenization_examples(tokenized_datasets: Dict[str, DatasetDict], tokenizer: AutoTokenizer, num_examples: int = 2):\n",
    "    \"\"\"Show decoded examples from tokenized datasets\"\"\"\n",
    "    \n",
    "    print(f\"\\nüìù Sample tokenized examples:\")\n",
    "    \n",
    "    for dataset_name, dataset_dict in list(tokenized_datasets.items())[:1]:  # Show first dataset only\n",
    "        if 'train' in dataset_dict and len(dataset_dict['train']) > 0:\n",
    "            dataset = dataset_dict['train']\n",
    "            \n",
    "            print(f\"\\n{dataset_name} examples:\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            for i in range(min(num_examples, len(dataset))):\n",
    "                example = dataset[i]\n",
    "                \n",
    "                print(f\"\\nExample {i+1}:\")\n",
    "                print(f\"Length: {len(example['input_ids'])} tokens\")\n",
    "                \n",
    "                # Decode and show\n",
    "                decoded = tokenizer.decode(example['input_ids'], skip_special_tokens=False)\n",
    "                print(f\"Decoded text:\")\n",
    "                print(decoded)\n",
    "                \n",
    "                # Show first 20 token IDs\n",
    "                print(f\"\\nFirst 20 token IDs: {example['input_ids'][:20]}\")\n",
    "                print(f\"First 20 tokens: {[tokenizer.decode([tid]) for tid in example['input_ids'][:20]]}\")\n",
    "                print(\"-\" * 40)\n",
    "\n",
    "# Run quality checks\n",
    "if tokenized_datasets:\n",
    "    issues = validate_tokenization_quality(tokenized_datasets, tokenizer)\n",
    "    \n",
    "    # Show examples\n",
    "    show_tokenization_examples(tokenized_datasets, tokenizer, num_examples=2)\n",
    "    \n",
    "    if not issues:\n",
    "        print(f\"\\nüéâ Tokenization completed successfully!\")\n",
    "        print(f\"Ready for training with {sum(sum(len(ds[split]) for split in ds.keys()) for ds in tokenized_datasets.values())} total examples\")\nelse:\n    print(\"‚ùå No tokenized datasets available for validation\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
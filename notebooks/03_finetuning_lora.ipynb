{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üöÄ LLaMA Fine-tuning with LoRA for Insurance Tasks\n",
    "\n",
    "This notebook performs the main fine-tuning of LLaMA using LoRA (Low-Rank Adaptation) for insurance-specific tasks:\n",
    "\n",
    "## What this notebook does:\n",
    "1. Load the base LLaMA model with quantization\n",
    "2. Set up LoRA configuration for efficient fine-tuning\n",
    "3. Load tokenized datasets\n",
    "4. Configure training arguments and trainer\n",
    "5. Monitor training progress with W&B\n",
    "6. Save checkpoints and final model\n",
    "7. Merge and export the fine-tuned model\n",
    "\n",
    "**‚ö†Ô∏è Important: Make sure GPU is enabled and you have sufficient memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## 1. Import Libraries and Check Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports-code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Union\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Core ML libraries\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "# PEFT and LoRA\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "# Quantization\n",
    "from transformers import BitsAndBytesConfig\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "# Datasets\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "\n",
    "# Monitoring\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Utilities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from huggingface_hub import login\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"Current GPU memory usage: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\nelse:\n    print(\"‚ö†Ô∏è No GPU detected - training will be very slow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## 2. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config-code"
   },
   "outputs": [],
   "source": [
    "# Model and data configuration\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "TOKENIZED_DATA_DIR = Path(\"data/tokenized\")\n",
    "OUTPUT_DIR = Path(\"outputs\")\n",
    "CHECKPOINT_DIR = OUTPUT_DIR / \"checkpoints\"\n",
    "FINAL_MODEL_DIR = OUTPUT_DIR / \"final_model\"\n",
    "LOGS_DIR = OUTPUT_DIR / \"logs\"\n",
    "\n",
    "# Create output directories\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "FINAL_MODEL_DIR.mkdir(exist_ok=True)\n",
    "LOGS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Load training configuration\n",
    "config_file = Path(\"config/training_args.json\")\n",
    "if config_file.exists():\n",
    "    with open(config_file, 'r') as f:\n",
    "        TRAINING_CONFIG = json.load(f)\nelse:\n    # Default configuration for Colab\n",
    "    TRAINING_CONFIG = {\n",
    "        \"output_dir\": str(CHECKPOINT_DIR),\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"per_device_train_batch_size\": 2,\n",
    "        \"per_device_eval_batch_size\": 2,\n",
    "        \"gradient_accumulation_steps\": 8,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"fp16\": True,\n",
    "        \"max_grad_norm\": 0.3,\n",
    "        \"warmup_ratio\": 0.03,\n",
    "        \"lr_scheduler_type\": \"cosine\",\n",
    "        \"save_steps\": 500,\n",
    "        \"eval_steps\": 500,\n",
    "        \"logging_steps\": 50,\n",
    "        \"save_total_limit\": 3,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"metric_for_best_model\": \"eval_loss\",\n",
    "        \"greater_is_better\": False,\n",
    "        \"evaluation_strategy\": \"steps\",\n",
    "        \"save_strategy\": \"steps\"\n",
    "    }\n",
    "\n",
    "# Load LoRA configuration\n",
    "lora_config_file = Path(\"config/lora_config.json\")\n",
    "if lora_config_file.exists():\n",
    "    with open(lora_config_file, 'r') as f:\n",
    "        LORA_CONFIG = json.load(f)\nelse:\n",
    "    # Default LoRA configuration\n",
    "    LORA_CONFIG = {\n",
    "        \"r\": 8,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"bias\": \"none\",\n",
    "        \"task_type\": \"CAUSAL_LM\",\n",
    "        \"target_modules\": [\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Quantization configuration for 4-bit training\n",
    "QUANTIZATION_CONFIG = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    ")\n",
    "\n",
    "# Training parameters\n",
    "USE_WANDB = True  # Set to False if you don't want W&B logging\n",
    "WANDB_PROJECT = \"llama-insurance-finetune\"\n",
    "RUN_NAME = f\"llama-insurance-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "print(f\"Configuration loaded:\")\nprint(f\"- Model: {MODEL_NAME}\")\nprint(f\"- Output directory: {OUTPUT_DIR}\")\nprint(f\"- Batch size: {TRAINING_CONFIG['per_device_train_batch_size']}\")\nprint(f\"- Gradient accumulation: {TRAINING_CONFIG['gradient_accumulation_steps']}\")\nprint(f\"- Learning rate: {TRAINING_CONFIG['learning_rate']}\")\nprint(f\"- LoRA rank: {LORA_CONFIG['r']}\")\nprint(f\"- Quantization: 4-bit with {QUANTIZATION_CONFIG.bnb_4bit_compute_dtype}\")\nprint(f\"- W&B logging: {USE_WANDB}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load-data"
   },
   "source": [
    "## 3. Load Tokenized Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-data-code"
   },
   "outputs": [],
   "source": [
    "def load_tokenized_datasets() -> tuple[DatasetDict, AutoTokenizer]:\n",
    "    \"\"\"Load tokenized datasets and tokenizer\"\"\"\n",
    "    \n",
    "    print(f\"Loading tokenized datasets from {TOKENIZED_DATA_DIR}...\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer_dir = TOKENIZED_DATA_DIR / \"tokenizer\"\n",
    "    if not tokenizer_dir.exists():\n",
    "        print(f\"‚ùå Tokenizer not found at {tokenizer_dir}\")\n",
    "        print(\"Please run 02_tokenization.ipynb first\")\n",
    "        return None, None\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
    "    print(f\"‚úÖ Tokenizer loaded from {tokenizer_dir}\")\n",
    "    \n",
    "    # Load combined dataset (or first available dataset)\n",
    "    dataset_dict = None\n",
    "    \n",
    "    # Try to load combined dataset first\n",
    "    combined_dir = TOKENIZED_DATA_DIR / \"combined\"\n",
    "    if combined_dir.exists():\n",
    "        print(f\"Loading combined dataset from {combined_dir}...\")\n",
    "        try:\n",
    "            dataset_dict = DatasetDict()\n",
    "            for split in ['train', 'validation', 'test']:\n",
    "                split_dir = combined_dir / split\n",
    "                if split_dir.exists():\n",
    "                    dataset = load_from_disk(split_dir)\n",
    "                    dataset_dict[split] = dataset\n",
    "                    print(f\"  {split}: {len(dataset)} examples\")\n",
    "            \n",
    "            if dataset_dict:\n",
    "                print(f\"‚úÖ Combined dataset loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading combined dataset: {e}\")\n",
    "            dataset_dict = None\n",
    "    \n",
    "    # If no combined dataset, try individual task datasets\n",
    "    if not dataset_dict:\n",
    "        task_dirs = [d for d in TOKENIZED_DATA_DIR.iterdir() \n",
    "                    if d.is_dir() and d.name not in ['tokenizer', 'combined']]\n",
    "        \n",
    "        if task_dirs:\n",
    "            # Use the first available task dataset\n",
    "            task_dir = task_dirs[0]\n",
    "            print(f\"Loading {task_dir.name} dataset from {task_dir}...\")\n",
    "            \n",
    "            try:\n",
    "                dataset_dict = DatasetDict()\n",
    "                for split in ['train', 'validation', 'test']:\n",
    "                    split_dir = task_dir / split\n",
    "                    if split_dir.exists():\n",
    "                        dataset = load_from_disk(split_dir)\n",
    "                        dataset_dict[split] = dataset\n",
    "                        print(f\"  {split}: {len(dataset)} examples\")\n",
    "                \n",
    "                print(f\"‚úÖ {task_dir.name} dataset loaded\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error loading {task_dir.name} dataset: {e}\")\n",
    "                return None, None\n",
    "    \n",
    "    if not dataset_dict:\n",
    "        print(f\"‚ùå No tokenized datasets found in {TOKENIZED_DATA_DIR}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Validate dataset\n",
    "    for split_name, dataset in dataset_dict.items():\n",
    "        if len(dataset) > 0:\n",
    "            sample = dataset[0]\n",
    "            required_keys = ['input_ids', 'labels', 'attention_mask']\n",
    "            missing_keys = [key for key in required_keys if key not in sample]\n",
    "            if missing_keys:\n",
    "                print(f\"‚ö†Ô∏è Missing keys in {split_name}: {missing_keys}\")\n",
    "            else:\n",
    "                print(f\"‚úÖ {split_name} dataset validated\")\n",
    "    \n",
    "    return dataset_dict, tokenizer\n",
    "\n",
    "# Load datasets\n",
    "train_dataset, tokenizer = load_tokenized_datasets()\n",
    "\n",
    "if train_dataset is None:\n",
    "    print(\"‚ùå Failed to load datasets. Cannot proceed with training.\")\n",
    "    print(\"Please run the previous notebooks first:\")\n",
    "    print(\"1. 01_data_preprocessing.ipynb\")\n",
    "    print(\"2. 02_tokenization.ipynb\")\nelse:\n    print(f\"\\nüìä Dataset Summary:\")\n    total_examples = 0\n    for split_name, dataset in train_dataset.items():\n        examples = len(dataset)\n        total_examples += examples\n        print(f\"  {split_name}: {examples:,} examples\")\n    \n    print(f\"  Total: {total_examples:,} examples\")\n    \n    # Show sample\n    if 'train' in train_dataset and len(train_dataset['train']) > 0:\n        sample = train_dataset['train'][0]\n        print(f\"\\nSample data structure:\")\n        print(f\"  Keys: {list(sample.keys())}\")\n        print(f\"  Input length: {len(sample['input_ids'])} tokens\")\n        print(f\"  Label length: {len(sample['labels'])} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load-model"
   },
   "source": [
    "## 4. Load and Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-model-code"
   },
   "outputs": [],
   "source": [
    "def load_base_model(model_name: str, tokenizer: AutoTokenizer) -> AutoModelForCausalLM:\n",
    "    \"\"\"Load the base LLaMA model with quantization\"\"\"\n",
    "    \n",
    "    print(f\"Loading base model {model_name}...\")\n",
    "    print(f\"Using quantization: {QUANTIZATION_CONFIG.load_in_4bit}-bit\")\n",
    "    \n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=QUANTIZATION_CONFIG,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            use_cache=False  # Disable cache for training\n",
    "        )\n",
    "        \n",
    "        # Resize token embeddings if tokenizer was modified\n",
    "        if len(tokenizer) > model.config.vocab_size:\n",
    "            print(f\"Resizing token embeddings: {model.config.vocab_size} -> {len(tokenizer)}\")\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "        print(f\"‚úÖ Model loaded successfully\")\n",
    "        print(f\"  Model size: ~{sum(p.numel() for p in model.parameters()) / 1e6:.0f}M parameters\")\n",
    "        print(f\"  Vocab size: {model.config.vocab_size}\")\n",
    "        print(f\"  Max position embeddings: {model.config.max_position_embeddings}\")\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        print(\"Make sure you're authenticated with Hugging Face and have access to LLaMA models\")\n",
    "        raise\n",
    "\n",
    "def setup_lora_model(model: AutoModelForCausalLM, lora_config: dict) -> PeftModel:\n",
    "    \"\"\"Set up LoRA configuration and wrap the model\"\"\"\n",
    "    \n",
    "    print(f\"Setting up LoRA with configuration:\")\n",
    "    for key, value in lora_config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Prepare model for k-bit training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    # Create LoRA configuration\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=lora_config['r'],\n",
    "        lora_alpha=lora_config['lora_alpha'],\n",
    "        lora_dropout=lora_config['lora_dropout'],\n",
    "        bias=lora_config['bias'],\n",
    "        target_modules=lora_config['target_modules'],\n",
    "        inference_mode=False\n",
    "    )\n",
    "    \n",
    "    # Wrap model with LoRA\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"\\n‚úÖ LoRA model ready\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%})\")\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load model if datasets are available\n",
    "if train_dataset is not None and tokenizer is not None:\n",
    "    # Clear GPU memory first\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # Load base model\n",
    "    base_model = load_base_model(MODEL_NAME, tokenizer)\n",
    "    \n",
    "    # Set up LoRA\n",
    "    model = setup_lora_model(base_model, LORA_CONFIG)\n",
    "    \n",
    "    # Check GPU memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        memory_used = torch.cuda.memory_allocated(0) / 1e9\n",
    "        memory_reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "        print(f\"\\nGPU Memory Usage:\")\n",
    "        print(f\"  Allocated: {memory_used:.2f} GB\")\n",
    "        print(f\"  Reserved: {memory_reserved:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå Skipping model loading due to missing datasets\")\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-training"
   },
   "source": [
    "## 5. Setup Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-training-code"
   },
   "outputs": [],
   "source": [
    "def setup_training_arguments(config: dict) -> TrainingArguments:\n",
    "    \"\"\"Create training arguments from configuration\"\"\"\n",
    "    \n",
    "    # Update output directory to use our paths\n",
    "    config = config.copy()\n",
    "    config['output_dir'] = str(CHECKPOINT_DIR)\n",
    "    config['logging_dir'] = str(LOGS_DIR)\n",
    "    \n",
    "    # Add W&B configuration if enabled\n",
    "    if USE_WANDB:\n",
    "        config['report_to'] = 'wandb'\n",
    "        config['run_name'] = RUN_NAME\n",
    "    else:\n",
    "        config['report_to'] = []\n",
    "    \n",
    "    # Ensure directories exist\n",
    "    Path(config['output_dir']).mkdir(exist_ok=True)\n",
    "    Path(config['logging_dir']).mkdir(exist_ok=True)\n",
    "    \n",
    "    training_args = TrainingArguments(**config)\n",
    "    \n",
    "    print(f\"Training arguments configured:\")\n",
    "    print(f\"  Output dir: {training_args.output_dir}\")\n",
    "    print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "    print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "    print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "    print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "    print(f\"  Weight decay: {training_args.weight_decay}\")\n",
    "    print(f\"  Warmup ratio: {training_args.warmup_ratio}\")\n",
    "    print(f\"  Save steps: {training_args.save_steps}\")\n",
    "    print(f\"  Eval steps: {training_args.eval_steps}\")\n",
    "    print(f\"  FP16: {training_args.fp16}\")\n",
    "    print(f\"  Gradient checkpointing: {training_args.gradient_checkpointing}\")\n",
    "    \n",
    "    return training_args\n",
    "\n",
    "def setup_data_collator(tokenizer: AutoTokenizer) -> DataCollatorForLanguageModeling:\n",
    "    \"\"\"Setup data collator for causal language modeling\"\"\"\n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,  # Causal LM, not masked LM\n",
    "        pad_to_multiple_of=8,  # For efficiency\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Data collator configured for causal LM\")\n",
    "    return data_collator\n",
    "\n",
    "class TrainingCallbacks:\n",
    "    \"\"\"Custom callbacks for training monitoring\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_callbacks():\n",
    "        callbacks = []\n",
    "        \n",
    "        # Early stopping callback\n",
    "        early_stopping = EarlyStoppingCallback(\n",
    "            early_stopping_patience=3,\n",
    "            early_stopping_threshold=0.01\n",
    "        )\n",
    "        callbacks.append(early_stopping)\n",
    "        \n",
    "        return callbacks\n",
    "\n",
    "# Setup training components if model is available\n",
    "if model is not None and train_dataset is not None:\n",
    "    print(\"Setting up training components...\")\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = setup_training_arguments(TRAINING_CONFIG)\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = setup_data_collator(tokenizer)\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = TrainingCallbacks.get_callbacks()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Training components ready\")\n",
    "    print(f\"  Callbacks: {len(callbacks)} configured\")\n",
    "else:\n",
    "    print(\"‚ùå Skipping training setup due to missing model or datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "initialize-wandb"
   },
   "source": [
    "## 6. Initialize Weights & Biases (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "initialize-wandb-code"
   },
   "outputs": [],
   "source": [
    "def initialize_wandb():\n",
    "    \"\"\"Initialize Weights & Biases for experiment tracking\"\"\"\n",
    "    \n",
    "    if not USE_WANDB:\n",
    "        print(\"W&B logging disabled\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Initialize wandb\n",
    "        wandb.init(\n",
    "            project=WANDB_PROJECT,\n",
    "            name=RUN_NAME,\n",
    "            config={\n",
    "                'model_name': MODEL_NAME,\n",
    "                'lora_config': LORA_CONFIG,\n",
    "                'training_config': TRAINING_CONFIG,\n",
    "                'quantization': '4-bit',\n",
    "                'dataset_size': {\n",
    "                    split: len(dataset) for split, dataset in train_dataset.items()\n",
    "                } if train_dataset else {}\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ W&B initialized\")\n",
    "        print(f\"  Project: {WANDB_PROJECT}\")\n",
    "        print(f\"  Run name: {RUN_NAME}\")\n",
    "        print(f\"  Dashboard: {wandb.run.url}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è W&B initialization failed: {e}\")\n",
    "        print(\"Training will continue without W&B logging\")\n",
    "        global USE_WANDB\n",
    "        USE_WANDB = False\n",
    "\n",
    "# Initialize W&B\n",
    "initialize_wandb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create-trainer"
   },
   "source": [
    "## 7. Create Trainer and Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-trainer-code"
   },
   "outputs": [],
   "source": [
    "def create_trainer() -> Trainer:\n",
    "    \"\"\"Create the Hugging Face Trainer\"\"\"\n",
    "    \n",
    "    if model is None or train_dataset is None:\n",
    "        raise ValueError(\"Model and dataset must be loaded before creating trainer\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset['train'],\n",
    "        eval_dataset=train_dataset.get('validation'),\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Trainer created\")\n",
    "    print(f\"  Training samples: {len(train_dataset['train'])}\")\n",
    "    if 'validation' in train_dataset:\n",
    "        print(f\"  Validation samples: {len(train_dataset['validation'])}\")\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "def calculate_training_time(trainer: Trainer) -> tuple:\n",
    "    \"\"\"Calculate estimated training time\"\"\"\n",
    "    \n",
    "    train_dataloader = trainer.get_train_dataloader()\n",
    "    num_batches_per_epoch = len(train_dataloader)\n",
    "    num_epochs = training_args.num_train_epochs\n",
    "    \n",
    "    total_steps = num_batches_per_epoch * num_epochs\n",
    "    \n",
    "    # Rough estimate: 1-3 seconds per step on T4/V100\n",
    "    estimated_seconds_per_step = 2.0\n",
    "    estimated_total_seconds = total_steps * estimated_seconds_per_step\n",
    "    \n",
    "    hours = estimated_total_seconds // 3600\n",
    "    minutes = (estimated_total_seconds % 3600) // 60\n",
    "    \n",
    "    print(f\"Training estimation:\")\n",
    "    print(f\"  Batches per epoch: {num_batches_per_epoch}\")\n",
    "    print(f\"  Total epochs: {num_epochs}\")\n",
    "    print(f\"  Total steps: {total_steps}\")\n",
    "    print(f\"  Estimated time: {int(hours)}h {int(minutes)}m\")\n",
    "    \n",
    "    return total_steps, estimated_total_seconds\n",
    "\n",
    "# Create trainer and estimate training time\n",
    "if model is not None and train_dataset is not None:\n",
    "    print(\"Creating trainer...\")\n",
    "    trainer = create_trainer()\n",
    "    \n",
    "    # Calculate training time\n",
    "    total_steps, estimated_time = calculate_training_time(trainer)\n",
    "    \n",
    "    print(f\"\\nüöÄ Ready to start training!\")\n",
    "    print(f\"Training will begin in the next cell...\")\nelse:\n",
    "    print(\"‚ùå Cannot create trainer - missing model or datasets\")\n",
    "    trainer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "start-training"
   },
   "source": [
    "## 8. Start Training üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start-training-code"
   },
   "outputs": [],
   "source": [
    "def start_training(trainer: Trainer):\n",
    "    \"\"\"Start the training process\"\"\"\n",
    "    \n",
    "    print(f\"üöÄ Starting training at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"=\"*60)\n",
    "    \n",
    "    # Clear GPU memory before training\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"GPU memory before training:\")\n",
    "        print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "        print(f\"  Reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "    \n",
    "    try:\n",
    "        # Start training\n",
    "        start_time = time.time()\n",
    "        \n",
    "        training_result = trainer.train()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        training_duration = end_time - start_time\n",
    "        \n",
    "        print(f\"\\n‚úÖ Training completed successfully!\")\n",
    "        print(f\"Training time: {training_duration/3600:.2f} hours\")\n",
    "        \n",
    "        # Print training summary\n",
    "        print(f\"\\nTraining Summary:\")\n",
    "        print(f\"  Total steps: {training_result.global_step}\")\n",
    "        print(f\"  Final train loss: {training_result.training_loss:.4f}\")\n",
    "        print(f\"  Best model checkpoint: {trainer.state.best_model_checkpoint}\")\n",
    "        \n",
    "        # Log final GPU memory\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"\\nGPU memory after training:\")\n",
    "            print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "            print(f\"  Reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "        \n",
    "        return training_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training failed: {e}\")\n",
    "        print(f\"This might be due to:\")\n",
    "        print(f\"  - Insufficient GPU memory\")\n",
    "        print(f\"  - Invalid configuration\")\n",
    "        print(f\"  - Dataset issues\")\n",
    "        \n",
    "        # Try to save current state\n",
    "        try:\n",
    "            emergency_save_path = CHECKPOINT_DIR / \"emergency_checkpoint\"\n",
    "            trainer.save_model(emergency_save_path)\n",
    "            print(f\"Emergency checkpoint saved to: {emergency_save_path}\")\n",
    "        except:\n",
    "            print(\"Could not save emergency checkpoint\")\n",
    "        \n",
    "        raise\n",
    "\n",
    "# Start training if everything is ready\n",
    "if trainer is not None:\n",
    "    # Final confirmation\n",
    "    print(f\"‚ö†Ô∏è About to start training with the following configuration:\")\n",
    "    print(f\"  Model: {MODEL_NAME}\")\n",
    "    print(f\"  Training samples: {len(train_dataset['train'])}\")\n",
    "    print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "    print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "    print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "    print(f\"  Output directory: {training_args.output_dir}\")\n",
    "    \n",
    "    print(f\"\\nStarting training in 3 seconds...\")\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Start training\n",
    "    training_result = start_training(trainer)\n",
    "    \nelse:\n",
    "    print(\"‚ùå Cannot start training - trainer not initialized\")\n",
    "    print(\"Please check the previous cells for errors\")\n    training_result = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save-model"
   },
   "source": [
    "## 9. Save and Export Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-model-code"
   },
   "outputs": [],
   "source": [
    "def save_final_model(trainer: Trainer, model, tokenizer: AutoTokenizer):\n",
    "    \"\"\"Save the final trained model and tokenizer\"\"\"\n",
    "    \n",
    "    print(f\"Saving final model to {FINAL_MODEL_DIR}...\")\n",
    "    \n",
    "    # Save the LoRA model\n",
    "    lora_model_dir = FINAL_MODEL_DIR / \"lora_model\"\n",
    "    lora_model_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    model.save_pretrained(lora_model_dir)\n",
    "    tokenizer.save_pretrained(lora_model_dir)\n",
    "    \n",
    "    print(f\"‚úÖ LoRA model saved to: {lora_model_dir}\")\n",
    "    \n",
    "    # Also save using trainer (includes training state)\n",
    "    trainer_model_dir = FINAL_MODEL_DIR / \"trainer_model\"\n",
    "    trainer.save_model(trainer_model_dir)\n",
    "    \n",
    "    print(f\"‚úÖ Trainer model saved to: {trainer_model_dir}\")\n",
    "    \n",
    "    # Save model configuration and metadata\n",
    "    model_info = {\n",
    "        'base_model': MODEL_NAME,\n",
    "        'model_type': 'LLaMA-2-7B with LoRA',\n",
    "        'task': 'Insurance Domain Fine-tuning',\n",
    "        'lora_config': LORA_CONFIG,\n",
    "        'training_config': TRAINING_CONFIG,\n",
    "        'training_completed': datetime.now().isoformat(),\n",
    "        'final_loss': training_result.training_loss if training_result else None,\n",
    "        'total_steps': training_result.global_step if training_result else None,\n",
    "        'best_checkpoint': trainer.state.best_model_checkpoint if hasattr(trainer.state, 'best_model_checkpoint') else None\n",
    "    }\n",
    "    \n",
    "    info_file = FINAL_MODEL_DIR / \"model_info.json\"\n",
    "    with open(info_file, 'w') as f:\n",
    "        json.dump(model_info, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Model info saved to: {info_file}\")\n",
    "    \n",
    "    return lora_model_dir, trainer_model_dir\n",
    "\n",
    "def merge_and_save_model(model, tokenizer: AutoTokenizer, base_model_name: str):\n",
    "    \"\"\"Merge LoRA weights with base model and save (optional)\"\"\"\n",
    "    \n",
    "    print(f\"\\nOptional: Merging LoRA weights with base model...\")\n",
    "    print(f\"‚ö†Ô∏è This will require additional GPU memory and time\")\n",
    "    \n",
    "    try:\n",
    "        # Load base model for merging\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        # Merge LoRA weights\n",
    "        merged_model = model.merge_and_unload()\n",
    "        \n",
    "        # Save merged model\n",
    "        merged_model_dir = FINAL_MODEL_DIR / \"merged_model\"\n",
    "        merged_model_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        merged_model.save_pretrained(merged_model_dir)\n",
    "        tokenizer.save_pretrained(merged_model_dir)\n",
    "        \n",
    "        print(f\"‚úÖ Merged model saved to: {merged_model_dir}\")\n",
    "        \n",
    "        # Clean up\n",
    "        del base_model, merged_model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return merged_model_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Model merging failed: {e}\")\n",
    "        print(f\"This is optional - you can still use the LoRA model\")\n",
    "        return None\n",
    "\n",
    "def create_model_card(model_dir: Path):\n",
    "    \"\"\"Create a model card for the trained model\"\"\"\n",
    "    \n",
    "    model_card_content = f\"\"\"\n",
    "# LLaMA Insurance Fine-tuned Model\n",
    "\n",
    "## Model Description\n",
    "This model is a fine-tuned version of {MODEL_NAME} specifically trained for insurance domain tasks.\n",
    "\n",
    "## Training Details\n",
    "- **Base Model**: {MODEL_NAME}\n",
    "- **Fine-tuning Method**: LoRA (Low-Rank Adaptation)\n",
    "- **Training Date**: {datetime.now().strftime('%Y-%m-%d')}\n",
    "- **Training Framework**: Transformers + PEFT\n",
    "- **Quantization**: 4-bit with BitsAndBytesConfig\n",
    "\n",
    "## LoRA Configuration\n",
    "- **Rank (r)**: {LORA_CONFIG['r']}\n",
    "- **Alpha**: {LORA_CONFIG['lora_alpha']}\n",
    "- **Dropout**: {LORA_CONFIG['lora_dropout']}\n",
    "- **Target Modules**: {', '.join(LORA_CONFIG['target_modules'])}\n",
    "\n",
    "## Training Configuration\n",
    "- **Learning Rate**: {TRAINING_CONFIG['learning_rate']}\n",
    "- **Batch Size**: {TRAINING_CONFIG['per_device_train_batch_size']}\n",
    "- **Epochs**: {TRAINING_CONFIG['num_train_epochs']}\n",
    "- **Gradient Accumulation**: {TRAINING_CONFIG['gradient_accumulation_steps']}\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{model_dir}\")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"{MODEL_NAME}\")\n",
    "\n",
    "# Load LoRA model\n",
    "model = PeftModel.from_pretrained(base_model, \"{model_dir}\")\n",
    "\n",
    "# Generate text\n",
    "inputs = tokenizer(\"[INST] Explain health insurance coverage [/INST]\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=200)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "```\n",
    "\n",
    "## Tasks Supported\n",
    "- Insurance claim classification\n",
    "- Policy document summarization\n",
    "- FAQ generation\n",
    "- Compliance checking\n",
    "- Contract question-answering\n",
    "\n",
    "## Limitations\n",
    "- Specialized for insurance domain only\n",
    "- Requires validation by subject matter experts\n",
    "- Should not be used for final decision making without human oversight\n",
    "\n",
    "## License\n",
    "Subject to LLaMA 2 license terms and conditions.\n",
    "\"\"\"\n",
    "    \n",
    "    model_card_file = model_dir / \"README.md\"\n",
    "    with open(model_card_file, 'w') as f:\n",
    "        f.write(model_card_content.strip())\n",
    "    \n",
    "    print(f\"‚úÖ Model card created: {model_card_file}\")\n",
    "\n",
    "# Save models if training completed successfully\n",
    "if training_result is not None and trainer is not None:\n",
    "    print(\"Saving trained models...\")\n",
    "    \n",
    "    # Save LoRA model\n",
    "    lora_dir, trainer_dir = save_final_model(trainer, model, tokenizer)\n",
    "    \n",
    "    # Create model card\n",
    "    create_model_card(lora_dir)\n",
    "    \n",
    "    # Optional: merge and save full model (comment out if memory is limited)\n",
    "    # merged_dir = merge_and_save_model(model, tokenizer, MODEL_NAME)\n",
    "    \n",
    "    print(f\"\\nüéâ Model saving complete!\")\n",
    "    print(f\"\\nSaved models:\")\n",
    "    print(f\"  LoRA model: {lora_dir}\")\n",
    "    print(f\"  Trainer model: {trainer_dir}\")\n",
    "    # if merged_dir:\n",
    "    #     print(f\"  Merged model: {merged_dir}\")\n",
    "    \n",
    "    print(f\"\\nNext steps:\")\n",
    "    print(f\"1. Run 04_evaluation.ipynb to evaluate model performance\")\n",
    "    print(f\"2. Run 05_inference_demo.ipynb to test the model\")\n",
    "    print(f\"3. Consider uploading to Hugging Face Hub for sharing\")\n",
    "    \nelse:\n",
    "    print(\"‚ùå Cannot save models - training did not complete successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup"
   },
   "source": [
    "## 10. Cleanup and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup-code"
   },
   "outputs": [],
   "source": [
    "def cleanup_and_summary():\n",
    "    \"\"\"Clean up memory and provide training summary\"\"\"\n",
    "    \n",
    "    print(\"üßπ Cleaning up memory...\")\n",
    "    \n",
    "    # Clean up models from memory\n",
    "    global model, base_model, trainer\n",
    "    \n",
    "    if 'model' in globals():\n",
    "        del model\n",
    "    if 'base_model' in globals():\n",
    "        del base_model\n",
    "    if 'trainer' in globals():\n",
    "        trainer = None\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"GPU memory after cleanup:\")\n",
    "        print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "        print(f\"  Reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Close W&B run\n",
    "    if USE_WANDB:\n",
    "        try:\n",
    "            wandb.finish()\n",
    "            print(\"‚úÖ W&B run completed\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Create training summary\n",
    "    summary = {\n",
    "        'training_completed': training_result is not None,\n",
    "        'completion_time': datetime.now().isoformat(),\n",
    "        'model_name': MODEL_NAME,\n",
    "        'output_directory': str(FINAL_MODEL_DIR),\n",
    "        'lora_config': LORA_CONFIG,\n",
    "        'training_config': TRAINING_CONFIG\n",
    "    }\n",
    "    \n",
    "    if training_result:\n",
    "        summary.update({\n",
    "            'final_loss': training_result.training_loss,\n",
    "            'total_steps': training_result.global_step,\n",
    "        })\n",
    "    \n",
    "    # Save summary\n",
    "    summary_file = OUTPUT_DIR / \"training_summary.json\"\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüìä Training Summary:\")\n",
    "    print(json.dumps(summary, indent=2))\n",
    "    print(f\"\\nSummary saved to: {summary_file}\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Final cleanup and summary\n",
    "final_summary = cleanup_and_summary()\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\nif training_result is not None:\n    print(f\"üéâ LLaMA Insurance Fine-tuning COMPLETED SUCCESSFULLY! üéâ\")\n    print(f\"\\nYour fine-tuned model is ready for evaluation and inference.\")\n    print(f\"Check the outputs/ directory for saved models and logs.\")\nelse:\n    print(f\"‚ùå Training was not completed successfully.\")\n    print(f\"Please check the error messages above and try again.\")\n    print(f\"Common issues: insufficient GPU memory, authentication problems, or data issues.\")\n\nprint(f\"\\nNext notebooks to run:\")\nprint(f\"1. 04_evaluation.ipynb - Evaluate model performance\")\nprint(f\"2. 05_inference_demo.ipynb - Test model inference\")\nprint(f\"\\nThank you for using this LLaMA insurance fine-tuning notebook!\")\nprint(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
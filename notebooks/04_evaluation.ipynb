{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# 📊 Model Evaluation for Insurance LLaMA\n",
    "\n",
    "This notebook evaluates the fine-tuned LLaMA model on insurance-specific tasks:\n",
    "\n",
    "## What this notebook does:\n",
    "1. Load the fine-tuned model and test datasets\n",
    "2. Run comprehensive evaluations for each task type\n",
    "3. Calculate insurance-specific metrics (ROUGE, BLEU, F1, accuracy)\n",
    "4. Generate sample predictions and compare with ground truth\n",
    "5. Create evaluation reports and visualizations\n",
    "6. Analyze model performance and identify areas for improvement\n",
    "\n",
    "**⚠️ Important: Make sure you have a trained model from notebook 03**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports-code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Core ML libraries\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import evaluate\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Text processing\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## 2. Configuration and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config-code"
   },
   "outputs": [],
   "source": [
    "# Model and data paths\n",
    "BASE_MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "LORA_MODEL_PATH = Path(\"outputs/final_model/lora_model\")\n",
    "TOKENIZED_DATA_DIR = Path(\"data/tokenized\")\n",
    "EVALUATION_RESULTS_DIR = Path(\"outputs/evaluation\")\n",
    "\n",
    "# Create evaluation results directory\n",
    "EVALUATION_RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Generation configuration\n",
    "GENERATION_CONFIG = {\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": None,  # Will be set when tokenizer is loaded\n",
    "    \"repetition_penalty\": 1.1\n",
    "}\n",
    "\n",
    "# Insurance task types\n",
    "TASK_TYPES = {\n",
    "    'CLAIM_CLASSIFICATION': 'Classification',\n",
    "    'POLICY_SUMMARIZATION': 'Summarization',\n",
    "    'FAQ_GENERATION': 'Generation',\n",
    "    'COMPLIANCE_CHECK': 'Analysis',\n",
    "    'CONTRACT_QA': 'Question Answering'\n",
    "}\n",
    "\n",
    "# Evaluation metrics configuration\n",
    "ROUGE_TYPES = ['rouge1', 'rouge2', 'rougeL']\n",
    "CLASSIFICATION_METRICS = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"- Base model: {BASE_MODEL_NAME}\")\n",
    "print(f\"- LoRA model path: {LORA_MODEL_PATH}\")\n",
    "print(f\"- Evaluation results: {EVALUATION_RESULTS_DIR}\")\n",
    "print(f\"- Max new tokens: {GENERATION_CONFIG['max_new_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load-model"
   },
   "source": [
    "## 3. Load Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-model-code"
   },
   "outputs": [],
   "source": [
    "def load_finetuned_model() -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\"Load the fine-tuned LoRA model and tokenizer\"\"\"\n",
    "    \n",
    "    print(f\"Loading fine-tuned model from {LORA_MODEL_PATH}...\")\n",
    "    \n",
    "    # Check if LoRA model exists\n",
    "    if not LORA_MODEL_PATH.exists():\n",
    "        print(f\"❌ LoRA model not found at {LORA_MODEL_PATH}\")\n",
    "        print(\"Please run notebook 03_finetuning_lora.ipynb first to train the model\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(LORA_MODEL_PATH)\n",
    "        \n",
    "        # Load base model\n",
    "        print(f\"Loading base model: {BASE_MODEL_NAME}\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            BASE_MODEL_NAME,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Load LoRA model\n",
    "        print(f\"Loading LoRA adapters...\")\n",
    "        model = PeftModel.from_pretrained(base_model, LORA_MODEL_PATH)\n",
    "        \n",
    "        # Set pad token for generation\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        GENERATION_CONFIG['pad_token_id'] = tokenizer.pad_token_id\n",
    "        \n",
    "        print(f\"✅ Model loaded successfully\")\n",
    "        print(f\"  Tokenizer vocab size: {len(tokenizer)}\")\n",
    "        print(f\"  Model device: {next(model.parameters()).device}\")\n",
    "        \n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def load_baseline_model() -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\"Load the baseline (non-fine-tuned) model for comparison\"\"\"\n",
    "    \n",
    "    print(f\"Loading baseline model: {BASE_MODEL_NAME}\")\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            BASE_MODEL_NAME,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        print(f\"✅ Baseline model loaded\")\n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not load baseline model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Load models\n",
    "print(\"Loading fine-tuned model...\")\n",
    "finetuned_model, finetuned_tokenizer = load_finetuned_model()\n",
    "\n",
    "if finetuned_model is None:\n",
    "    print(\"❌ Cannot proceed without a fine-tuned model\")\n",
    "else:\n",
    "    print(\"\\nOptionally loading baseline model for comparison...\")\n",
    "    baseline_model, baseline_tokenizer = load_baseline_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load-test-data"
   },
   "source": [
    "## 4. Load Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-test-data-code"
   },
   "outputs": [],
   "source": [
    "def load_test_datasets() -> Dict[str, Dataset]:\n",
    "    \"\"\"Load test datasets for evaluation\"\"\"\n",
    "    \n",
    "    print(f\"Loading test datasets from {TOKENIZED_DATA_DIR}...\")\n",
    "    \n",
    "    test_datasets = {}\n",
    "    \n",
    "    # Load combined dataset first\n",
    "    combined_test_path = TOKENIZED_DATA_DIR / \"combined\" / \"test_hf\"\n",
    "    if combined_test_path.exists():\n",
    "        try:\n",
    "            test_dataset = load_from_disk(combined_test_path)\n",
    "            test_datasets['combined'] = test_dataset\n",
    "            print(f\"✅ Combined test dataset: {len(test_dataset)} examples\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not load combined test dataset: {e}\")\n",
    "    \n",
    "    # Load individual task datasets\n",
    "    for task_name in TASK_TYPES.keys():\n",
    "        task_test_path = TOKENIZED_DATA_DIR / task_name.lower() / \"test_hf\"\n",
    "        if task_test_path.exists():\n",
    "            try:\n",
    "                task_dataset = load_from_disk(task_test_path)\n",
    "                test_datasets[task_name] = task_dataset\n",
    "                print(f\"✅ {task_name} test dataset: {len(task_dataset)} examples\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Could not load {task_name} test dataset: {e}\")\n",
    "    \n",
    "    if not test_datasets:\n",
    "        print(\"❌ No test datasets found\")\n",
    "        print(\"Please run the previous notebooks to create test data\")\n",
    "    \n",
    "    return test_datasets\n",
    "\n",
    "def load_original_test_data() -> List[Dict]:\n",
    "    \"\"\"Load original test data (before tokenization) for evaluation\"\"\"\n",
    "    \n",
    "    original_test_path = Path(\"data/processed/combined/test.json\")\n",
    "    \n",
    "    if original_test_path.exists():\n",
    "        try:\n",
    "            with open(original_test_path, 'r') as f:\n",
    "                original_test_data = json.load(f)\n",
    "            print(f\"✅ Original test data loaded: {len(original_test_data)} examples\")\n",
    "            return original_test_data\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not load original test data: {e}\")\n",
    "    else:\n",
    "        print(f\"⚠️ Original test data not found at {original_test_path}\")\n",
    "    \n",
    "    return []\n",
    "\n",
    "# Load test datasets\n",
    "if finetuned_model is not None:\n",
    "    test_datasets = load_test_datasets()\n",
    "    original_test_data = load_original_test_data()\n",
    "    \n",
    "    if test_datasets:\n",
    "        print(f\"\\n📊 Test Data Summary:\")\n",
    "        total_examples = sum(len(dataset) for dataset in test_datasets.values())\n",
    "        print(f\"  Total test examples: {total_examples}\")\n",
    "        \n",
    "        for name, dataset in test_datasets.items():\n",
    "            print(f\"  {name}: {len(dataset)} examples\")\nelse:\n",
    "    test_datasets = {}\n",
    "    original_test_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation-functions"
   },
   "source": [
    "## 5. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluation-functions-code"
   },
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt: str, generation_config: dict) -> str:\n",
    "    \"\"\"Generate response from model given a prompt\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                **generation_config,\n",
    "                use_cache=True\n",
    "            )\n",
    "        \n",
    "        # Decode response (remove input prompt)\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        generated_tokens = outputs[0][input_length:]\n",
    "        response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        return response.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error generating response: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def calculate_rouge_scores(predictions: List[str], references: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"Calculate ROUGE scores\"\"\"\n",
    "    \n",
    "    scorer = rouge_scorer.RougeScorer(ROUGE_TYPES, use_stemmer=True)\n",
    "    scores = {rouge_type: [] for rouge_type in ROUGE_TYPES}\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        rouge_scores = scorer.score(ref, pred)\n",
    "        for rouge_type in ROUGE_TYPES:\n",
    "            scores[rouge_type].append(rouge_scores[rouge_type].fmeasure)\n",
    "    \n",
    "    # Calculate average scores\n",
    "    avg_scores = {}\n",
    "    for rouge_type in ROUGE_TYPES:\n",
    "        avg_scores[f\"{rouge_type}_precision\"] = np.mean([scorer.score(ref, pred)[rouge_type].precision for pred, ref in zip(predictions, references)])\n",
    "        avg_scores[f\"{rouge_type}_recall\"] = np.mean([scorer.score(ref, pred)[rouge_type].recall for pred, ref in zip(predictions, references)])\n",
    "        avg_scores[f\"{rouge_type}_fmeasure\"] = np.mean(scores[rouge_type])\n",
    "    \n",
    "    return avg_scores\n",
    "\n",
    "def calculate_bleu_scores(predictions: List[str], references: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"Calculate BLEU scores\"\"\"\n",
    "    \n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_scores = []\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        # Tokenize\n",
    "        pred_tokens = word_tokenize(pred.lower())\n",
    "        ref_tokens = [word_tokenize(ref.lower())]\n",
    "        \n",
    "        # Calculate BLEU\n",
    "        try:\n",
    "            bleu_score = sentence_bleu(ref_tokens, pred_tokens, smoothing_function=smoothing)\n",
    "            bleu_scores.append(bleu_score)\n",
    "        except:\n",
    "            bleu_scores.append(0.0)\n",
    "    \n",
    "    return {\n",
    "        'bleu_score': np.mean(bleu_scores),\n",
    "        'bleu_std': np.std(bleu_scores)\n",
    "    }\n",
    "\n",
    "def calculate_classification_metrics(predictions: List[str], references: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"Calculate classification metrics (accuracy, precision, recall, F1)\"\"\"\n",
    "    \n",
    "    # Simple string matching for classification\n",
    "    y_true = references\n",
    "    y_pred = predictions\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def extract_answer_from_response(response: str, task_type: str) -> str:\n",
    "    \"\"\"Extract the actual answer from model response\"\"\"\n",
    "    \n",
    "    if not response:\n",
    "        return \"\"\n",
    "    \n",
    "    # Clean up the response\n",
    "    response = response.strip()\n",
    "    \n",
    "    # For classification, try to extract class name\n",
    "    if task_type == 'CLAIM_CLASSIFICATION':\n",
    "        # Look for patterns like \"This is a X claim\" or \"X claim\"\n",
    "        import re\n",
    "        patterns = [\n",
    "            r'This is an? ([\\w_]+) claim',\n",
    "            r'([\\w_]+) claim',\n",
    "            r'Category: ([\\w_]+)',\n",
    "            r'Type: ([\\w_]+)'\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, response, re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(1).lower().replace(' ', '_')\n",
    "    \n",
    "    # For other tasks, return first sentence or first 100 chars\n",
    "    sentences = sent_tokenize(response)\n",
    "    if sentences:\n",
    "        return sentences[0]\n",
    "    \n",
    "    return response[:100] if len(response) > 100 else response\n",
    "\n",
    "print(\"✅ Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run-evaluation"
   },
   "source": [
    "## 6. Run Comprehensive Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-evaluation-code"
   },
   "outputs": [],
   "source": [
    "def evaluate_model_on_dataset(model, tokenizer, test_examples: List[Dict], task_type: str) -> Dict:\n",
    "    \"\"\"Evaluate model on a specific dataset\"\"\"\n",
    "    \n",
    "    print(f\"\\nEvaluating {task_type} ({len(test_examples)} examples)...\")\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    prompts = []\n",
    "    \n",
    "    for example in tqdm(test_examples, desc=f\"Evaluating {task_type}\"):\n",
    "        # Create prompt\n",
    "        instruction = example.get('instruction', '')\n",
    "        input_text = example.get('input', '')\n",
    "        expected_output = example.get('output', '')\n",
    "        \n",
    "        prompt = f\"[INST] {instruction}\\n\\n{input_text} [/INST]\"\n",
    "        \n",
    "        # Generate prediction\n",
    "        response = generate_response(model, tokenizer, prompt, GENERATION_CONFIG)\n",
    "        \n",
    "        # Extract answer\n",
    "        prediction = extract_answer_from_response(response, task_type)\n",
    "        \n",
    "        predictions.append(prediction)\n",
    "        references.append(expected_output)\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    # Calculate metrics based on task type\n",
    "    metrics = {}\n",
    "    \n",
    "    if task_type in ['POLICY_SUMMARIZATION', 'FAQ_GENERATION', 'COMPLIANCE_CHECK']:\n",
    "        # Text generation tasks - use ROUGE and BLEU\n",
    "        rouge_scores = calculate_rouge_scores(predictions, references)\n",
    "        bleu_scores = calculate_bleu_scores(predictions, references)\n",
    "        metrics.update(rouge_scores)\n",
    "        metrics.update(bleu_scores)\n",
    "    \n",
    "    elif task_type == 'CLAIM_CLASSIFICATION':\n",
    "        # Classification task - use accuracy, precision, recall, F1\n",
    "        classification_metrics = calculate_classification_metrics(predictions, references)\n",
    "        metrics.update(classification_metrics)\n",
    "    \n",
    "    elif task_type == 'CONTRACT_QA':\n",
    "        # QA task - use both ROUGE and exact match\n",
    "        rouge_scores = calculate_rouge_scores(predictions, references)\n",
    "        exact_matches = [pred.lower().strip() == ref.lower().strip() for pred, ref in zip(predictions, references)]\n",
    "        metrics.update(rouge_scores)\n",
    "        metrics['exact_match'] = np.mean(exact_matches)\n",
    "    \n",
    "    # Add sample predictions for review\n",
    "    num_samples = min(5, len(predictions))\n",
    "    samples = []\n",
    "    for i in range(num_samples):\n",
    "        samples.append({\n",
    "            'prompt': prompts[i][:200] + '...' if len(prompts[i]) > 200 else prompts[i],\n",
    "            'prediction': predictions[i],\n",
    "            'reference': references[i],\n",
    "            'match': predictions[i].lower().strip() == references[i].lower().strip()\n",
    "        })\n",
    "    \n",
    "    results = {\n",
    "        'task_type': task_type,\n",
    "        'num_examples': len(test_examples),\n",
    "        'metrics': metrics,\n",
    "        'samples': samples\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_full_evaluation() -> Dict[str, Dict]:\n",
    "    \"\"\"Run evaluation on all available test datasets\"\"\"\n",
    "    \n",
    "    if not original_test_data:\n",
    "        print(\"❌ No test data available for evaluation\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"🔍 Starting comprehensive evaluation...\")\n",
    "    print(f\"Evaluating on {len(original_test_data)} test examples\")\n",
    "    \n",
    "    # Group examples by task type\n",
    "    task_examples = {}\n",
    "    for example in original_test_data:\n",
    "        task_type = example.get('task_type', 'POLICY_SUMMARIZATION')\n",
    "        if task_type not in task_examples:\n",
    "            task_examples[task_type] = []\n",
    "        task_examples[task_type].append(example)\n",
    "    \n",
    "    print(f\"\\nTask distribution:\")\n",
    "    for task_type, examples in task_examples.items():\n",
    "        print(f\"  {task_type}: {len(examples)} examples\")\n",
    "    \n",
    "    # Evaluate each task\n",
    "    evaluation_results = {}\n",
    "    \n",
    "    for task_type, examples in task_examples.items():\n",
    "        if examples:  # Only evaluate if we have examples\n",
    "            results = evaluate_model_on_dataset(finetuned_model, finetuned_tokenizer, examples, task_type)\n",
    "            evaluation_results[task_type] = results\n",
    "    \n",
    "    return evaluation_results\n",
    "\n",
    "# Run evaluation if model and data are available\n",
    "if finetuned_model is not None and original_test_data:\n",
    "    evaluation_results = run_full_evaluation()\n",
    "    \n",
    "    print(f\"\\n✅ Evaluation completed!\")\n",
    "    print(f\"Evaluated {len(evaluation_results)} task types\")\nelse:\n",
    "    print(\"⚠️ Skipping evaluation - model or test data not available\")\n",
    "    evaluation_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results-analysis"
   },
   "source": [
    "## 7. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "results-analysis-code"
   },
   "outputs": [],
   "source": [
    "def create_results_summary(evaluation_results: Dict[str, Dict]) -> pd.DataFrame:\n",
    "    \"\"\"Create a summary DataFrame of evaluation results\"\"\"\n",
    "    \n",
    "    if not evaluation_results:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    summary_data = []\n",
    "    \n",
    "    for task_type, results in evaluation_results.items():\n",
    "        row = {\n",
    "            'Task Type': task_type,\n",
    "            'Task Name': TASK_TYPES.get(task_type, task_type),\n",
    "            'Examples': results['num_examples']\n",
    "        }\n",
    "        \n",
    "        # Add metrics\n",
    "        metrics = results['metrics']\n",
    "        \n",
    "        # Add relevant metrics based on task type\n",
    "        if 'accuracy' in metrics:\n",
    "            row['Accuracy'] = f\"{metrics['accuracy']:.3f}\"\n",
    "        if 'f1' in metrics:\n",
    "            row['F1 Score'] = f\"{metrics['f1']:.3f}\"\n",
    "        if 'rouge1_fmeasure' in metrics:\n",
    "            row['ROUGE-1'] = f\"{metrics['rouge1_fmeasure']:.3f}\"\n",
    "        if 'rouge2_fmeasure' in metrics:\n",
    "            row['ROUGE-2'] = f\"{metrics['rouge2_fmeasure']:.3f}\"\n",
    "        if 'rougeL_fmeasure' in metrics:\n",
    "            row['ROUGE-L'] = f\"{metrics['rougeL_fmeasure']:.3f}\"\n",
    "        if 'bleu_score' in metrics:\n",
    "            row['BLEU'] = f\"{metrics['bleu_score']:.3f}\"\n",
    "        if 'exact_match' in metrics:\n",
    "            row['Exact Match'] = f\"{metrics['exact_match']:.3f}\"\n",
    "        \n",
    "        summary_data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(summary_data)\n",
    "\n",
    "def plot_evaluation_metrics(evaluation_results: Dict[str, Dict]):\n",
    "    \"\"\"Create visualizations of evaluation metrics\"\"\"\n",
    "    \n",
    "    if not evaluation_results:\n",
    "        print(\"No evaluation results to plot\")\n",
    "        return\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('LLaMA Insurance Model Evaluation Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Task Performance Overview\n",
    "    task_names = []\n",
    "    primary_scores = []\n",
    "    \n",
    "    for task_type, results in evaluation_results.items():\n",
    "        task_names.append(TASK_TYPES.get(task_type, task_type))\n",
    "        metrics = results['metrics']\n",
    "        \n",
    "        # Choose primary metric based on task type\n",
    "        if 'accuracy' in metrics:\n",
    "            primary_scores.append(metrics['accuracy'])\n",
    "        elif 'rouge1_fmeasure' in metrics:\n",
    "            primary_scores.append(metrics['rouge1_fmeasure'])\n",
    "        elif 'exact_match' in metrics:\n",
    "            primary_scores.append(metrics['exact_match'])\n",
    "        else:\n",
    "            primary_scores.append(0.0)\n",
    "    \n",
    "    axes[0, 0].bar(task_names, primary_scores, color='skyblue', alpha=0.7)\n",
    "    axes[0, 0].set_title('Primary Metric by Task')\n",
    "    axes[0, 0].set_ylabel('Score')\n",
    "    axes[0, 0].set_ylim(0, 1)\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, score in enumerate(primary_scores):\n",
    "        axes[0, 0].text(i, score + 0.01, f'{score:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Plot 2: ROUGE Scores (for text generation tasks)\n",
    "    rouge_data = {'Task': [], 'ROUGE-1': [], 'ROUGE-2': [], 'ROUGE-L': []}\n",
    "    \n",
    "    for task_type, results in evaluation_results.items():\n",
    "        metrics = results['metrics']\n",
    "        if 'rouge1_fmeasure' in metrics:\n",
    "            rouge_data['Task'].append(TASK_TYPES.get(task_type, task_type))\n",
    "            rouge_data['ROUGE-1'].append(metrics.get('rouge1_fmeasure', 0))\n",
    "            rouge_data['ROUGE-2'].append(metrics.get('rouge2_fmeasure', 0))\n",
    "            rouge_data['ROUGE-L'].append(metrics.get('rougeL_fmeasure', 0))\n",
    "    \n",
    "    if rouge_data['Task']:\n",
    "        rouge_df = pd.DataFrame(rouge_data)\n",
    "        rouge_df.set_index('Task').plot(kind='bar', ax=axes[0, 1], alpha=0.7)\n",
    "        axes[0, 1].set_title('ROUGE Scores by Task')\n",
    "        axes[0, 1].set_ylabel('ROUGE Score')\n",
    "        axes[0, 1].legend(loc='upper right')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    else:\n",
    "        axes[0, 1].text(0.5, 0.5, 'No ROUGE scores available', ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "        axes[0, 1].set_title('ROUGE Scores by Task')\n",
    "    \n",
    "    # Plot 3: Sample Size Distribution\n",
    "    sample_sizes = [results['num_examples'] for results in evaluation_results.values()]\n",
    "    task_labels = [TASK_TYPES.get(task_type, task_type) for task_type in evaluation_results.keys()]\n",
    "    \n",
    "    axes[1, 0].pie(sample_sizes, labels=task_labels, autopct='%1.1f%%', startangle=90)\n",
    "    axes[1, 0].set_title('Test Sample Distribution')\n",
    "    \n",
    "    # Plot 4: Metric Comparison\n",
    "    metric_comparison = {'Metric': [], 'Average Score': []}\n",
    "    \n",
    "    # Collect all metrics\n",
    "    all_metrics = {}\n",
    "    for results in evaluation_results.values():\n",
    "        for metric, value in results['metrics'].items():\n",
    "            if metric.endswith('_fmeasure') or metric in ['accuracy', 'f1', 'exact_match', 'bleu_score']:\n",
    "                if metric not in all_metrics:\n",
    "                    all_metrics[metric] = []\n",
    "                all_metrics[metric].append(value)\n",
    "    \n",
    "    for metric, values in all_metrics.items():\n",
    "        if values:  # Only include metrics with values\n",
    "            metric_comparison['Metric'].append(metric.replace('_fmeasure', '').replace('_', ' ').title())\n",
    "            metric_comparison['Average Score'].append(np.mean(values))\n",
    "    \n",
    "    if metric_comparison['Metric']:\n",
    "        axes[1, 1].barh(metric_comparison['Metric'], metric_comparison['Average Score'], alpha=0.7)\n",
    "        axes[1, 1].set_title('Average Metric Scores')\n",
    "        axes[1, 1].set_xlabel('Average Score')\n",
    "        axes[1, 1].set_xlim(0, 1)\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'No metrics available', ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "        axes[1, 1].set_title('Average Metric Scores')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_file = EVALUATION_RESULTS_DIR / 'evaluation_plots.png'\n",
    "    plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✅ Evaluation plots saved to: {plot_file}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def display_sample_predictions(evaluation_results: Dict[str, Dict]):\n",
    "    \"\"\"Display sample predictions for manual review\"\"\"\n",
    "    \n",
    "    if not evaluation_results:\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SAMPLE PREDICTIONS FOR MANUAL REVIEW\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for task_type, results in evaluation_results.items():\n",
    "        print(f\"\\n🔍 {task_type} ({TASK_TYPES.get(task_type, task_type)})\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for i, sample in enumerate(results['samples'][:3], 1):  # Show first 3 samples\n",
    "            print(f\"\\nExample {i}:\")\n",
    "            print(f\"Prompt: {sample['prompt']}\")\n",
    "            print(f\"Prediction: {sample['prediction']}\")\n",
    "            print(f\"Reference: {sample['reference']}\")\n",
    "            print(f\"Match: {'✅' if sample['match'] else '❌'}\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "# Generate results analysis if we have evaluation results\n",
    "if evaluation_results:\n",
    "    print(\"\\n📊 Generating results analysis...\")\n",
    "    \n",
    "    # Create summary table\n",
    "    results_summary = create_results_summary(evaluation_results)\n",
    "    \n",
    "    print(\"\\n📈 EVALUATION RESULTS SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    if not results_summary.empty:\n",
    "        print(results_summary.to_string(index=False))\n",
    "    else:\n",
    "        print(\"No results to display\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    plot_evaluation_metrics(evaluation_results)\n",
    "    \n",
    "    # Display sample predictions\n",
    "    display_sample_predictions(evaluation_results)\n",
    "    \nelse:\n",
    "    print(\"⚠️ No evaluation results to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save-results"
   },
   "source": [
    "## 8. Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-results-code"
   },
   "outputs": [],
   "source": [
    "def save_evaluation_results(evaluation_results: Dict[str, Dict]):\n",
    "    \"\"\"Save evaluation results to files\"\"\"\n",
    "    \n",
    "    if not evaluation_results:\n",
    "        print(\"No evaluation results to save\")\n",
    "        return\n",
    "    \n",
    "    print(f\"💾 Saving evaluation results to {EVALUATION_RESULTS_DIR}...\")\n",
    "    \n",
    "    # Save detailed results as JSON\n",
    "    results_file = EVALUATION_RESULTS_DIR / 'evaluation_results.json'\n",
    "    \n",
    "    # Prepare results for JSON serialization\n",
    "    json_results = {}\n",
    "    for task_type, results in evaluation_results.items():\n",
    "        json_results[task_type] = {\n",
    "            'task_type': results['task_type'],\n",
    "            'num_examples': results['num_examples'],\n",
    "            'metrics': results['metrics'],\n",
    "            'samples': results['samples']  # Include sample predictions\n",
    "        }\n",
    "    \n",
    "    # Add metadata\n",
    "    evaluation_metadata = {\n",
    "        'evaluation_date': datetime.now().isoformat(),\n",
    "        'model_path': str(LORA_MODEL_PATH),\n",
    "        'base_model': BASE_MODEL_NAME,\n",
    "        'generation_config': GENERATION_CONFIG,\n",
    "        'total_examples': sum(results['num_examples'] for results in evaluation_results.values()),\n",
    "        'tasks_evaluated': list(evaluation_results.keys())\n",
    "    }\n",
    "    \n",
    "    final_results = {\n",
    "        'metadata': evaluation_metadata,\n",
    "        'results': json_results\n",
    "    }\n",
    "    \n",
    "    with open(results_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"✅ Detailed results saved to: {results_file}\")\n",
    "    \n",
    "    # Save summary table as CSV\n",
    "    results_summary = create_results_summary(evaluation_results)\n",
    "    if not results_summary.empty:\n",
    "        csv_file = EVALUATION_RESULTS_DIR / 'evaluation_summary.csv'\n",
    "        results_summary.to_csv(csv_file, index=False)\n",
    "        print(f\"✅ Summary CSV saved to: {csv_file}\")\n",
    "    \n",
    "    # Create evaluation report\n",
    "    create_evaluation_report(evaluation_results, evaluation_metadata)\n",
    "\n",
    "def create_evaluation_report(evaluation_results: Dict[str, Dict], metadata: Dict):\n",
    "    \"\"\"Create a comprehensive evaluation report\"\"\"\n",
    "    \n",
    "    report_file = EVALUATION_RESULTS_DIR / 'evaluation_report.md'\n",
    "    \n",
    "    report_content = f\"\"\"\n",
    "# LLaMA Insurance Model Evaluation Report\n",
    "\n",
    "## Overview\n",
    "- **Evaluation Date**: {metadata['evaluation_date']}\n",
    "- **Base Model**: {metadata['base_model']}\n",
    "- **Fine-tuned Model**: {metadata['model_path']}\n",
    "- **Total Test Examples**: {metadata['total_examples']}\n",
    "- **Tasks Evaluated**: {len(metadata['tasks_evaluated'])}\n",
    "\n",
    "## Generation Configuration\n",
    "```json\n",
    "{json.dumps(metadata['generation_config'], indent=2)}\n",
    "```\n",
    "\n",
    "## Results by Task\n",
    "\"\"\"\n",
    "    \n",
    "    for task_type, results in evaluation_results.items():\n",
    "        task_name = TASK_TYPES.get(task_type, task_type)\n",
    "        metrics = results['metrics']\n",
    "        \n",
    "        report_content += f\"\"\"\n",
    "### {task_name} ({task_type})\n",
    "- **Examples**: {results['num_examples']}\n",
    "\n",
    "#### Metrics\n",
    "\"\"\"\n",
    "        \n",
    "        for metric, value in metrics.items():\n",
    "            if isinstance(value, float):\n",
    "                report_content += f\"- **{metric.replace('_', ' ').title()}**: {value:.4f}\\n\"\n",
    "            else:\n",
    "                report_content += f\"- **{metric.replace('_', ' ').title()}**: {value}\\n\"\n",
    "        \n",
    "        report_content += \"\\n#### Sample Predictions\\n\"\n",
    "        \n",
    "        for i, sample in enumerate(results['samples'][:2], 1):\n",
    "            report_content += f\"\"\"\n",
    "**Example {i}:**\n",
    "- **Input**: {sample['prompt'][:150]}...\n",
    "- **Prediction**: {sample['prediction']}\n",
    "- **Reference**: {sample['reference']}\n",
    "- **Match**: {'✅' if sample['match'] else '❌'}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    report_content += \"\"\"\n",
    "## Analysis and Recommendations\n",
    "\n",
    "### Strengths\n",
    "- The model shows competency across multiple insurance-specific tasks\n",
    "- Fine-tuning has successfully adapted the base LLaMA model to the insurance domain\n",
    "\n",
    "### Areas for Improvement\n",
    "- Consider increasing training data for tasks with lower performance\n",
    "- Experiment with different generation parameters for better outputs\n",
    "- Add more diverse examples to improve generalization\n",
    "\n",
    "### Next Steps\n",
    "1. Collect additional training data for underperforming tasks\n",
    "2. Fine-tune generation parameters based on task-specific requirements\n",
    "3. Consider ensemble methods or multi-stage training\n",
    "4. Deploy model for user testing and feedback collection\n",
    "\n",
    "---\n",
    "*Report generated automatically by LLaMA Insurance Evaluation Pipeline*\n",
    "\"\"\"\n",
    "    \n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(report_content.strip())\n",
    "    \n",
    "    print(f\"✅ Evaluation report saved to: {report_file}\")\n",
    "\n",
    "# Save results if available\n",
    "if evaluation_results:\n",
    "    save_evaluation_results(evaluation_results)\n",
    "    \n",
    "    print(f\"\\n🎉 Evaluation Complete!\")\n",
    "    print(f\"\\nResults saved to: {EVALUATION_RESULTS_DIR}\")\n",
    "    print(f\"\\nFiles created:\")\n",
    "    print(f\"- evaluation_results.json (detailed results)\")\n",
    "    print(f\"- evaluation_summary.csv (summary table)\")\n",
    "    print(f\"- evaluation_report.md (comprehensive report)\")\n",
    "    print(f\"- evaluation_plots.png (visualizations)\")\n",
    "    \n",
    "    print(f\"\\nNext steps:\")\n",
    "    print(f\"1. Review the evaluation report for insights\")\n",
    "    print(f\"2. Run 05_inference_demo.ipynb to test the model interactively\")\n",
    "    print(f\"3. Consider additional training or data collection based on results\")\n",
    "else:\n",
    "    print(\"⚠️ No evaluation results to save\")\n",
    "    print(\"Please ensure you have:\")\n",
    "    print(\"1. A trained model from notebook 03\")\n",
    "    print(\"2. Test data from notebook 01\")\n",
    "    print(\"3. Processed datasets from notebook 02\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
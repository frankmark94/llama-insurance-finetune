{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üöÄ Insurance LLaMA Interactive Demo\n",
    "\n",
    "This notebook provides an interactive demo of the fine-tuned LLaMA model for insurance tasks:\n",
    "\n",
    "## What this notebook does:\n",
    "1. Load the fine-tuned insurance model\n",
    "2. Provide interactive demos for each task type\n",
    "3. Allow custom input testing\n",
    "4. Compare with baseline model (optional)\n",
    "5. Showcase real-world insurance use cases\n",
    "6. Generate sample outputs for different scenarios\n",
    "\n",
    "**‚ö†Ô∏è Important: Make sure you have a trained model from notebook 03**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports-code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "# Core ML libraries\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "\n",
    "# Interactive widgets for Colab/Jupyter\n",
    "try:\n",
    "    from ipywidgets import interact, widgets, Layout\n",
    "    from IPython.display import display, HTML, clear_output\n",
    "    WIDGETS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"ipywidgets not available - using basic interface\")\n",
    "    WIDGETS_AVAILABLE = False\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"Interactive widgets: {'Available' if WIDGETS_AVAILABLE else 'Not available'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## 2. Configuration and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config-code"
   },
   "outputs": [],
   "source": [
    "# Model paths\n",
    "BASE_MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "LORA_MODEL_PATH = Path(\"outputs/final_model/lora_model\")\n",
    "\n",
    "# Generation configurations for different tasks\n",
    "GENERATION_CONFIGS = {\n",
    "    'creative': {\n",
    "        'max_new_tokens': 512,\n",
    "        'temperature': 0.8,\n",
    "        'top_p': 0.9,\n",
    "        'top_k': 50,\n",
    "        'do_sample': True,\n",
    "        'repetition_penalty': 1.1\n",
    "    },\n",
    "    'factual': {\n",
    "        'max_new_tokens': 256,\n",
    "        'temperature': 0.3,\n",
    "        'top_p': 0.8,\n",
    "        'top_k': 40,\n",
    "        'do_sample': True,\n",
    "        'repetition_penalty': 1.05\n",
    "    },\n",
    "    'precise': {\n",
    "        'max_new_tokens': 128,\n",
    "        'temperature': 0.1,\n",
    "        'top_p': 0.7,\n",
    "        'top_k': 20,\n",
    "        'do_sample': True,\n",
    "        'repetition_penalty': 1.02\n",
    "    }\n",
    "}\n",
    "\n",
    "# Insurance task templates\n",
    "TASK_TEMPLATES = {\n",
    "    'CLAIM_CLASSIFICATION': {\n",
    "        'instruction': 'Classify this insurance claim into the appropriate category (auto, health, life, property, etc.):',\n",
    "        'config': 'precise'\n",
    "    },\n",
    "    'POLICY_SUMMARIZATION': {\n",
    "        'instruction': 'Summarize the key points of this insurance policy in simple terms:',\n",
    "        'config': 'factual'\n",
    "    },\n",
    "    'FAQ_GENERATION': {\n",
    "        'instruction': 'Generate 3-5 frequently asked questions and answers based on this insurance information:',\n",
    "        'config': 'creative'\n",
    "    },\n",
    "    'COMPLIANCE_CHECK': {\n",
    "        'instruction': 'Identify the key compliance and regulatory requirements mentioned in this insurance document:',\n",
    "        'config': 'factual'\n",
    "    },\n",
    "    'CONTRACT_QA': {\n",
    "        'instruction': 'Answer this question based on the insurance contract information provided:',\n",
    "        'config': 'precise'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"- Base model: {BASE_MODEL_NAME}\")\n",
    "print(f\"- LoRA model path: {LORA_MODEL_PATH}\")\n",
    "print(f\"- Generation configs: {list(GENERATION_CONFIGS.keys())}\")\n",
    "print(f\"- Available tasks: {list(TASK_TEMPLATES.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load-model"
   },
   "source": [
    "## 3. Load Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-model-code"
   },
   "outputs": [],
   "source": [
    "class InsuranceLLaMAModel:\n",
    "    \"\"\"Wrapper class for the fine-tuned insurance LLaMA model\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: Path, base_model_name: str):\n",
    "        self.model_path = model_path\n",
    "        self.base_model_name = base_model_name\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the fine-tuned model and tokenizer\"\"\"\n",
    "        \n",
    "        print(f\"Loading fine-tuned model from {self.model_path}...\")\n",
    "        \n",
    "        if not self.model_path.exists():\n",
    "            raise FileNotFoundError(f\"Model not found at {self.model_path}. Please run training notebook first.\")\n",
    "        \n",
    "        try:\n",
    "            # Load tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "            \n",
    "            # Load base model\n",
    "            print(f\"Loading base model: {self.base_model_name}\")\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.base_model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            # Load LoRA model\n",
    "            print(f\"Loading LoRA adapters...\")\n",
    "            self.model = PeftModel.from_pretrained(base_model, self.model_path)\n",
    "            \n",
    "            # Set pad token\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            print(f\"‚úÖ Model loaded successfully!\")\n",
    "            print(f\"  Device: {next(self.model.parameters()).device}\")\n",
    "            print(f\"  Vocab size: {len(self.tokenizer)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_response(self, prompt: str, generation_config: Dict) -> str:\n",
    "        \"\"\"Generate response from the model\"\"\"\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"Model not loaded. Call load_model() first.\")\n",
    "        \n",
    "        try:\n",
    "            # Add pad_token_id to generation config\n",
    "            gen_config = generation_config.copy()\n",
    "            gen_config['pad_token_id'] = self.tokenizer.pad_token_id\n",
    "            gen_config['eos_token_id'] = self.tokenizer.eos_token_id\n",
    "            \n",
    "            # Tokenize input\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048)\n",
    "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    **gen_config,\n",
    "                    use_cache=True\n",
    "                )\n",
    "            \n",
    "            # Decode response (remove input prompt)\n",
    "            input_length = inputs['input_ids'].shape[1]\n",
    "            generated_tokens = outputs[0][input_length:]\n",
    "            response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "            \n",
    "            return response.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error generating response: {e}\")\n",
    "            return \"Error generating response. Please try again.\"\n",
    "    \n",
    "    def format_insurance_prompt(self, task_type: str, user_input: str, context: str = \"\") -> str:\n",
    "        \"\"\"Format input into proper instruction prompt for insurance tasks\"\"\"\n",
    "        \n",
    "        if task_type not in TASK_TEMPLATES:\n",
    "            raise ValueError(f\"Unknown task type: {task_type}\")\n",
    "        \n",
    "        template = TASK_TEMPLATES[task_type]\n",
    "        instruction = template['instruction']\n",
    "        \n",
    "        if task_type == 'CONTRACT_QA' and context:\n",
    "            full_input = f\"Context: {context}\\n\\nQuestion: {user_input}\"\n",
    "        else:\n",
    "            full_input = user_input\n",
    "        \n",
    "        prompt = f\"[INST] {instruction}\\n\\n{full_input} [/INST]\"\n",
    "        return prompt\n",
    "    \n",
    "    def process_insurance_task(self, task_type: str, user_input: str, context: str = \"\") -> Dict:\n",
    "        \"\"\"Process a complete insurance task\"\"\"\n",
    "        \n",
    "        # Format prompt\n",
    "        prompt = self.format_insurance_prompt(task_type, user_input, context)\n",
    "        \n",
    "        # Get generation config\n",
    "        config_name = TASK_TEMPLATES[task_type]['config']\n",
    "        generation_config = GENERATION_CONFIGS[config_name]\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.generate_response(prompt, generation_config)\n",
    "        \n",
    "        return {\n",
    "            'task_type': task_type,\n",
    "            'user_input': user_input,\n",
    "            'context': context,\n",
    "            'prompt': prompt,\n",
    "            'response': response,\n",
    "            'config_used': config_name,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "# Initialize and load the model\n",
    "print(\"Initializing Insurance LLaMA Model...\")\n",
    "insurance_model = InsuranceLLaMAModel(LORA_MODEL_PATH, BASE_MODEL_NAME)\n",
    "\n",
    "try:\n",
    "    insurance_model.load_model()\n",
    "    MODEL_LOADED = True\n",
    "    print(\"\\nüéâ Model ready for inference!\")\nexcept Exception as e:\n",
    "    print(f\"‚ùå Failed to load model: {e}\")\n",
    "    print(\"\\nPlease ensure you have:\")\n",
    "    print(\"1. Completed training in notebook 03_finetuning_lora.ipynb\")\n",
    "    print(\"2. Model files saved in outputs/final_model/lora_model/\")\n",
    "    print(\"3. Sufficient GPU memory available\")\n",
    "    MODEL_LOADED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sample-demos"
   },
   "source": [
    "## 4. Sample Insurance Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sample-demos-code"
   },
   "outputs": [],
   "source": [
    "# Sample insurance data for demos\n",
    "SAMPLE_SCENARIOS = {\n",
    "    'CLAIM_CLASSIFICATION': [\n",
    "        \"Vehicle collision on I-95 involving two cars. Front-end damage to insured vehicle. No injuries reported. Police report filed. Towing required.\",\n",
    "        \"Water damage to kitchen due to burst pipe. Hardwood floors damaged, cabinets need replacement. Plumber has been contacted.\",\n",
    "        \"Annual wellness checkup at primary care physician. Routine blood work and physical examination. No issues found.\",\n",
    "        \"House fire caused by electrical fault. Significant damage to roof and upper floor. Fire department response required.\"\n",
    "    ],\n",
    "    'POLICY_SUMMARIZATION': [\n",
    "        \"\"\"COMPREHENSIVE AUTO INSURANCE POLICY\n",
    "        \n",
    "Coverage Limits: Bodily Injury Liability $100,000 per person, $300,000 per accident. Property Damage Liability $50,000 per accident. Comprehensive and Collision coverage with $500 deductible each.\n",
    "\n",
    "Premium: $1,200 annually, paid in monthly installments of $100. Late payment fee of $25 applies after 10-day grace period.\n",
    "\n",
    "Exclusions: Racing, commercial use, intentional damage, normal wear and tear, mechanical breakdown not covered.\n",
    "\n",
    "Additional Benefits: Roadside assistance, rental car coverage up to $30/day for 30 days, new car replacement if total loss occurs within first year.\"\"\",\n",
    "        \n",
    "        \"\"\"HEALTH INSURANCE POLICY\n",
    "        \n",
    "Plan Type: PPO with nationwide network coverage. Annual deductible $2,000 individual, $4,000 family.\n",
    "\n",
    "Coverage: 80% coinsurance after deductible for in-network providers, 60% for out-of-network. Preventive care covered 100%.\n",
    "\n",
    "Prescription Benefits: $10 generic, $30 brand name, $60 specialty drugs. 90-day supply available by mail.\n",
    "\n",
    "Maximum Out-of-Pocket: $8,000 individual, $16,000 family per year.\"\"\"\n",
    "    ],\n",
    "    'FAQ_GENERATION': [\n",
    "        \"\"\"HOME INSURANCE BASICS\n",
    "        \n",
    "Homeowners insurance protects your home and personal belongings from covered perils like fire, theft, and storms. It also provides liability coverage if someone is injured on your property.\n",
    "\n",
    "Standard policies cover the dwelling, other structures, personal property, and liability. Additional living expenses are covered if your home becomes uninhabitable.\n",
    "\n",
    "Common exclusions include floods, earthquakes, and normal wear and tear. These may require separate policies or endorsements.\"\"\",\n",
    "        \n",
    "        \"\"\"LIFE INSURANCE OVERVIEW\n",
    "        \n",
    "Life insurance provides financial protection for your beneficiaries when you die. Term life provides coverage for a specific period, while whole life provides permanent coverage with cash value.\n",
    "\n",
    "Premiums depend on age, health, coverage amount, and policy type. Medical exams are typically required for larger policies.\n",
    "\n",
    "Beneficiaries receive death benefits tax-free. Policies can also be used for estate planning and business protection.\"\"\"\n",
    "    ],\n",
    "    'COMPLIANCE_CHECK': [\n",
    "        \"\"\"INSURANCE COMPANY OPERATIONS\n",
    "        \n",
    "All insurance operations must comply with state insurance regulations and maintain adequate reserves. Companies must file annual financial statements with state commissioners.\n",
    "\n",
    "Consumer protection laws require clear policy language, fair claims handling, and prompt payment of valid claims. Unfair trade practices are prohibited.\n",
    "\n",
    "Health insurance operations must comply with HIPAA privacy requirements and ACA regulations including essential health benefits and pre-existing condition protections.\n",
    "\n",
    "All marketing materials must be approved by the state and cannot be misleading or deceptive.\"\"\"\n",
    "    ],\n",
    "    'CONTRACT_QA': [\n",
    "        {\n",
    "            'context': \"\"\"AUTO INSURANCE POLICY - SECTION 4: COLLISION COVERAGE\n",
    "            \n",
    "We will pay for direct and accidental loss to your covered auto caused by collision with another object or by upset of your covered auto. We will pay the lesser of: (1) actual cash value minus applicable deductible, or (2) amount necessary to repair or replace the property minus applicable deductible.\n",
    "            \n",
    "Your deductible is $500 per occurrence. Collision coverage applies only if you have purchased this coverage, as shown in the Declarations.\"\"\",\n",
    "            'questions': [\n",
    "                \"What is my collision deductible?\",\n",
    "                \"Does collision coverage apply to all accidents?\",\n",
    "                \"How much will I receive if my car is totaled?\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Sample scenarios loaded\")\n",
    "print(f\"Available demo scenarios:\")\n",
    "for task, scenarios in SAMPLE_SCENARIOS.items():\n",
    "    if task == 'CONTRACT_QA':\n",
    "        print(f\"  {task}: {len(scenarios[0]['questions'])} Q&A examples\")\n",
    "    else:\n",
    "        print(f\"  {task}: {len(scenarios)} scenarios\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "demo-functions"
   },
   "source": [
    "## 5. Demo Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "demo-functions-code"
   },
   "outputs": [],
   "source": [
    "def run_sample_demo(task_type: str, sample_index: int = 0):\n",
    "    \"\"\"Run a sample demo for a specific task type\"\"\"\n",
    "    \n",
    "    if not MODEL_LOADED:\n",
    "        print(\"‚ùå Model not loaded. Please load the model first.\")\n",
    "        return\n",
    "    \n",
    "    if task_type not in SAMPLE_SCENARIOS:\n",
    "        print(f\"‚ùå Unknown task type: {task_type}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üéØ {task_type} Demo\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    scenarios = SAMPLE_SCENARIOS[task_type]\n",
    "    \n",
    "    if task_type == 'CONTRACT_QA':\n",
    "        # Handle Q&A scenarios\n",
    "        scenario = scenarios[0]\n",
    "        context = scenario['context']\n",
    "        questions = scenario['questions']\n",
    "        \n",
    "        print(f\"Context: {context[:200]}...\")\n",
    "        print(f\"\\nAnswering {len(questions)} questions:\\n\")\n",
    "        \n",
    "        for i, question in enumerate(questions, 1):\n",
    "            print(f\"Q{i}: {question}\")\n",
    "            \n",
    "            result = insurance_model.process_insurance_task(task_type, question, context)\n",
    "            \n",
    "            print(f\"A{i}: {result['response']}\")\n",
    "            print(\"-\" * 40)\n",
    "    \n",
    "    else:\n",
    "        # Handle other task types\n",
    "        if sample_index >= len(scenarios):\n",
    "            sample_index = 0\n",
    "        \n",
    "        scenario = scenarios[sample_index]\n",
    "        \n",
    "        print(f\"Input: {scenario[:300]}{'...' if len(scenario) > 300 else ''}\")\n",
    "        print(f\"\\nProcessing...\")\n",
    "        \n",
    "        result = insurance_model.process_insurance_task(task_type, scenario)\n",
    "        \n",
    "        print(f\"\\nOutput:\")\n",
    "        print(f\"{result['response']}\")\n",
    "        print(f\"\\nConfig used: {result['config_used']}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "def run_custom_demo(task_type: str, user_input: str, context: str = \"\"):\n",
    "    \"\"\"Run a custom demo with user input\"\"\"\n",
    "    \n",
    "    if not MODEL_LOADED:\n",
    "        print(\"‚ùå Model not loaded. Please load the model first.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üéØ Custom {task_type} Demo\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if context:\n",
    "        print(f\"Context: {context[:200]}{'...' if len(context) > 200 else ''}\")\n",
    "    \n",
    "    print(f\"Input: {user_input}\")\n",
    "    print(f\"\\nProcessing...\")\n",
    "    \n",
    "    result = insurance_model.process_insurance_task(task_type, user_input, context)\n",
    "    \n",
    "    print(f\"\\nOutput:\")\n",
    "    print(f\"{result['response']}\")\n",
    "    print(f\"\\nTask: {task_type} | Config: {result['config_used']}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def compare_generation_configs(task_type: str, user_input: str, context: str = \"\"):\n",
    "    \"\"\"Compare different generation configurations\"\"\"\n",
    "    \n",
    "    if not MODEL_LOADED:\n",
    "        print(\"‚ùå Model not loaded. Please load the model first.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üî¨ Generation Config Comparison: {task_type}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    prompt = insurance_model.format_insurance_prompt(task_type, user_input, context)\n",
    "    \n",
    "    for config_name, config in GENERATION_CONFIGS.items():\n",
    "        print(f\"\\nüìä {config_name.upper()} Configuration:\")\n",
    "        print(f\"Temperature: {config['temperature']} | Top-p: {config['top_p']} | Max tokens: {config['max_new_tokens']}\")\n",
    "        \n",
    "        response = insurance_model.generate_response(prompt, config)\n",
    "        \n",
    "        print(f\"Output: {response}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "print(\"‚úÖ Demo functions ready\")\n",
    "print(f\"Available functions:\")\n",
    "print(f\"  run_sample_demo(task_type, sample_index=0)\")\n",
    "print(f\"  run_custom_demo(task_type, user_input, context='')\")\n",
    "print(f\"  compare_generation_configs(task_type, user_input, context='')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "interactive-demos"
   },
   "source": [
    "## 6. Interactive Demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "demo1-claim-classification"
   },
   "outputs": [],
   "source": [
    "# Demo 1: Claim Classification\n",
    "if MODEL_LOADED:\n",
    "    print(\"üöó DEMO 1: Claim Classification\")\n",
    "    run_sample_demo('CLAIM_CLASSIFICATION', 0)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Model not loaded - skipping demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "demo2-policy-summarization"
   },
   "outputs": [],
   "source": [
    "# Demo 2: Policy Summarization\n",
    "if MODEL_LOADED:\n",
    "    print(\"üìÑ DEMO 2: Policy Summarization\")\n",
    "    run_sample_demo('POLICY_SUMMARIZATION', 0)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Model not loaded - skipping demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "demo3-faq-generation"
   },
   "outputs": [],
   "source": [
    "# Demo 3: FAQ Generation\n",
    "if MODEL_LOADED:\n",
    "    print(\"‚ùì DEMO 3: FAQ Generation\")\n",
    "    run_sample_demo('FAQ_GENERATION', 0)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Model not loaded - skipping demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "demo4-compliance-check"
   },
   "outputs": [],
   "source": [
    "# Demo 4: Compliance Check\n",
    "if MODEL_LOADED:\n",
    "    print(\"‚öñÔ∏è DEMO 4: Compliance Check\")\n",
    "    run_sample_demo('COMPLIANCE_CHECK', 0)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Model not loaded - skipping demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "demo5-contract-qa"
   },
   "outputs": [],
   "source": [
    "# Demo 5: Contract Q&A\n",
    "if MODEL_LOADED:\n",
    "    print(\"‚ùì DEMO 5: Contract Q&A\")\n",
    "    run_sample_demo('CONTRACT_QA')\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Model not loaded - skipping demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "custom-testing"
   },
   "source": [
    "## 7. Custom Input Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "custom-testing-code"
   },
   "outputs": [],
   "source": [
    "# Custom testing area\n",
    "if MODEL_LOADED:\n",
    "    print(\"üß™ CUSTOM TESTING AREA\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Use the functions below to test your own inputs:\")\n",
    "    print()\n",
    "    \n",
    "    # Example custom test\n",
    "    custom_claim = \"\"\"Patient visited emergency room after experiencing chest pain. \n",
    "    EKG and blood work performed. Diagnosis: anxiety attack, not cardiac event. \n",
    "    Patient discharged with follow-up instructions.\"\"\"\n",
    "    \n",
    "    print(\"Example custom claim classification:\")\n",
    "    run_custom_demo('CLAIM_CLASSIFICATION', custom_claim)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Try your own examples by calling:\")\n",
    "    print(\"run_custom_demo('TASK_TYPE', 'your input here')\")\n",
    "    print(\"\\nAvailable task types:\")\n",
    "    for task in TASK_TEMPLATES.keys():\n",
    "        print(f\"  - {task}\")\n",
    "        \nelse:\n",
    "    print(\"‚ö†Ô∏è Model not loaded - custom testing not available\")\n",
    "    print(\"\\nTo test custom inputs, ensure the model is loaded and use:\")\n",
    "    print(\"run_custom_demo('TASK_TYPE', 'your_input_here')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generation-comparison"
   },
   "source": [
    "## 8. Generation Configuration Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generation-comparison-code"
   },
   "outputs": [],
   "source": [
    "# Compare different generation configurations\n",
    "if MODEL_LOADED:\n",
    "    print(\"üî¨ GENERATION CONFIGURATION COMPARISON\")\n",
    "    \n",
    "    test_input = \"Explain what comprehensive auto insurance covers and what it doesn't cover.\"\n",
    "    \n",
    "    compare_generation_configs('POLICY_SUMMARIZATION', test_input)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Model not loaded - configuration comparison not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "interactive-widget"
   },
   "source": [
    "## 9. Interactive Widget Interface (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "interactive-widget-code"
   },
   "outputs": [],
   "source": [
    "# Interactive widget interface (if available)\n",
    "if WIDGETS_AVAILABLE and MODEL_LOADED:\n",
    "    \n",
    "    def create_interactive_demo():\n",
    "        \"\"\"Create an interactive widget-based demo\"\"\"\n",
    "        \n",
    "        # Task selector\n",
    "        task_selector = widgets.Dropdown(\n",
    "            options=list(TASK_TEMPLATES.keys()),\n",
    "            value=list(TASK_TEMPLATES.keys())[0],\n",
    "            description='Task:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Input text area\n",
    "        input_text = widgets.Textarea(\n",
    "            value='Enter your insurance-related text here...',\n",
    "            placeholder='Type your insurance document or question here',\n",
    "            description='Input:',\n",
    "            layout=widgets.Layout(width='100%', height='150px')\n",
    "        )\n",
    "        \n",
    "        # Context text area (for Q&A tasks)\n",
    "        context_text = widgets.Textarea(\n",
    "            value='',\n",
    "            placeholder='Optional: Add context for Q&A tasks',\n",
    "            description='Context:',\n",
    "            layout=widgets.Layout(width='100%', height='100px')\n",
    "        )\n",
    "        \n",
    "        # Generation config selector\n",
    "        config_selector = widgets.Dropdown(\n",
    "            options=list(GENERATION_CONFIGS.keys()),\n",
    "            value='factual',\n",
    "            description='Config:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Submit button\n",
    "        submit_button = widgets.Button(\n",
    "            description='Generate Response',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='200px')\n",
    "        )\n",
    "        \n",
    "        # Output area\n",
    "        output_area = widgets.Output()\n",
    "        \n",
    "        def on_submit_clicked(b):\n",
    "            \"\"\"Handle submit button click\"\"\"\n",
    "            \n",
    "            with output_area:\n",
    "                clear_output(wait=True)\n",
    "                \n",
    "                task_type = task_selector.value\n",
    "                user_input = input_text.value\n",
    "                context = context_text.value\n",
    "                \n",
    "                if not user_input or user_input.strip() == 'Enter your insurance-related text here...':\n",
    "                    print(\"‚ùå Please enter some input text\")\n",
    "                    return\n",
    "                \n",
    "                print(f\"üéØ Processing {task_type}...\")\n",
    "                print(\"=\" * 50)\n",
    "                \n",
    "                try:\n",
    "                    # Override default config if user selected different one\n",
    "                    prompt = insurance_model.format_insurance_prompt(task_type, user_input, context)\n",
    "                    generation_config = GENERATION_CONFIGS[config_selector.value]\n",
    "                    response = insurance_model.generate_response(prompt, generation_config)\n",
    "                    \n",
    "                    print(f\"‚úÖ Response:\")\n",
    "                    print(response)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error: {e}\")\n",
    "        \n",
    "        submit_button.on_click(on_submit_clicked)\n",
    "        \n",
    "        # Layout\n",
    "        demo_interface = widgets.VBox([\n",
    "            widgets.HTML(value=\"<h3>ü§ñ Interactive Insurance LLaMA Demo</h3>\"),\n",
    "            task_selector,\n",
    "            input_text,\n",
    "            context_text,\n",
    "            config_selector,\n",
    "            submit_button,\n",
    "            output_area\n",
    "        ])\n",
    "        \n",
    "        return demo_interface\n",
    "    \n",
    "    # Create and display the interface\n",
    "    demo_widget = create_interactive_demo()\n",
    "    display(demo_widget)\n",
    "    \nelse:\n",
    "    if not WIDGETS_AVAILABLE:\n",
    "        print(\"üì± Interactive widgets not available\")\n",
    "        print(\"Use the demo functions above for testing\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Model not loaded - interactive demo not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## 10. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "conclusion-code"
   },
   "outputs": [],
   "source": [
    "def display_summary():\n",
    "    \"\"\"Display summary and next steps\"\"\"\n",
    "    \n",
    "    print(\"üéâ LLaMA Insurance Model Demo Complete!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if MODEL_LOADED:\n",
    "        print(\"‚úÖ Model Status: Loaded and Ready\")\n",
    "        print(f\"üìä Available Tasks: {len(TASK_TEMPLATES)}\")\n",
    "        print(f\"‚öôÔ∏è Generation Configs: {len(GENERATION_CONFIGS)}\")\n",
    "        print(f\"üéØ Sample Scenarios: Available\")\n",
    "        \n",
    "        print(\"\\nüöÄ What you can do:\")\n",
    "        print(\"1. Test different insurance scenarios with run_sample_demo()\")\n",
    "        print(\"2. Try your own inputs with run_custom_demo()\")\n",
    "        print(\"3. Compare generation settings with compare_generation_configs()\")\n",
    "        print(\"4. Use the interactive widget interface (if available)\")\n",
    "        \n",
    "        print(\"\\nüìã Task Types:\")\n",
    "        for task, desc in TASK_TEMPLATES.items():\n",
    "            print(f\"  ‚Ä¢ {task}: {desc['instruction'][:50]}...\")\n",
    "        \n",
    "        print(\"\\nüí° Tips for Best Results:\")\n",
    "        print(\"‚Ä¢ Use 'precise' config for factual answers\")\n",
    "        print(\"‚Ä¢ Use 'creative' config for content generation\")\n",
    "        print(\"‚Ä¢ Use 'factual' config for summaries and explanations\")\n",
    "        print(\"‚Ä¢ Provide clear, specific inputs\")\n",
    "        print(\"‚Ä¢ Add context for Q&A tasks\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Model Status: Not Loaded\")\n",
    "        print(\"\\nüîß To use this demo:\")\n",
    "        print(\"1. Complete training in notebook 03_finetuning_lora.ipynb\")\n",
    "        print(\"2. Ensure model files are saved in outputs/final_model/lora_model/\")\n",
    "        print(\"3. Restart this notebook and load the model\")\n",
    "    \n",
    "    print(\"\\nüéØ Production Deployment:\")\n",
    "    print(\"‚Ä¢ Consider model optimization for faster inference\")\n",
    "    print(\"‚Ä¢ Implement proper input validation and safety checks\")\n",
    "    print(\"‚Ä¢ Add logging and monitoring for production use\")\n",
    "    print(\"‚Ä¢ Set up A/B testing for different configurations\")\n",
    "    \n",
    "    print(\"\\nüìö Project Complete!\")\n",
    "    print(\"You now have a fully functional insurance-specialized LLaMA model\")\n",
    "    print(\"=\" * 60)\n\n",
    "# Display final summary\n",
    "display_summary()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
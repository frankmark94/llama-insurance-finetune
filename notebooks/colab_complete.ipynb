{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Complete LLaMA Insurance Fine-tuning Pipeline for Google Colab\n",
    "\n",
    "This notebook combines the entire fine-tuning pipeline in one place for seamless execution in Google Colab:\n",
    "\n",
    "## Pipeline Overview\n",
    "1. **Data Preprocessing** - PII removal, cleaning, dataset creation\n",
    "2. **Tokenization** - LLaMA tokenizer setup and data formatting\n",
    "3. **Model Training** - LoRA fine-tuning with quantization\n",
    "4. **Evaluation** - Comprehensive metrics and analysis\n",
    "5. **Inference Demo** - Interactive testing of the trained model\n",
    "\n",
    "## Before Running:\n",
    "1. **Enable GPU**: Runtime → Change runtime type → GPU (T4 recommended)\n",
    "2. **Authenticate**: You'll need HuggingFace access for LLaMA models\n",
    "3. **Optional**: Set up W&B for training monitoring\n",
    "\n",
    "**⚠️ This notebook requires ~4-6 hours to complete on Colab T4 GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Setup and Dependencies\n",
    "\n",
    "Install required libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers accelerate datasets bitsandbytes peft wandb\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q rouge-score nltk scikit-learn matplotlib seaborn\n",
    "!pip install -q pandas numpy tqdm ipywidgets\n",
    "\n",
    "print(\"✅ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# ML libraries  \n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    GenerationConfig,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType, PeftModel\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Text processing\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# HuggingFace authentication\n",
    "from huggingface_hub import login\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()\n",
    "\n",
    "print(f\"✅ Libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ No GPU detected - training will be very slow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with HuggingFace (required for LLaMA access)\n",
    "print(\"🔐 HuggingFace Authentication Required\")\n",
    "print(\"Please get your access token from: https://huggingface.co/settings/tokens\")\n",
    "print(\"Make sure you have access to LLaMA models\")\n",
    "\n",
    "try:\n",
    "    login()\n",
    "    print(\"✅ HuggingFace authentication successful\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Authentication failed: {e}\")\n",
    "    print(\"Please run this cell again and enter your HF token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create project directory structure\n",
    "project_dirs = [\n",
    "    \"data/raw\",\n",
    "    \"data/processed\",\n",
    "    \"data/tokenized\", \n",
    "    \"data/annotations\",\n",
    "    \"outputs/checkpoints\",\n",
    "    \"outputs/final_model\",\n",
    "    \"outputs/logs\",\n",
    "    \"outputs/evaluation\",\n",
    "    \"config\"\n",
    "]\n",
    "\n",
    "for dir_path in project_dirs:\n",
    "    Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"✅ Project directory structure created\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Save memory management function\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU and system memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(f\"🧹 Memory cleared\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB allocated\")\n",
    "\n",
    "# Clear memory to start fresh\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Data Preprocessing\n",
    "\n",
    "Create sample insurance data and preprocess it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for data preprocessing\n",
    "RAW_DATA_DIR = Path(\"data/raw\")\n",
    "PROCESSED_DATA_DIR = Path(\"data/processed\")\n",
    "ANNOTATIONS_DIR = Path(\"data/annotations\")\n",
    "\n",
    "# Data split ratios\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "# Insurance task types\n",
    "TASK_TYPES = {\n",
    "    'CLAIM_CLASSIFICATION': 'Categorize insurance claims',\n",
    "    'POLICY_SUMMARIZATION': 'Summarize policy documents',\n",
    "    'FAQ_GENERATION': 'Generate FAQs from policies',\n",
    "    'COMPLIANCE_CHECK': 'Identify compliance requirements',\n",
    "    'CONTRACT_QA': 'Answer questions about contracts'\n",
    "}\n",
    "\n",
    "print(f\"📊 Data preprocessing configuration loaded\")\n",
    "print(f\"Task types: {list(TASK_TYPES.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PII Removal Class\n",
    "import re\n",
    "\n",
    "class PIIRemover:\n",
    "    \"\"\"Class to handle PII detection and removal from insurance documents\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Regex patterns for common PII\n",
    "        self.patterns = {\n",
    "            'ssn': r'\\b\\d{3}-\\d{2}-\\d{4}\\b|\\b\\d{9}\\b',\n",
    "            'phone': r'\\b(?:\\+?1[-.]?)?\\(?([0-9]{3})\\)?[-.]?([0-9]{3})[-.]?([0-9]{4})\\b',\n",
    "            'email': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
    "            'zip_code': r'\\b\\d{5}(?:-\\d{4})?\\b',\n",
    "            'credit_card': r'\\b(?:\\d{4}[-\\s]?){3}\\d{4}\\b',\n",
    "            'account_number': r'\\b(?:account|acct|policy)\\s*#?\\s*\\d{6,}\\b',\n",
    "            'date_of_birth': r'\\b(?:0?[1-9]|1[0-2])[/-](?:0?[1-9]|[12]\\d|3[01])[/-](?:19|20)\\d{2}\\b',\n",
    "            'address_number': r'\\b\\d{1,5}\\s+[A-Za-z\\s]+(?:Street|St|Avenue|Ave|Road|Rd|Drive|Dr|Lane|Ln|Boulevard|Blvd)\\b',\n",
    "        }\n",
    "    \n",
    "    def remove_pii(self, text: str) -> Tuple[str, Dict[str, int]]:\n",
    "        \"\"\"Remove PII from text and return cleaned text with removal stats\"\"\"\n",
    "        cleaned_text = text\n",
    "        removal_stats = {}\n",
    "        \n",
    "        # Replace PII with generic placeholders\n",
    "        replacements = {\n",
    "            'ssn': '[SSN]',\n",
    "            'phone': '[PHONE]',\n",
    "            'email': '[EMAIL]',\n",
    "            'zip_code': '[ZIP]',\n",
    "            'credit_card': '[CARD_NUMBER]',\n",
    "            'account_number': '[ACCOUNT_NUMBER]',\n",
    "            'date_of_birth': '[DATE_OF_BIRTH]',\n",
    "            'address_number': '[ADDRESS]',\n",
    "        }\n",
    "        \n",
    "        for pii_type, replacement in replacements.items():\n",
    "            pattern = self.patterns[pii_type]\n",
    "            matches = re.findall(pattern, cleaned_text, re.IGNORECASE)\n",
    "            removal_stats[pii_type] = len(matches)\n",
    "            cleaned_text = re.sub(pattern, replacement, cleaned_text, flags=re.IGNORECASE)\n",
    "        \n",
    "        return cleaned_text, removal_stats\n",
    "\n",
    "# Initialize PII remover\n",
    "pii_remover = PIIRemover()\n",
    "\n",
    "# Test PII removal\n",
    "test_text = \"John Smith's SSN is 123-45-6789 and phone is (555) 123-4567.\"\n",
    "cleaned, stats = pii_remover.remove_pii(test_text)\n",
    "print(f\"PII Removal Test:\")\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Cleaned: {cleaned}\")\n",
    "print(f\"✅ PII remover ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample insurance data for training\n",
    "def create_sample_insurance_data():\n",
    "    \"\"\"Create comprehensive sample insurance documents for training\"\"\"\n",
    "    \n",
    "    sample_documents = [\n",
    "        {\n",
    "            'id': 'health_policy_001',\n",
    "            'source': 'sample_data',\n",
    "            'content': '''Health Insurance Policy - Premium Coverage\n",
    "            \n",
    "Coverage: This comprehensive health insurance policy provides coverage for medical expenses including hospital stays, doctor visits, prescription medications, and emergency care. The annual coverage limit is $1,000,000 per insured individual.\n",
    "            \n",
    "Deductible: Annual deductible of $1,500 per individual, $3,000 per family. After meeting the deductible, the plan covers 80% of eligible medical expenses.\n",
    "            \n",
    "Exclusions: Pre-existing conditions diagnosed within 12 months prior to policy effective date, cosmetic procedures, experimental treatments, and services not deemed medically necessary are excluded from coverage.\n",
    "            \n",
    "Premium: Monthly premium of $450 for individual coverage, $1,200 for family coverage. Premiums are due on the first of each month.''',\n",
    "            'type': 'health_policy',\n",
    "            'task_type': 'POLICY_SUMMARIZATION'\n",
    "        },\n",
    "        {\n",
    "            'id': 'auto_claim_001', \n",
    "            'source': 'sample_data',\n",
    "            'content': '''Auto Insurance Claim - Vehicle Collision\n",
    "            \n",
    "Claim Details: Vehicle collision occurred on Highway 101 involving two vehicles. Insured vehicle sustained front-end damage requiring repair. No injuries reported. Police report filed, case number 2024-001234.\n",
    "            \n",
    "Coverage Applied: Collision coverage with $500 deductible. Estimated repair cost $3,200. Coverage approved for $2,700 after deductible.\n",
    "            \n",
    "Settlement: Claim approved and processed. Payment issued to approved repair facility. Rental car coverage provided for 5 days during repair period.''',\n",
    "            'type': 'auto_claim',\n",
    "            'task_type': 'CLAIM_CLASSIFICATION'\n",
    "        },\n",
    "        {\n",
    "            'id': 'life_policy_001',\n",
    "            'source': 'sample_data', \n",
    "            'content': '''Life Insurance Policy - Term Life Coverage\n",
    "            \n",
    "Coverage: $500,000 term life insurance policy with 20-year level premium guarantee. Coverage includes accidental death and dismemberment benefit.\n",
    "            \n",
    "Beneficiaries: Primary beneficiary receives full death benefit. Secondary beneficiaries receive equal distribution if primary is deceased.\n",
    "            \n",
    "Premium: Monthly premium of $65 guaranteed for 20 years. Policy renewable at end of term with updated rates based on age and health.''',\n",
    "            'type': 'life_policy',\n",
    "            'task_type': 'POLICY_SUMMARIZATION'\n",
    "        },\n",
    "        {\n",
    "            'id': 'home_claim_001',\n",
    "            'source': 'sample_data',\n",
    "            'content': '''Homeowners Insurance Claim - Water Damage\n",
    "            \n",
    "Claim Details: Water damage to kitchen and dining room caused by burst pipe. Damage includes flooring, cabinets, and drywall. Professional water extraction and drying completed.\n",
    "            \n",
    "Coverage Applied: Dwelling coverage and personal property coverage. $1,000 deductible applied. Total claim amount $8,500 minus deductible equals $7,500 payout.\n",
    "            \n",
    "Settlement: Claim approved. Contractor payment authorized for repairs. Temporary living expenses covered during repair period.''',\n",
    "            'type': 'home_claim',\n",
    "            'task_type': 'CLAIM_CLASSIFICATION'\n",
    "        },\n",
    "        {\n",
    "            'id': 'compliance_doc_001',\n",
    "            'source': 'sample_data',\n",
    "            'content': '''Insurance Regulatory Compliance Requirements\n",
    "            \n",
    "HIPAA Compliance: All health insurance operations must comply with Health Insurance Portability and Accountability Act requirements for protecting patient health information privacy and security.\n",
    "            \n",
    "State Regulations: Insurance products must be filed with and approved by state insurance commissioners before sale. Rate changes require regulatory approval.\n",
    "            \n",
    "Consumer Protection: All marketing materials must be clear, truthful, and not misleading. Claims processing must be fair and timely according to state prompt payment laws.''',\n",
    "            'type': 'compliance',\n",
    "            'task_type': 'COMPLIANCE_CHECK'\n",
    "        },\n",
    "        {\n",
    "            'id': 'disability_policy_001',\n",
    "            'source': 'sample_data',\n",
    "            'content': '''Disability Insurance Policy - Short Term Coverage\n",
    "            \n",
    "Coverage: Provides 60% of monthly income up to $3,000 per month for temporary disability lasting 14 days to 2 years. Covers illness and injury preventing work.\n",
    "            \n",
    "Waiting Period: 14-day elimination period before benefits begin. Pre-existing conditions covered after 12 months of continuous coverage.\n",
    "            \n",
    "Benefits: Monthly benefit payments made directly to insured. Coverage continues until recovery, end of benefit period, or return to work.''',\n",
    "            'type': 'disability_policy',\n",
    "            'task_type': 'POLICY_SUMMARIZATION' \n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Add more examples for better training\n",
    "    expanded_documents = []\n",
    "    \n",
    "    # Create variations of each document type\n",
    "    for doc in sample_documents:\n",
    "        expanded_documents.append(doc)\n",
    "        \n",
    "        # Create a variation with different values\n",
    "        if 'policy' in doc['type']:\n",
    "            variation = doc.copy()\n",
    "            variation['id'] = doc['id'].replace('001', '002')\n",
    "            # Simple content variation\n",
    "            variation['content'] = doc['content'].replace('$500,000', '$750,000').replace('$1,000,000', '$1,500,000')\n",
    "            expanded_documents.append(variation)\n",
    "        \n",
    "        elif 'claim' in doc['type']:\n",
    "            variation = doc.copy()\n",
    "            variation['id'] = doc['id'].replace('001', '002')\n",
    "            variation['content'] = doc['content'].replace('approved', 'pending review')\n",
    "            expanded_documents.append(variation)\n",
    "    \n",
    "    # Save sample data to files\n",
    "    sample_file = RAW_DATA_DIR / 'sample_insurance_docs.json'\n",
    "    with open(sample_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(expanded_documents, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"✅ Created {len(expanded_documents)} sample documents\")\n",
    "    return expanded_documents\n",
    "\n",
    "# Create sample data\n",
    "sample_docs = create_sample_insurance_data()\n",
    "print(f\"Sample documents by task:\")\n",
    "for task_type in TASK_TYPES.keys():\n",
    "    count = len([doc for doc in sample_docs if doc['task_type'] == task_type])\n",
    "    print(f\"  {task_type}: {count} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process documents and create training datasets\n",
    "def process_and_create_datasets(documents):\n",
    "    \"\"\"Process documents and create task-specific datasets\"\"\"\n",
    "    \n",
    "    processed_examples = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Clean text and remove PII\n",
    "        content = doc['content']\n",
    "        cleaned_content, pii_stats = pii_remover.remove_pii(content)\n",
    "        \n",
    "        task_type = doc['task_type']\n",
    "        \n",
    "        if task_type == 'POLICY_SUMMARIZATION':\n",
    "            # Create summarization example\n",
    "            summary = f\"This {doc['type']} document covers key insurance terms including coverage details, deductibles, premiums, and important policy conditions.\"\n",
    "            \n",
    "            example = {\n",
    "                'instruction': 'Summarize the following insurance policy document.',\n",
    "                'input': cleaned_content,\n",
    "                'output': summary,\n",
    "                'task_type': task_type,\n",
    "                'doc_id': doc['id']\n",
    "            }\n",
    "            processed_examples.append(example)\n",
    "        \n",
    "        elif task_type == 'CLAIM_CLASSIFICATION':\n",
    "            # Create classification example\n",
    "            classification = f\"This is a {doc['type']} requiring {doc['type'].split('_')[0]} coverage review.\"\n",
    "            \n",
    "            example = {\n",
    "                'instruction': 'Classify this insurance claim into the appropriate category.',\n",
    "                'input': cleaned_content,\n",
    "                'output': classification,\n",
    "                'task_type': task_type,\n",
    "                'doc_id': doc['id']\n",
    "            }\n",
    "            processed_examples.append(example)\n",
    "        \n",
    "        elif task_type == 'COMPLIANCE_CHECK':\n",
    "            # Create compliance checking example\n",
    "            compliance_info = 'Key compliance requirements include HIPAA privacy protections, state regulatory approvals, consumer protection standards, and prompt payment regulations.'\n",
    "            \n",
    "            example = {\n",
    "                'instruction': 'Identify compliance requirements in this insurance document.',\n",
    "                'input': cleaned_content,\n",
    "                'output': compliance_info,\n",
    "                'task_type': task_type,\n",
    "                'doc_id': doc['id']\n",
    "            }\n",
    "            processed_examples.append(example)\n",
    "        \n",
    "        # Add FAQ generation examples for policies\n",
    "        if 'policy' in doc['type']:\n",
    "            faq_example = {\n",
    "                'instruction': 'Generate frequently asked questions for this insurance policy.',\n",
    "                'input': cleaned_content,\n",
    "                'output': f\"Q: What is the coverage limit? A: The policy provides comprehensive coverage as outlined. Q: What is the deductible? A: Deductible amounts vary by coverage type. Q: How do I file a claim? A: Contact your insurance provider to begin the claims process.\",\n",
    "                'task_type': 'FAQ_GENERATION',\n",
    "                'doc_id': doc['id'] + '_faq'\n",
    "            }\n",
    "            processed_examples.append(faq_example)\n",
    "        \n",
    "        # Add Q&A examples\n",
    "        qa_example = {\n",
    "            'instruction': 'Answer the following question about this insurance document.',\n",
    "            'input': f\"Document: {cleaned_content}\\n\\nQuestion: What are the key benefits covered?\",\n",
    "            'output': \"The key benefits include coverage for specified risks, with defined limits and deductibles as outlined in the policy terms.\",\n",
    "            'task_type': 'CONTRACT_QA',\n",
    "            'doc_id': doc['id'] + '_qa'\n",
    "        }\n",
    "        processed_examples.append(qa_example)\n",
    "    \n",
    "    return processed_examples\n",
    "\n",
    "def create_train_test_splits(examples):\n",
    "    \"\"\"Create train/validation/test splits\"\"\"\n",
    "    \n",
    "    if len(examples) < 10:\n",
    "        # For small datasets, use simple splits\n",
    "        train_size = int(len(examples) * 0.7)\n",
    "        val_size = int(len(examples) * 0.15)\n",
    "        \n",
    "        train_examples = examples[:train_size]\n",
    "        val_examples = examples[train_size:train_size + val_size]\n",
    "        test_examples = examples[train_size + val_size:]\n",
    "        \n",
    "        # Ensure we have at least one example in each split\n",
    "        if not val_examples:\n",
    "            val_examples = train_examples[:1]\n",
    "        if not test_examples:\n",
    "            test_examples = train_examples[:1]\n",
    "    else:\n",
    "        # Use sklearn for larger datasets\n",
    "        train_examples, temp_examples = train_test_split(\n",
    "            examples, test_size=0.3, random_state=42\n",
    "        )\n",
    "        val_examples, test_examples = train_test_split(\n",
    "            temp_examples, test_size=0.5, random_state=42\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        'train': train_examples,\n",
    "        'validation': val_examples,\n",
    "        'test': test_examples\n",
    "    }\n",
    "\n",
    "# Process documents and create datasets\n",
    "print(\"🔄 Processing documents and creating datasets...\")\n",
    "processed_examples = process_and_create_datasets(sample_docs)\n",
    "\n",
    "print(f\"\\nCreated {len(processed_examples)} training examples:\")\n",
    "task_counts = {}\n",
    "for example in processed_examples:\n",
    "    task_type = example['task_type']\n",
    "    task_counts[task_type] = task_counts.get(task_type, 0) + 1\n",
    "\n",
    "for task_type, count in task_counts.items():\n",
    "    print(f\"  {task_type}: {count} examples\")\n",
    "\n",
    "# Create splits\n",
    "data_splits = create_train_test_splits(processed_examples)\n",
    "\n",
    "print(f\"\\nData splits created:\")\n",
    "for split_name, examples in data_splits.items():\n",
    "    print(f\"  {split_name}: {len(examples)} examples\")\n",
    "\n",
    "# Save datasets\n",
    "combined_dir = PROCESSED_DATA_DIR / 'combined'\n",
    "combined_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for split_name, examples in data_splits.items():\n",
    "    json_file = combined_dir / f\"{split_name}.json\"\n",
    "    with open(json_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(examples, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n✅ Data preprocessing complete!\")\n",
    "print(f\"Datasets saved to: {combined_dir}\")\n",
    "\n",
    "# Clear memory\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Tokenization\n",
    "\n",
    "Set up the LLaMA tokenizer and format data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization configuration\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "TOKENIZED_DATA_DIR = Path(\"data/tokenized\")\n",
    "TOKENIZED_DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Tokenization parameters\n",
    "MAX_LENGTH = 2048\n",
    "PADDING_SIDE = \"right\"\n",
    "TRUNCATION = True\n",
    "ADD_EOS_TOKEN = True\n",
    "\n",
    "# Instruction formatting template\n",
    "INSTRUCTION_TEMPLATE = {\n",
    "    'system': \"You are a helpful AI assistant specialized in insurance and financial services. Provide accurate, helpful, and compliant information.\",\n",
    "    'user_prefix': \"[INST]\",\n",
    "    'user_suffix': \"[/INST]\",\n",
    "    'assistant_prefix': \"\",\n",
    "    'assistant_suffix': \"</s>\"\n",
    "}\n",
    "\n",
    "print(f\"🔤 Tokenization configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Max length: {MAX_LENGTH}\")\n",
    "print(f\"  Output: {TOKENIZED_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup tokenizer\n",
    "def setup_tokenizer(model_name: str) -> AutoTokenizer:\n",
    "    \"\"\"Load and configure the LLaMA tokenizer\"\"\"\n",
    "    print(f\"Loading tokenizer for {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            use_fast=True,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=PADDING_SIDE\n",
    "        )\n",
    "        \n",
    "        # Set special tokens\n",
    "        if tokenizer.pad_token is None:\n",
    "            if tokenizer.eos_token is not None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            else:\n",
    "                tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        \n",
    "        print(f\"✅ Tokenizer loaded successfully\")\n",
    "        print(f\"  Vocab size: {len(tokenizer)}\")\n",
    "        print(f\"  Pad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "        print(f\"  EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "        \n",
    "        return tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading tokenizer: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = setup_tokenizer(MODEL_NAME)\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"This is a test of the LLaMA tokenizer for insurance documents.\"\n",
    "test_tokens = tokenizer.encode(test_text)\n",
    "decoded_text = tokenizer.decode(test_tokens)\n",
    "\n",
    "print(f\"\\nTokenization test:\")\n",
    "print(f\"  Original: {test_text}\")\n",
    "print(f\"  Tokens: {len(test_tokens)} tokens\")\n",
    "print(f\"  Decoded: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data for instruction tuning\n",
    "def format_instruction(example: Dict[str, str]) -> str:\n",
    "    \"\"\"Format example into instruction-following format for LLaMA\"\"\"\n",
    "    \n",
    "    instruction = example.get('instruction', '')\n",
    "    user_input = example.get('input', '')\n",
    "    assistant_output = example.get('output', '')\n",
    "    \n",
    "    # Format in LLaMA chat format\n",
    "    formatted_text = f\"{INSTRUCTION_TEMPLATE['user_prefix']} {instruction}\\n\\n{user_input} {INSTRUCTION_TEMPLATE['user_suffix']} {assistant_output}{INSTRUCTION_TEMPLATE['assistant_suffix']}\"\n",
    "    \n",
    "    return formatted_text\n",
    "\n",
    "def tokenize_function(examples: Dict[str, List[str]]) -> Dict[str, List[List[int]]]:\n",
    "    \"\"\"Tokenize a batch of examples\"\"\"\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=TRUNCATION,\n",
    "        padding=False,  # Pad dynamically during training\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=None,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # For causal LM, labels are same as input_ids\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "    \n",
    "    # Calculate lengths for filtering\n",
    "    tokenized['length'] = [len(ids) for ids in tokenized['input_ids']]\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Load and tokenize datasets\n",
    "print(\"🔄 Loading and tokenizing datasets...\")\n",
    "\n",
    "# Load processed data\n",
    "combined_data = {}\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    json_file = combined_dir / f\"{split}.json\"\n",
    "    if json_file.exists():\n",
    "        with open(json_file, 'r') as f:\n",
    "            combined_data[split] = json.load(f)\n",
    "\n",
    "# Format and tokenize each split\n",
    "tokenized_datasets = {}\n",
    "\n",
    "for split_name, examples in combined_data.items():\n",
    "    print(f\"\\nTokenizing {split_name} ({len(examples)} examples)...\")\n",
    "    \n",
    "    # Format examples for instruction tuning\n",
    "    formatted_examples = []\n",
    "    for example in examples:\n",
    "        formatted_text = format_instruction(example)\n",
    "        formatted_examples.append({\n",
    "            'text': formatted_text,\n",
    "            'task_type': example.get('task_type', 'POLICY_SUMMARIZATION'),\n",
    "            'original_id': example.get('doc_id', 'unknown')\n",
    "        })\n",
    "    \n",
    "    # Create dataset and tokenize\n",
    "    dataset = Dataset.from_list(formatted_examples)\n",
    "    \n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "        desc=f\"Tokenizing {split_name}\"\n",
    "    )\n",
    "    \n",
    "    # Filter out examples that are too long or too short\n",
    "    def filter_length(example):\n",
    "        length = example['length']\n",
    "        return 10 <= length <= MAX_LENGTH\n",
    "    \n",
    "    filtered_dataset = tokenized_dataset.filter(filter_length)\n",
    "    \n",
    "    print(f\"  After filtering: {len(filtered_dataset)} examples\")\n",
    "    \n",
    "    if len(filtered_dataset) > 0:\n",
    "        lengths = [ex['length'] for ex in filtered_dataset]\n",
    "        print(f\"  Length stats - Min: {min(lengths)}, Max: {max(lengths)}, Avg: {np.mean(lengths):.1f}\")\n",
    "    \n",
    "    tokenized_datasets[split_name] = filtered_dataset\n",
    "\n",
    "# Save tokenized datasets\n",
    "combined_tokenized_dir = TOKENIZED_DATA_DIR / \"combined\"\n",
    "combined_tokenized_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for split_name, dataset in tokenized_datasets.items():\n",
    "    if len(dataset) > 0:\n",
    "        split_dir = combined_tokenized_dir / split_name\n",
    "        dataset.save_to_disk(split_dir)\n",
    "        print(f\"✅ Saved {split_name} dataset: {len(dataset)} examples\")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer_dir = TOKENIZED_DATA_DIR / \"tokenizer\"\n",
    "tokenizer.save_pretrained(tokenizer_dir)\n",
    "print(f\"✅ Tokenizer saved to: {tokenizer_dir}\")\n",
    "\n",
    "# Save tokenization metadata\n",
    "metadata = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'vocab_size': len(tokenizer),\n",
    "    'datasets': {split: len(dataset) for split, dataset in tokenized_datasets.items()}\n",
    "}\n",
    "\n",
    "metadata_file = TOKENIZED_DATA_DIR / \"tokenization_metadata.json\"\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\n✅ Tokenization complete!\")\n",
    "print(f\"Total tokenized examples: {sum(len(dataset) for dataset in tokenized_datasets.values())}\")\n",
    "\n",
    "# Clear memory\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: LoRA Fine-tuning\n",
    "\n",
    "Train the LLaMA model using LoRA for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "OUTPUT_DIR = Path(\"outputs\")\n",
    "CHECKPOINT_DIR = OUTPUT_DIR / \"checkpoints\"\n",
    "FINAL_MODEL_DIR = OUTPUT_DIR / \"final_model\"\n",
    "LOGS_DIR = OUTPUT_DIR / \"logs\"\n",
    "\n",
    "# Training parameters optimized for Colab\n",
    "TRAINING_CONFIG = {\n",
    "    \"output_dir\": str(CHECKPOINT_DIR),\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"per_device_eval_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"fp16\": True,\n",
    "    \"max_grad_norm\": 0.3,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"save_steps\": 500,\n",
    "    \"eval_steps\": 500,\n",
    "    \"logging_steps\": 50,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"eval_loss\",\n",
    "    \"greater_is_better\": False,\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"report_to\": []\n",
    "}\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_CONFIG = {\n",
    "    \"r\": 8,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "    \"target_modules\": [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Quantization configuration\n",
    "QUANTIZATION_CONFIG = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    ")\n",
    "\n",
    "print(f\"🚀 Training configuration:\")\n",
    "print(f\"  Epochs: {TRAINING_CONFIG['num_train_epochs']}\")\n",
    "print(f\"  Batch size: {TRAINING_CONFIG['per_device_train_batch_size']}\")\n",
    "print(f\"  Learning rate: {TRAINING_CONFIG['learning_rate']}\")\n",
    "print(f\"  LoRA rank: {LORA_CONFIG['r']}\")\n",
    "print(f\"  Quantization: 4-bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenized datasets for training\n",
    "def load_tokenized_datasets_for_training() -> tuple[DatasetDict, AutoTokenizer]:\n",
    "    \"\"\"Load tokenized datasets and tokenizer for training\"\"\"\n",
    "    \n",
    "    print(f\"Loading tokenized datasets from {TOKENIZED_DATA_DIR}...\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer_dir = TOKENIZED_DATA_DIR / \"tokenizer\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
    "    print(f\"✅ Tokenizer loaded\")\n",
    "    \n",
    "    # Load datasets\n",
    "    dataset_dict = DatasetDict()\n",
    "    combined_dir = TOKENIZED_DATA_DIR / \"combined\"\n",
    "    \n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        split_dir = combined_dir / split\n",
    "        if split_dir.exists():\n",
    "            from datasets import load_from_disk\n",
    "            dataset = load_from_disk(split_dir)\n",
    "            dataset_dict[split] = dataset\n",
    "            print(f\"✅ {split}: {len(dataset)} examples\")\n",
    "    \n",
    "    return dataset_dict, tokenizer\n",
    "\n",
    "# Load datasets for training\n",
    "train_datasets, train_tokenizer = load_tokenized_datasets_for_training()\n",
    "\n",
    "if not train_datasets:\n",
    "    print(\"❌ No training datasets found\")\n",
    "else:\n",
    "    total_examples = sum(len(dataset) for dataset in train_datasets.values())\n",
    "    print(f\"\\n📊 Training data loaded: {total_examples} total examples\")\n",
    "    \n",
    "    # Show sample\n",
    "    if 'train' in train_datasets and len(train_datasets['train']) > 0:\n",
    "        sample = train_datasets['train'][0]\n",
    "        print(f\"Sample input length: {len(sample['input_ids'])} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare model for training\n",
    "def load_base_model(model_name: str, tokenizer: AutoTokenizer) -> AutoModelForCausalLM:\n",
    "    \"\"\"Load the base LLaMA model with quantization\"\"\"\n",
    "    \n",
    "    print(f\"Loading base model {model_name}...\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=QUANTIZATION_CONFIG,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        use_cache=False  # Disable for training\n",
    "    )\n",
    "    \n",
    "    # Resize embeddings if needed\n",
    "    if len(tokenizer) > model.config.vocab_size:\n",
    "        print(f\"Resizing embeddings: {model.config.vocab_size} -> {len(tokenizer)}\")\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    print(f\"✅ Model loaded successfully\")\n",
    "    print(f\"  Parameters: ~{sum(p.numel() for p in model.parameters()) / 1e6:.0f}M\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def setup_lora_model(model: AutoModelForCausalLM, lora_config: dict) -> PeftModel:\n",
    "    \"\"\"Set up LoRA configuration and wrap the model\"\"\"\n",
    "    \n",
    "    print(f\"Setting up LoRA...\")\n",
    "    \n",
    "    # Prepare model for k-bit training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    # Create LoRA configuration\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=lora_config['r'],\n",
    "        lora_alpha=lora_config['lora_alpha'],\n",
    "        lora_dropout=lora_config['lora_dropout'],\n",
    "        bias=lora_config['bias'],\n",
    "        target_modules=lora_config['target_modules'],\n",
    "        inference_mode=False\n",
    "    )\n",
    "    \n",
    "    # Wrap with LoRA\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"✅ LoRA setup complete\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%})\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load and setup model if datasets are available\n",
    "if train_datasets and train_tokenizer:\n",
    "    print(\"🔄 Loading and setting up model...\")\n",
    "    \n",
    "    # Clear memory first\n",
    "    clear_memory()\n",
    "    \n",
    "    # Load base model\n",
    "    base_model = load_base_model(MODEL_NAME, train_tokenizer)\n",
    "    \n",
    "    # Setup LoRA\n",
    "    model = setup_lora_model(base_model, LORA_CONFIG)\n",
    "    \n",
    "    # Check GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        memory_used = torch.cuda.memory_allocated(0) / 1e9\n",
    "        print(f\"GPU Memory Usage: {memory_used:.2f} GB\")\n",
    "else:\n",
    "    print(\"❌ Cannot load model - no training datasets\")\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training components\n",
    "if model is not None and train_datasets:\n",
    "    print(\"⚙️ Setting up training components...\")\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(**TRAINING_CONFIG)\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=train_tokenizer,\n",
    "        mlm=False,  # Causal LM\n",
    "        pad_to_multiple_of=8,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_datasets['train'],\n",
    "        eval_dataset=train_datasets.get('validation'),\n",
    "        tokenizer=train_tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Training components ready\")\n",
    "    print(f\"  Training samples: {len(train_datasets['train'])}\")\n",
    "    if 'validation' in train_datasets:\n",
    "        print(f\"  Validation samples: {len(train_datasets['validation'])}\")\n",
    "    \n",
    "    # Calculate estimated training time\n",
    "    train_dataloader = trainer.get_train_dataloader()\n",
    "    num_batches = len(train_dataloader)\n",
    "    total_steps = num_batches * training_args.num_train_epochs\n",
    "    estimated_hours = (total_steps * 2.0) / 3600  # ~2 seconds per step\n",
    "    \n",
    "    print(f\"\\n⏱️ Training estimation:\")\n",
    "    print(f\"  Total steps: {total_steps}\")\n",
    "    print(f\"  Estimated time: {estimated_hours:.1f} hours\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Cannot setup training - missing model or data\")\n",
    "    trainer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "if trainer is not None:\n",
    "    print(f\"🚀 Starting training at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"=\" * 60)\n",
    "    \n",
    "    # Clear memory before training\n",
    "    clear_memory()\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Start training\n",
    "        training_result = trainer.train()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        training_duration = (end_time - start_time) / 3600  # hours\n",
    "        \n",
    "        print(f\"\\n✅ Training completed successfully!\")\n",
    "        print(f\"Training time: {training_duration:.2f} hours\")\n",
    "        print(f\"Final training loss: {training_result.training_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint info\n",
    "        training_success = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Training failed: {e}\")\n",
    "        print(f\"This might be due to insufficient GPU memory or other issues\")\n",
    "        \n",
    "        # Try emergency save\n",
    "        try:\n",
    "            emergency_path = CHECKPOINT_DIR / \"emergency_checkpoint\"\n",
    "            trainer.save_model(emergency_path)\n",
    "            print(f\"Emergency checkpoint saved to: {emergency_path}\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        training_success = False\n",
    "        training_result = None\n",
    "\n",
    "else:\n",
    "    print(\"❌ Cannot start training - trainer not ready\")\n",
    "    training_success = False\n",
    "    training_result = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "if training_success and trainer is not None:\n",
    "    print(f\"💾 Saving final model...\")\n",
    "    \n",
    "    # Save LoRA model\n",
    "    lora_model_dir = FINAL_MODEL_DIR / \"lora_model\"\n",
    "    lora_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    model.save_pretrained(lora_model_dir)\n",
    "    train_tokenizer.save_pretrained(lora_model_dir)\n",
    "    \n",
    "    print(f\"✅ LoRA model saved to: {lora_model_dir}\")\n",
    "    \n",
    "    # Save training info\n",
    "    model_info = {\n",
    "        'base_model': MODEL_NAME,\n",
    "        'model_type': 'LLaMA-2-7B with LoRA',\n",
    "        'task': 'Insurance Domain Fine-tuning',\n",
    "        'lora_config': LORA_CONFIG,\n",
    "        'training_config': TRAINING_CONFIG,\n",
    "        'training_completed': datetime.now().isoformat(),\n",
    "        'final_loss': training_result.training_loss if training_result else None,\n",
    "        'total_steps': training_result.global_step if training_result else None\n",
    "    }\n",
    "    \n",
    "    info_file = FINAL_MODEL_DIR / \"model_info.json\"\n",
    "    with open(info_file, 'w') as f:\n",
    "        json.dump(model_info, f, indent=2)\n",
    "    \n",
    "    print(f\"✅ Model info saved to: {info_file}\")\n",
    "    print(f\"\\n🎉 Training pipeline completed successfully!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Cannot save model - training was not successful\")\n",
    "\n",
    "# Clear memory after training\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Model Evaluation\n",
    "\n",
    "Evaluate the fine-tuned model's performance on insurance tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation configuration\n",
    "EVALUATION_RESULTS_DIR = Path(\"outputs/evaluation\")\n",
    "EVALUATION_RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "LORA_MODEL_PATH = Path(\"outputs/final_model/lora_model\")\n",
    "\n",
    "# Generation configuration for evaluation\n",
    "GENERATION_CONFIG = {\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"do_sample\": True,\n",
    "    \"repetition_penalty\": 1.1\n",
    "}\n",
    "\n",
    "print(f\"📊 Evaluation setup:\")\n",
    "print(f\"  Model path: {LORA_MODEL_PATH}\")\n",
    "print(f\"  Results dir: {EVALUATION_RESULTS_DIR}\")\n",
    "print(f\"  Max tokens: {GENERATION_CONFIG['max_new_tokens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned model for evaluation\n",
    "def load_finetuned_model_for_eval() -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\"Load the fine-tuned LoRA model for evaluation\"\"\"\n",
    "    \n",
    "    if not LORA_MODEL_PATH.exists():\n",
    "        print(f\"❌ LoRA model not found at {LORA_MODEL_PATH}\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading fine-tuned model for evaluation...\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(LORA_MODEL_PATH)\n",
    "        \n",
    "        # Load base model\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Load LoRA model\n",
    "        model = PeftModel.from_pretrained(base_model, LORA_MODEL_PATH)\n",
    "        \n",
    "        # Set pad token\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        GENERATION_CONFIG['pad_token_id'] = tokenizer.pad_token_id\n",
    "        \n",
    "        print(f\"✅ Fine-tuned model loaded for evaluation\")\n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading fine-tuned model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Load model if training was successful\n",
    "if training_success:\n",
    "    eval_model, eval_tokenizer = load_finetuned_model_for_eval()\n",
    "else:\n",
    "    print(\"⚠️ Skipping evaluation - no trained model available\")\n",
    "    eval_model, eval_tokenizer = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data for evaluation\n",
    "def load_test_data_for_eval() -> List[Dict]:\n",
    "    \"\"\"Load test data for evaluation\"\"\"\n",
    "    \n",
    "    test_file = Path(\"data/processed/combined/test.json\")\n",
    "    \n",
    "    if test_file.exists():\n",
    "        with open(test_file, 'r') as f:\n",
    "            test_data = json.load(f)\n",
    "        print(f\"✅ Test data loaded: {len(test_data)} examples\")\n",
    "        return test_data\n",
    "    else:\n",
    "        print(f\"❌ Test data not found at {test_file}\")\n",
    "        return []\n",
    "\n",
    "# Evaluation functions\n",
    "def generate_response(model, tokenizer, prompt: str, generation_config: dict) -> str:\n",
    "    \"\"\"Generate response from model\"\"\"\n",
    "    \n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                **generation_config,\n",
    "                use_cache=True\n",
    "            )\n",
    "        \n",
    "        # Decode response (remove input)\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        generated_tokens = outputs[0][input_length:]\n",
    "        response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        return response.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def calculate_simple_metrics(predictions: List[str], references: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"Calculate simple evaluation metrics\"\"\"\n",
    "    \n",
    "    if not predictions or not references:\n",
    "        return {'accuracy': 0.0, 'avg_length': 0.0}\n",
    "    \n",
    "    # Simple exact match accuracy\n",
    "    exact_matches = sum(1 for p, r in zip(predictions, references) \n",
    "                       if p.lower().strip() == r.lower().strip())\n",
    "    accuracy = exact_matches / len(predictions)\n",
    "    \n",
    "    # Average response length\n",
    "    avg_length = np.mean([len(p.split()) for p in predictions])\n",
    "    \n",
    "    # ROUGE-like overlap (simple version)\n",
    "    overlap_scores = []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_words = set(pred.lower().split())\n",
    "        ref_words = set(ref.lower().split())\n",
    "        if ref_words:\n",
    "            overlap = len(pred_words & ref_words) / len(ref_words)\n",
    "            overlap_scores.append(overlap)\n",
    "        else:\n",
    "            overlap_scores.append(0.0)\n",
    "    \n",
    "    avg_overlap = np.mean(overlap_scores) if overlap_scores else 0.0\n",
    "    \n",
    "    return {\n",
    "        'exact_match_accuracy': accuracy,\n",
    "        'average_response_length': avg_length,\n",
    "        'word_overlap_score': avg_overlap\n",
    "    }\n",
    "\n",
    "# Load test data\n",
    "if eval_model is not None:\n",
    "    test_data = load_test_data_for_eval()\n",
    "else:\n",
    "    test_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "if eval_model is not None and eval_tokenizer is not None and test_data:\n",
    "    print(f\"🔍 Running evaluation on {len(test_data)} test examples...\")\n",
    "    \n",
    "    # Group by task type\n",
    "    task_examples = {}\n",
    "    for example in test_data:\n",
    "        task_type = example.get('task_type', 'POLICY_SUMMARIZATION')\n",
    "        if task_type not in task_examples:\n",
    "            task_examples[task_type] = []\n",
    "        task_examples[task_type].append(example)\n",
    "    \n",
    "    print(f\"\\nTask distribution:\")\n",
    "    for task_type, examples in task_examples.items():\n",
    "        print(f\"  {task_type}: {len(examples)} examples\")\n",
    "    \n",
    "    # Run evaluation for each task\n",
    "    evaluation_results = {}\n",
    "    \n",
    "    for task_type, examples in task_examples.items():\n",
    "        print(f\"\\nEvaluating {task_type}...\")\n",
    "        \n",
    "        predictions = []\n",
    "        references = []\n",
    "        sample_outputs = []\n",
    "        \n",
    "        # Limit to first 5 examples for demo purposes (faster)\n",
    "        eval_examples = examples[:5]\n",
    "        \n",
    "        for i, example in enumerate(tqdm(eval_examples, desc=f\"Evaluating {task_type}\")):\n",
    "            # Create prompt\n",
    "            instruction = example.get('instruction', '')\n",
    "            input_text = example.get('input', '')\n",
    "            expected_output = example.get('output', '')\n",
    "            \n",
    "            prompt = f\"[INST] {instruction}\\n\\n{input_text} [/INST]\"\n",
    "            \n",
    "            # Generate prediction\n",
    "            response = generate_response(eval_model, eval_tokenizer, prompt, GENERATION_CONFIG)\n",
    "            \n",
    "            predictions.append(response)\n",
    "            references.append(expected_output)\n",
    "            \n",
    "            # Save sample for manual review\n",
    "            if i < 3:  # Save first 3 examples\n",
    "                sample_outputs.append({\n",
    "                    'prompt': prompt[:200] + '...' if len(prompt) > 200 else prompt,\n",
    "                    'prediction': response,\n",
    "                    'reference': expected_output,\n",
    "                    'match': response.lower().strip() == expected_output.lower().strip()\n",
    "                })\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = calculate_simple_metrics(predictions, references)\n",
    "        \n",
    "        evaluation_results[task_type] = {\n",
    "            'task_type': task_type,\n",
    "            'num_examples': len(eval_examples),\n",
    "            'metrics': metrics,\n",
    "            'samples': sample_outputs\n",
    "        }\n",
    "        \n",
    "        print(f\"  Exact match accuracy: {metrics['exact_match_accuracy']:.3f}\")\n",
    "        print(f\"  Word overlap score: {metrics['word_overlap_score']:.3f}\")\n",
    "        print(f\"  Avg response length: {metrics['average_response_length']:.1f} words\")\n",
    "    \n",
    "    print(f\"\\n✅ Evaluation completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ Skipping evaluation - model or test data not available\")\n",
    "    evaluation_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display evaluation results\n",
    "if evaluation_results:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVALUATION RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create summary table\n",
    "    summary_data = []\n",
    "    for task_type, results in evaluation_results.items():\n",
    "        metrics = results['metrics']\n",
    "        summary_data.append({\n",
    "            'Task': TASK_TYPES.get(task_type, task_type),\n",
    "            'Examples': results['num_examples'],\n",
    "            'Exact Match': f\"{metrics['exact_match_accuracy']:.3f}\",\n",
    "            'Word Overlap': f\"{metrics['word_overlap_score']:.3f}\",\n",
    "            'Avg Length': f\"{metrics['average_response_length']:.1f}\"\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Show sample predictions\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SAMPLE PREDICTIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for task_type, results in evaluation_results.items():\n",
    "        print(f\"\\n🔍 {task_type}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for i, sample in enumerate(results['samples'][:2], 1):  # Show first 2\n",
    "            print(f\"\\nExample {i}:\")\n",
    "            print(f\"Prompt: {sample['prompt']}\")\n",
    "            print(f\"Prediction: {sample['prediction']}\")\n",
    "            print(f\"Reference: {sample['reference']}\")\n",
    "            print(f\"Match: {'✅' if sample['match'] else '❌'}\")\n",
    "            print(\"-\" * 40)\n",
    "    \n",
    "    # Save results\n",
    "    results_file = EVALUATION_RESULTS_DIR / 'evaluation_results.json'\n",
    "    final_results = {\n",
    "        'metadata': {\n",
    "            'evaluation_date': datetime.now().isoformat(),\n",
    "            'model_path': str(LORA_MODEL_PATH),\n",
    "            'base_model': MODEL_NAME,\n",
    "            'total_examples': sum(r['num_examples'] for r in evaluation_results.values())\n",
    "        },\n",
    "        'results': evaluation_results\n",
    "    }\n",
    "    \n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(final_results, f, indent=2)\n",
    "    \n",
    "    # Save summary CSV\n",
    "    summary_file = EVALUATION_RESULTS_DIR / 'evaluation_summary.csv'\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "    \n",
    "    print(f\"\\n💾 Results saved to:\")\n",
    "    print(f\"  Detailed: {results_file}\")\n",
    "    print(f\"  Summary: {summary_file}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No evaluation results to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Interactive Inference Demo\n",
    "\n",
    "Test the trained model with interactive examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive inference setup\n",
    "def test_model_inference(model, tokenizer, prompt: str, max_tokens: int = 200):\n",
    "    \"\"\"Test the model with a custom prompt\"\"\"\n",
    "    \n",
    "    if model is None or tokenizer is None:\n",
    "        return \"❌ Model not available for inference\"\n",
    "    \n",
    "    try:\n",
    "        # Format prompt\n",
    "        formatted_prompt = f\"[INST] {prompt} [/INST]\"\n",
    "        \n",
    "        # Generate response\n",
    "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        generation_config = {\n",
    "            'max_new_tokens': max_tokens,\n",
    "            'temperature': 0.7,\n",
    "            'top_p': 0.9,\n",
    "            'do_sample': True,\n",
    "            'pad_token_id': tokenizer.pad_token_id,\n",
    "            'repetition_penalty': 1.1\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, **generation_config)\n",
    "        \n",
    "        # Decode response\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        generated_tokens = outputs[0][input_length:]\n",
    "        response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        return response.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"❌ Error generating response: {e}\"\n",
    "\n",
    "# Test examples for different insurance tasks\n",
    "test_prompts = {\n",
    "    \"Policy Summary\": \"Summarize this health insurance policy: This policy provides comprehensive medical coverage including hospital stays, doctor visits, and prescription drugs. Annual deductible is $2,000 with 80% coverage after deductible. Monthly premium is $400.\",\n",
    "    \n",
    "    \"Claim Classification\": \"Classify this insurance claim: Customer's vehicle was damaged in a parking lot collision. Estimated repair cost is $3,500. No injuries reported. Police report filed.\",\n",
    "    \n",
    "    \"FAQ Generation\": \"Generate FAQs for this life insurance policy: $500,000 term life insurance with 20-year level premiums. Coverage includes accidental death benefit. Monthly premium is $75.\",\n",
    "    \n",
    "    \"Compliance Check\": \"Identify compliance requirements in this document: All customer health information must be protected according to privacy regulations. Claims must be processed within 30 days. Marketing materials must be truthful.\",\n",
    "    \n",
    "    \"Contract Q&A\": \"Based on this policy, answer: What is the deductible? Policy text: This homeowners insurance includes dwelling coverage up to $300,000 with a $1,000 deductible. Personal property coverage is 50% of dwelling amount.\"\n",
    "}\n",
    "\n",
    "print(\"🤖 Interactive Inference Demo\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if eval_model is not None and eval_tokenizer is not None:\n",
    "    print(\"✅ Model ready for inference testing\")\n",
    "    \n",
    "    for task_name, prompt in test_prompts.items():\n",
    "        print(f\"\\n📝 {task_name}\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Input: {prompt[:100]}...\")\n",
    "        \n",
    "        response = test_model_inference(eval_model, eval_tokenizer, prompt)\n",
    "        print(f\"Response: {response}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"❌ Model not available for inference testing\")\n",
    "    print(\"This could be because:\")\n",
    "    print(\"- Training was not successful\")\n",
    "    print(\"- Model failed to load\")\n",
    "    print(\"- GPU memory issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom prompt testing (you can modify this prompt)\n",
    "print(\"🎯 Custom Prompt Testing\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# You can change this prompt to test different scenarios\n",
    "custom_prompt = \"Explain the difference between term life insurance and whole life insurance in simple terms.\"\n",
    "\n",
    "if eval_model is not None and eval_tokenizer is not None:\n",
    "    print(f\"Testing custom prompt: {custom_prompt}\")\n",
    "    print(\"\\nResponse:\")\n",
    "    \n",
    "    custom_response = test_model_inference(eval_model, eval_tokenizer, custom_prompt, max_tokens=300)\n",
    "    print(custom_response)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"✨ Feel free to modify the custom_prompt variable above to test different scenarios!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Model not available for custom prompt testing\")\n",
    "\n",
    "# Clear memory one final time\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Complete! 🎉\n",
    "\n",
    "## Summary\n",
    "This notebook has completed the entire LLaMA insurance fine-tuning pipeline:\n",
    "\n",
    "1. ✅ **Data Preprocessing** - Created and cleaned sample insurance documents\n",
    "2. ✅ **Tokenization** - Formatted data for LLaMA instruction tuning\n",
    "3. ✅ **Fine-tuning** - Trained the model using LoRA for efficiency\n",
    "4. ✅ **Evaluation** - Tested model performance on insurance tasks\n",
    "5. ✅ **Inference Demo** - Interactive testing of the trained model\n",
    "\n",
    "## Files Created\n",
    "- **Model**: `outputs/final_model/lora_model/` - Your fine-tuned LoRA model\n",
    "- **Evaluation**: `outputs/evaluation/` - Performance metrics and results\n",
    "- **Data**: `data/processed/` and `data/tokenized/` - Processed datasets\n",
    "\n",
    "## Next Steps\n",
    "1. **Experiment** with different prompts in the inference demo above\n",
    "2. **Improve** the model by adding more diverse training data\n",
    "3. **Deploy** the model for production use\n",
    "4. **Share** your model on Hugging Face Hub\n",
    "\n",
    "## Need Help?\n",
    "- Review the individual notebooks (01-05) for detailed explanations\n",
    "- Check the outputs directory for detailed logs and results\n",
    "- Modify the configuration sections to experiment with different settings\n",
    "\n",
    "**Great job completing the LLaMA insurance fine-tuning pipeline! 🚀**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}